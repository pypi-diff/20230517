# Comparing `tmp/azureml_evaluate_mlflow-0.0.7-py3-none-any.whl.zip` & `tmp/azureml_evaluate_mlflow-0.0.8-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,63 +1,63 @@
-Zip file size: 173718 bytes, number of entries: 61
--rw-rw-rw-  2.0 fat      251 b- defN 23-May-15 22:05 azureml/__init__.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-May-15 22:05 azureml/evaluate/__init__.py
--rw-rw-rw-  2.0 fat   477828 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/NOTICE
--rw-rw-rw-  2.0 fat     7071 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/__init__.py
--rw-rw-rw-  2.0 fat       19 b- defN 23-May-15 22:10 azureml/evaluate/mlflow/_version.py
--rw-rw-rw-  2.0 fat      695 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/constants.py
--rw-rw-rw-  2.0 fat      373 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/exceptions.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/_loader_modules/__init__.py
--rw-rw-rw-  2.0 fat     2769 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/_loader_modules/hftransformers.py
--rw-rw-rw-  2.0 fat      457 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/_loader_modules/sklearn.py
--rw-rw-rw-  2.0 fat    33109 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/aml/__init__.py
--rw-rw-rw-  2.0 fat      930 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/aml/constants.py
--rw-rw-rw-  2.0 fat    14964 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/aml/model.py
--rw-rw-rw-  2.0 fat    12432 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/aml/utils.py
--rw-rw-rw-  2.0 fat    38122 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/hftransformers/__init__.py
--rw-rw-rw-  2.0 fat     3895 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/hftransformers/_task_based_pipeline_predictors.py
--rw-rw-rw-  2.0 fat    32935 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/hftransformers/_task_based_predictors.py
--rw-rw-rw-  2.0 fat     1050 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/hftransformers/constants.py
--rw-rw-rw-  2.0 fat     4875 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/hftransformers/dataset_wrappers.py
--rw-rw-rw-  2.0 fat     2675 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/hftransformers/utils.py
--rw-rw-rw-  2.0 fat     1832 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/__init__.py
--rw-rw-rw-  2.0 fat      540 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/__init__.py
--rw-rw-rw-  2.0 fat    42463 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/base.py
--rw-rw-rw-  2.0 fat      375 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/constants.py
--rw-rw-rw-  2.0 fat    42935 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/default_evaluator.py
--rw-rw-rw-  2.0 fat     2473 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/evaluator_registry.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/__init__.py
--rw-rw-rw-  2.0 fat     1286 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_classifier_evaluator.py
--rw-rw-rw-  2.0 fat     1830 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_classifier_multilabel_evaluator.py
--rw-rw-rw-  2.0 fat     1261 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_fill_mask_evaluator.py
--rw-rw-rw-  2.0 fat     2786 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_forecaster_evaluator.py
--rw-rw-rw-  2.0 fat     2113 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_image_classifier_evaluator.py
--rw-rw-rw-  2.0 fat     1204 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_ner_evaluator.py
--rw-rw-rw-  2.0 fat     1163 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_qna_evaluator.py
--rw-rw-rw-  2.0 fat     1058 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_regressor_evaluator.py
--rw-rw-rw-  2.0 fat     1270 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_summarization_evaluator.py
--rw-rw-rw-  2.0 fat      802 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_task_evaluator.py
--rw-rw-rw-  2.0 fat     2561 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_task_evaluator_factory.py
--rw-rw-rw-  2.0 fat     1273 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_text_generation_evaluator.py
--rw-rw-rw-  2.0 fat     1266 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/_translation_evaluator.py
--rw-rw-rw-  2.0 fat    12303 b- defN 23-May-15 22:05 azureml/evaluate/mlflow/models/evaluation/azureml/azureml_evaluator.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-May-15 22:05 tests/aml/__init__.py
--rw-rw-rw-  2.0 fat    51203 b- defN 23-May-15 22:05 tests/aml/test_model_export_with_class_and_artifacts.py
--rw-rw-rw-  2.0 fat    30719 b- defN 23-May-15 22:05 tests/aml/test_model_export_with_loader_module_and_data_path.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-May-15 22:05 tests/hftransformers/__init__.py
--rw-rw-rw-  2.0 fat      317 b- defN 23-May-15 22:05 tests/hftransformers/hf_test_predict.py
--rw-rw-rw-  2.0 fat      154 b- defN 23-May-15 22:05 tests/hftransformers/preprocess.py
--rw-rw-rw-  2.0 fat     4019 b- defN 23-May-15 22:05 tests/hftransformers/test_hf_load_pipeline.py
--rw-rw-rw-  2.0 fat    32623 b- defN 23-May-15 22:05 tests/hftransformers/test_hf_model_export.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-May-15 22:05 tests/models/__init__.py
--rw-rw-rw-  2.0 fat     4537 b- defN 23-May-15 22:05 tests/models/dummy_evaluator.py
--rw-rw-rw-  2.0 fat    27305 b- defN 23-May-15 22:05 tests/models/test_azureml_evaluator.py
--rw-rw-rw-  2.0 fat    43050 b- defN 23-May-15 22:05 tests/models/test_default_evaluator.py
--rw-rw-rw-  2.0 fat    37145 b- defN 23-May-15 22:05 tests/models/test_evaluation.py
--rw-rw-rw-  2.0 fat    13542 b- defN 23-May-15 22:05 tests/models/test_forecaster_evaluator.py
--rw-rw-rw-  2.0 fat    50782 b- defN 23-May-15 22:05 tests/models/utils.py
--rw-rw-rw-  2.0 fat     1021 b- defN 23-May-15 22:10 azureml_evaluate_mlflow-0.0.7.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     1686 b- defN 23-May-15 22:10 azureml_evaluate_mlflow-0.0.7.dist-info/METADATA
--rw-rw-rw-  2.0 fat       97 b- defN 23-May-15 22:10 azureml_evaluate_mlflow-0.0.7.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       14 b- defN 23-May-15 22:10 azureml_evaluate_mlflow-0.0.7.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     6513 b- defN 23-May-15 22:10 azureml_evaluate_mlflow-0.0.7.dist-info/RECORD
-61 files, 1063069 bytes uncompressed, 162872 bytes compressed:  84.7%
+Zip file size: 174133 bytes, number of entries: 61
+-rw-rw-rw-  2.0 fat      251 b- defN 23-May-16 08:41 azureml/__init__.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-May-16 08:41 azureml/evaluate/__init__.py
+-rw-rw-rw-  2.0 fat   477828 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/NOTICE
+-rw-rw-rw-  2.0 fat     7071 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/__init__.py
+-rw-rw-rw-  2.0 fat       19 b- defN 23-May-16 08:47 azureml/evaluate/mlflow/_version.py
+-rw-rw-rw-  2.0 fat      695 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/constants.py
+-rw-rw-rw-  2.0 fat      373 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/exceptions.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/_loader_modules/__init__.py
+-rw-rw-rw-  2.0 fat     2867 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/_loader_modules/hftransformers.py
+-rw-rw-rw-  2.0 fat      457 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/_loader_modules/sklearn.py
+-rw-rw-rw-  2.0 fat    33109 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/aml/__init__.py
+-rw-rw-rw-  2.0 fat      930 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/aml/constants.py
+-rw-rw-rw-  2.0 fat    14964 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/aml/model.py
+-rw-rw-rw-  2.0 fat    12432 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/aml/utils.py
+-rw-rw-rw-  2.0 fat    39007 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/hftransformers/__init__.py
+-rw-rw-rw-  2.0 fat     3895 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/hftransformers/_task_based_pipeline_predictors.py
+-rw-rw-rw-  2.0 fat    33130 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/hftransformers/_task_based_predictors.py
+-rw-rw-rw-  2.0 fat     1050 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/hftransformers/constants.py
+-rw-rw-rw-  2.0 fat     4875 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/hftransformers/dataset_wrappers.py
+-rw-rw-rw-  2.0 fat     2675 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/hftransformers/utils.py
+-rw-rw-rw-  2.0 fat     1832 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/__init__.py
+-rw-rw-rw-  2.0 fat      540 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/__init__.py
+-rw-rw-rw-  2.0 fat    42463 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/base.py
+-rw-rw-rw-  2.0 fat      375 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/constants.py
+-rw-rw-rw-  2.0 fat    42935 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/default_evaluator.py
+-rw-rw-rw-  2.0 fat     2473 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/evaluator_registry.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/azureml/__init__.py
+-rw-rw-rw-  2.0 fat     1286 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/azureml/_classifier_evaluator.py
+-rw-rw-rw-  2.0 fat     1830 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/azureml/_classifier_multilabel_evaluator.py
+-rw-rw-rw-  2.0 fat     1261 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/azureml/_fill_mask_evaluator.py
+-rw-rw-rw-  2.0 fat     2786 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/azureml/_forecaster_evaluator.py
+-rw-rw-rw-  2.0 fat     2113 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/azureml/_image_classifier_evaluator.py
+-rw-rw-rw-  2.0 fat     1204 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/azureml/_ner_evaluator.py
+-rw-rw-rw-  2.0 fat     1163 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/azureml/_qna_evaluator.py
+-rw-rw-rw-  2.0 fat     1058 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/azureml/_regressor_evaluator.py
+-rw-rw-rw-  2.0 fat     1270 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/azureml/_summarization_evaluator.py
+-rw-rw-rw-  2.0 fat      802 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/azureml/_task_evaluator.py
+-rw-rw-rw-  2.0 fat     2561 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/azureml/_task_evaluator_factory.py
+-rw-rw-rw-  2.0 fat     1273 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/azureml/_text_generation_evaluator.py
+-rw-rw-rw-  2.0 fat     1266 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/azureml/_translation_evaluator.py
+-rw-rw-rw-  2.0 fat    12303 b- defN 23-May-16 08:41 azureml/evaluate/mlflow/models/evaluation/azureml/azureml_evaluator.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-May-16 08:41 tests/aml/__init__.py
+-rw-rw-rw-  2.0 fat    52149 b- defN 23-May-16 08:41 tests/aml/test_model_export_with_class_and_artifacts.py
+-rw-rw-rw-  2.0 fat    31450 b- defN 23-May-16 08:41 tests/aml/test_model_export_with_loader_module_and_data_path.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-May-16 08:41 tests/hftransformers/__init__.py
+-rw-rw-rw-  2.0 fat      317 b- defN 23-May-16 08:41 tests/hftransformers/hf_test_predict.py
+-rw-rw-rw-  2.0 fat      154 b- defN 23-May-16 08:41 tests/hftransformers/preprocess.py
+-rw-rw-rw-  2.0 fat     4105 b- defN 23-May-16 08:41 tests/hftransformers/test_hf_load_pipeline.py
+-rw-rw-rw-  2.0 fat    33365 b- defN 23-May-16 08:41 tests/hftransformers/test_hf_model_export.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-May-16 08:41 tests/models/__init__.py
+-rw-rw-rw-  2.0 fat     4537 b- defN 23-May-16 08:41 tests/models/dummy_evaluator.py
+-rw-rw-rw-  2.0 fat    28181 b- defN 23-May-16 08:41 tests/models/test_azureml_evaluator.py
+-rw-rw-rw-  2.0 fat    44168 b- defN 23-May-16 08:41 tests/models/test_default_evaluator.py
+-rw-rw-rw-  2.0 fat    37704 b- defN 23-May-16 08:41 tests/models/test_evaluation.py
+-rw-rw-rw-  2.0 fat    13542 b- defN 23-May-16 08:41 tests/models/test_forecaster_evaluator.py
+-rw-rw-rw-  2.0 fat    50782 b- defN 23-May-16 08:41 tests/models/utils.py
+-rw-rw-rw-  2.0 fat     1021 b- defN 23-May-16 08:47 azureml_evaluate_mlflow-0.0.8.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     1783 b- defN 23-May-16 08:47 azureml_evaluate_mlflow-0.0.8.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       97 b- defN 23-May-16 08:47 azureml_evaluate_mlflow-0.0.8.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       14 b- defN 23-May-16 08:47 azureml_evaluate_mlflow-0.0.8.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     6513 b- defN 23-May-16 08:47 azureml_evaluate_mlflow-0.0.8.dist-info/RECORD
+61 files, 1069402 bytes uncompressed, 163287 bytes compressed:  84.7%
```

## zipnote {}

```diff
@@ -162,23 +162,23 @@
 
 Filename: tests/models/test_forecaster_evaluator.py
 Comment: 
 
 Filename: tests/models/utils.py
 Comment: 
 
-Filename: azureml_evaluate_mlflow-0.0.7.dist-info/LICENSE.txt
+Filename: azureml_evaluate_mlflow-0.0.8.dist-info/LICENSE.txt
 Comment: 
 
-Filename: azureml_evaluate_mlflow-0.0.7.dist-info/METADATA
+Filename: azureml_evaluate_mlflow-0.0.8.dist-info/METADATA
 Comment: 
 
-Filename: azureml_evaluate_mlflow-0.0.7.dist-info/WHEEL
+Filename: azureml_evaluate_mlflow-0.0.8.dist-info/WHEEL
 Comment: 
 
-Filename: azureml_evaluate_mlflow-0.0.7.dist-info/top_level.txt
+Filename: azureml_evaluate_mlflow-0.0.8.dist-info/top_level.txt
 Comment: 
 
-Filename: azureml_evaluate_mlflow-0.0.7.dist-info/RECORD
+Filename: azureml_evaluate_mlflow-0.0.8.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## azureml/evaluate/mlflow/_version.py

```diff
@@ -1 +1 @@
-VERSION = "0.0.7"
+VERSION = "0.0.8"
```

## azureml/evaluate/mlflow/_loader_modules/hftransformers.py

```diff
@@ -1,13 +1,15 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 import logging
+import os
+
+from mlflow import MlflowException
 
-import pandas as pd
 import azureml.evaluate.mlflow as mlflow
 from mlflow.utils.model_utils import _get_flavor_configuration
 
 _logger = logging.getLogger(__name__)
 
 
 class _AzureMLTransformersWrapper(mlflow.hftransformers._HFTransformersWrapper):
@@ -40,15 +42,15 @@
 
 def _load_azureml(path, **kwargs):
     """
     Load PyFunc implementation. Called by ``pyfunc.load_model``.
 
     :param path: Local filesystem path to the MLflow Model with the ``pytorch`` flavor.
     """
-    if "\\" in path:
-        path1 = "\\".join(path.split("\\")[:-1])
-    else:
-        path1 = "/".join(path.split("/")[:-1])
-    hf_conf = _get_flavor_configuration(path1, mlflow.hftransformers.FLAVOR_NAME)
+    path1 = os.sep.join(path.split(os.sep)[:-1])
+    try:
+        hf_conf = _get_flavor_configuration(path1, mlflow.hftransformers.FLAVOR_NAME_MLMODEL_LOGGING)
+    except MlflowException as e:
+        hf_conf = _get_flavor_configuration(path1, mlflow.hftransformers.FLAVOR_NAME)
     task_type, hf_model, tokenizer, config = mlflow.hftransformers._load_model(path, hf_conf)
     hf_conf["path"] = path1
     return _AzureMLTransformersWrapper(task_type, hf_model, tokenizer, config, hf_conf)
```

## azureml/evaluate/mlflow/hftransformers/__init__.py

```diff
@@ -54,15 +54,15 @@
     _validate_and_prepare_target_save_path,
 )
 from mlflow.tracking._model_registry import DEFAULT_AWAIT_MAX_SLEEP_SECONDS
 
 from azureml.evaluate.mlflow.hftransformers.constants import Constants
 
 FLAVOR_NAME = "hftransformers"
-
+FLAVOR_NAME_MLMODEL_LOGGING = "hftransformersv2"
 _HFMODEL_PATH = "model"
 _HF_TOKENIZER_PATH = "tokenizer"
 _HF_CONFIG_PATH = "config"
 _EXTRA_FILES_KEY = "extra_files"
 _REQUIREMENTS_FILE_KEY = "requirements_file"
 
 _logger = logging.getLogger(__name__)
@@ -80,14 +80,15 @@
             [
                 "torch",
                 "transformers",
                 # We include CloudPickle in the default environment because
                 # it's required by the default pickle module used by `save_model()`
                 # and `log_model()`: `mlflow.pytorch.pickle_module`.
                 "cloudpickle",
+                "azureml-evaluate-mlflow"
             ],
         )
     )
 
 
 def get_default_conda_env():
     """
@@ -263,15 +264,15 @@
     model_path = os.path.join(model_data_path, _HFMODEL_PATH)
     tokenizer_path = os.path.join(model_data_path, _HF_TOKENIZER_PATH)
     config_path = os.path.join(model_data_path, _HF_CONFIG_PATH)
 
     def save_or_copy(obj_type, obj, base_class, path_to_save):
         if isinstance(obj, base_class) or hasattr(obj, 'save_pretrained'):
             obj.save_pretrained(path_to_save)
-        elif isinstance(obj, str):
+        elif isinstance(obj, str) and os.path.exists(obj):
             shutil.copytree(obj, path_to_save)
         else:
             _logger.error(
                 f"The {obj_type} should be either a instance of {base_class}, or should be a path to the {obj_type}")
 
     save_or_copy("model", hf_model, PreTrainedModel, model_path)
     if tokenizer is not None:
@@ -329,27 +330,27 @@
 def _add_package_if_not_exists_conda(conda_env, package):
     pip_dependencies = conda_env.get("dependencies", [])
     for i, dep in enumerate(pip_dependencies):
         if isinstance(dep, dict) and "pip" in dep and isinstance(dep["pip"], list):
             for item in dep["pip"]:
                 if package in item:
                     return conda_env
-            conda_env["dependencies"][i]["pip"].append(package)
+            conda_env["dependencies"][i]["pip"].append(_get_pinned_requirement(package))
             return conda_env
     return conda_env
 
 
 def _add_package_if_not_exists_pip(pip_list, package):
     """
     Check if <package> str exists in pip_list
     """
     for item in pip_list:
         if package in item:
             return pip_list
-    pip_list.append(package)
+    pip_list.append(_get_pinned_requirement(package))
     return pip_list
 
 
 @format_docstring(LOG_MODEL_PARAM_DOCS.format(package_name=FLAVOR_NAME))
 def save_model(
         hf_model,
         path,
@@ -578,15 +579,15 @@
             #     os.mkdir(mlflow_model.artifact_path)
             np.save(path_to_list, hf_conf[key], allow_pickle=True)
             torchserve_artifacts_config[key] = {"path_list": key + ".npy"}
         else:
             torchserve_artifacts_config[key] = hf_conf[key]
 
     mlflow_model.add_flavor(
-        FLAVOR_NAME,
+        FLAVOR_NAME_MLMODEL_LOGGING,
         model_data=model_data_subpath,
         pytorch_version=str(torch.__version__),
         transformers_version=str(transformers.__version__),
         hf_pretrained_class=hf_pretrained_class,
         hf_tokenizer_class=hf_tokenizer_class,
         hf_config_class=hf_config_class,
         code=code_dir_subpath,
@@ -667,26 +668,32 @@
     config_path = os.path.join(path, _HF_CONFIG_PATH)
     task_type = hf_conf.get("task_type", None)
     config_path = config_path if os.path.exists(config_path) else model_path
     tokenizer_path = tokenizer_path if os.path.exists(tokenizer_path) else model_path
 
     config, tokenizer = None, None
     if hf_conf.get("force_load_config", True):
+        config_load_args = hf_conf.pop("config_hf_load_kwargs", {})
         config = _load_object_from_module(module_name=hf_conf.get("custom_config_module", "transformers"),
                                           class_name=hf_conf.get("hf_config_class", "AutoConfig"),
-                                          path_to_obj=config_path)
+                                          path_to_obj=config_path,
+                                          **config_load_args)
     if hf_conf.get("force_load_tokenizer", True):
+        tokenizer_load_args = hf_conf.pop("tokenizer_hf_load_kwargs", {})
         tokenizer = _load_object_from_module(module_name=hf_conf.get("custom_tokenizer_module", "transformers"),
                                              class_name=hf_conf.get("hf_tokenizer_class", "AutoTokenizer"),
                                              path_to_obj=tokenizer_path,
-                                             config=config)
+                                             config=config,
+                                             **tokenizer_load_args)
+    model_load_args = hf_conf.pop("model_hf_load_kwargs", {})
     hf_model = _load_object_from_module(module_name=hf_conf.get("custom_model_module", "transformers"),
                                         class_name=hf_conf.get("hf_pretrained_class", "AutoModel"),
                                         path_to_obj=model_path,
-                                        config=config)
+                                        config=config,
+                                        **model_load_args)
     return task_type, hf_model, tokenizer, config
 
 
 def load_model(model_uri, dst_path=None, **kwargs):
     """
     Load a PyTorch model from a local file or a run.
 
@@ -743,48 +750,54 @@
         predict X: 4.0, y_pred: 7.57
         predict X: 6.0, y_pred: 11.64
         predict X: 30.0, y_pred: 60.48
     """
     import transformers
 
     local_model_path = _download_artifact_from_uri(artifact_uri=model_uri, output_path=dst_path)
-    hf_conf = _get_flavor_configuration(model_path=local_model_path, flavor_name=FLAVOR_NAME)
+    try:
+        hf_conf = _get_flavor_configuration(local_model_path, FLAVOR_NAME_MLMODEL_LOGGING)
+    except MlflowException as e:
+        hf_conf = _get_flavor_configuration(local_model_path, FLAVOR_NAME)
     _add_code_from_conf_to_system_path(local_model_path, hf_conf)
 
     if transformers.__version__ != hf_conf["transformers_version"]:
         _logger.warning(
             "Stored model version '%s' does not match installed transformers_version version '%s'",
             hf_conf["transformers_version"],
             transformers.__version__,
         )
     hf_model_artifacts_path = os.path.join(local_model_path, hf_conf["model_data"])
     return _load_model(path=hf_model_artifacts_path, hf_conf=hf_conf)
 
 
 def load_pipeline(model_uri, dst_path=None, **kwargs):
     local_model_path = _download_artifact_from_uri(artifact_uri=model_uri, output_path=dst_path)
-    hf_conf = _get_flavor_configuration(local_model_path, FLAVOR_NAME)
+    try:
+        hf_conf = _get_flavor_configuration(local_model_path, FLAVOR_NAME_MLMODEL_LOGGING)
+    except MlflowException as e:
+        hf_conf = _get_flavor_configuration(local_model_path, FLAVOR_NAME)
     hf_model_artifacts_path = os.path.join(local_model_path, hf_conf["model_data"])
     task_type, hf_model, tokenizer, config = _load_model(hf_model_artifacts_path, hf_conf)
     task_type = kwargs.pop("task_type", task_type)
     hf_conf["path"] = local_model_path
     return _HFPipelineWrapper(task_type, hf_model, tokenizer, config, hf_conf, **kwargs)
 
 
 def _load_pyfunc(path, **kwargs):
     """
     Load PyFunc implementation. Called by ``pyfunc.load_model``.
 
     :param path: Local filesystem path to the MLflow Model with the ``pytorch`` flavor.
     """
-    if "\\" in path:
-        path1 = "\\".join(path.split("\\")[:-1])
-    else:
-        path1 = "/".join(path.split("/")[:-1])
-    hf_conf = _get_flavor_configuration(path1, FLAVOR_NAME)
+    path1 = os.sep.join(path.split(os.sep)[:-1])
+    try:
+        hf_conf = _get_flavor_configuration(path1, FLAVOR_NAME_MLMODEL_LOGGING)
+    except MlflowException as e:
+        hf_conf = _get_flavor_configuration(path1, FLAVOR_NAME)
     task_type, hf_model, tokenizer, config = _load_model(path, hf_conf)
     hf_conf["path"] = path1
     return _HFTransformersWrapper(task_type, hf_model, tokenizer, config, hf_conf)
 
 
 class _HFTransformersWrapper:
     """
```

## azureml/evaluate/mlflow/hftransformers/_task_based_predictors.py

```diff
@@ -163,17 +163,20 @@
 
     def _get_pipeline_parameters(self, **kwargs):
         """
         Extract relevant kwargs to set in pipeline
         @param kwargs:
         @return:
         """
+        pipeline_init_args = kwargs.pop("pipeline_init_args", {})
         parameters = {"device": kwargs.get("device", -1),
                       "batch_size": kwargs.get("batch_size", None),
-                      "model_kwargs": kwargs.pop("model_kwargs", None)}
+                      "model_kwargs": kwargs.pop("model_kwargs", None),
+                      "trust_remote_code": kwargs.pop("trust_remote_code", True),
+                      **pipeline_init_args}
         return parameters
 
     def _get_tokenizer_config(self, **kwargs):
         """
         Extract tokenizer config from kwargs
         :param kwargs: dict
         :return: Tokenizer Kwargs dict
```

## tests/aml/test_model_export_with_class_and_artifacts.py

```diff
@@ -123,14 +123,15 @@
         additional_pip_deps=[
             "cloudpickle=={}".format(cloudpickle.__version__),
             "scikit-learn=={}".format(sklearn.__version__),
         ],
     )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_model_save_load(sklearn_knn_model, main_scoped_model_class, iris_data, tmpdir):
     sklearn_model_path = os.path.join(str(tmpdir), "sklearn_model")
     azureml_mlflow.sklearn.save_model(sk_model=sklearn_knn_model, path=sklearn_model_path)
 
     def test_predict(sk_model, model_input):
         return sk_model.predict(model_input) * 2
 
@@ -154,14 +155,15 @@
 #     loaded_aml_model = azureml_mlflow.aml.load_model(model_uri=multiclass_llm_aml_model_uri)
 #     # loaded_aml_model = azureml_mlflow.aml.load_model(model_uri=multiclass_llm_model_uri)
 #     assert hasattr(loaded_aml_model, "predict")
 #     assert hasattr(loaded_aml_model, "predict_proba")
 #     # ToDo: add assertion for prediciton comparison
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_pyfunc_model_log_load_no_active_run(sklearn_knn_model, main_scoped_model_class, iris_data):
     sklearn_artifact_path = "sk_model_no_run"
     with azureml_mlflow.start_run():
         azureml_mlflow.sklearn.log_model(sk_model=sklearn_knn_model, artifact_path=sklearn_artifact_path)
         sklearn_model_uri = "runs:/{run_id}/{artifact_path}".format(
             run_id=azureml_mlflow.active_run().info.run_id, artifact_path=sklearn_artifact_path
         )
@@ -183,14 +185,15 @@
     np.testing.assert_array_equal(
         loaded_pyfunc_model.predict(iris_data[0]),
         test_predict(sk_model=sklearn_knn_model, model_input=iris_data[0]),
     )
     azureml_mlflow.end_run()
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_model_log_load(sklearn_knn_model, main_scoped_model_class, iris_data):
     sklearn_artifact_path = "sk_model"
     with azureml_mlflow.start_run():
         azureml_mlflow.sklearn.log_model(sk_model=sklearn_knn_model, artifact_path=sklearn_artifact_path)
         sklearn_model_uri = "runs:/{run_id}/{artifact_path}".format(
             run_id=azureml_mlflow.active_run().info.run_id, artifact_path=sklearn_artifact_path
         )
@@ -220,14 +223,15 @@
     assert model_config.to_yaml() == loaded_pyfunc_model.metadata.to_yaml()
     np.testing.assert_array_equal(
         loaded_pyfunc_model.predict(iris_data[0]),
         test_predict(sk_model=sklearn_knn_model, model_input=iris_data[0]),
     )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_signature_and_examples_are_saved_correctly(iris_data, main_scoped_model_class, tmpdir):
     sklearn_model_path = tmpdir.join("sklearn_model").strpath
     azureml_mlflow.sklearn.save_model(sk_model=sklearn_knn_model, path=sklearn_model_path)
 
     def test_predict(sk_model, model_input):
         return sk_model.predict(model_input) * 2
 
@@ -281,14 +285,15 @@
 #         )
 #         azureml_mlflow.register_model.assert_called_once_with(
 #             model_uri, "AdsModel1", await_registration_for=DEFAULT_AWAIT_MAX_SLEEP_SECONDS
 #         )
 #         azureml_mlflow.end_run()
 #
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_log_model_no_registered_model_name(sklearn_knn_model, main_scoped_model_class):
     register_model_patch = mock.patch("azureml.evaluate.mlflow.register_model")
     with register_model_patch:
         sklearn_artifact_path = "sk_model_no_run"
         with azureml_mlflow.start_run():
             azureml_mlflow.sklearn.log_model(
                 sk_model=sklearn_knn_model, artifact_path=sklearn_artifact_path
@@ -340,14 +345,15 @@
 #     loaded_pyfunc_model = azureml_mlflow.aml.load_model(model_uri=model_uri, model_type="classifier")
 #     np.testing.assert_array_equal(
 #         loaded_pyfunc_model.predict(iris_data[0]),
 #         test_predict(sk_model=sklearn_knn_model, model_input=iris_data[0]),
 #     )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_add_to_model_adds_specified_kwargs_to_mlmodel_configuration():
     custom_kwargs = {
         "key1": "value1",
         "key2": 20,
         "key3": range(10),
     }
     model_config = Model()
@@ -553,14 +559,15 @@
 #     assert 0 == process.wait(), "stderr = \n\n{}\n\n".format(stderr)
 #     result_df = pandas.read_json(output_json_path, orient="records")
 #     np.testing.assert_array_equal(
 #         result_df.values.transpose()[0], loaded_pyfunc_model.predict(sample_input)
 #     )
 #
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_save_model_persists_specified_conda_env_in_mlflow_model_directory(
         sklearn_knn_model, main_scoped_model_class, pyfunc_custom_env, tmpdir
 ):
     sklearn_model_path = os.path.join(str(tmpdir), "sklearn_model")
     azureml_mlflow.sklearn.save_model(
         sk_model=sklearn_knn_model,
         path=sklearn_model_path,
@@ -585,14 +592,15 @@
     with open(pyfunc_custom_env, "r") as f:
         pyfunc_custom_env_parsed = yaml.safe_load(f)
     with open(saved_conda_env_path, "r") as f:
         saved_conda_env_parsed = yaml.safe_load(f)
     assert saved_conda_env_parsed == pyfunc_custom_env_parsed
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_save_model_persists_requirements_in_mlflow_model_directory(
         sklearn_knn_model, main_scoped_model_class, pyfunc_custom_env, tmpdir
 ):
     sklearn_model_path = os.path.join(str(tmpdir), "sklearn_model")
     azureml_mlflow.sklearn.save_model(
         sk_model=sklearn_knn_model,
         path=sklearn_model_path,
@@ -686,14 +694,15 @@
 #         _assert_pip_requirements(
 #             azureml_mlflow.get_artifact_uri("model"),
 #             ["azureml_mlflow", *default_reqs, "b", "-c constraints.txt"],
 #             ["a"],
 #         )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_log_model_persists_specified_conda_env_in_mlflow_model_directory(
         sklearn_knn_model, main_scoped_model_class, pyfunc_custom_env
 ):
     sklearn_artifact_path = "sk_model"
     with azureml_mlflow.start_run():
         azureml_mlflow.sklearn.log_model(sk_model=sklearn_knn_model, artifact_path=sklearn_artifact_path)
         sklearn_run_id = azureml_mlflow.active_run().info.run_id
@@ -726,14 +735,15 @@
     with open(pyfunc_custom_env, "r") as f:
         pyfunc_custom_env_parsed = yaml.safe_load(f)
     with open(saved_conda_env_path, "r") as f:
         saved_conda_env_parsed = yaml.safe_load(f)
     assert saved_conda_env_parsed == pyfunc_custom_env_parsed
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_model_log_persists_requirements_in_mlflow_model_directory(
         sklearn_knn_model, main_scoped_model_class, pyfunc_custom_env
 ):
     sklearn_artifact_path = "sk_model"
     with azureml_mlflow.start_run():
         azureml_mlflow.sklearn.log_model(sk_model=sklearn_knn_model, artifact_path=sklearn_artifact_path)
         sklearn_run_id = azureml_mlflow.active_run().info.run_id
@@ -756,14 +766,15 @@
             )
         )
 
     saved_pip_req_path = os.path.join(pyfunc_model_path, "requirements.txt")
     _compare_conda_env_requirements(pyfunc_custom_env, saved_pip_req_path)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_save_model_without_specified_conda_env_uses_default_env_with_expected_dependencies(
         sklearn_logreg_model, main_scoped_model_class, tmpdir
 ):
     sklearn_model_path = os.path.join(str(tmpdir), "sklearn_model")
     azureml_mlflow.sklearn.save_model(sk_model=sklearn_logreg_model, path=sklearn_model_path)
 
     pyfunc_model_path = os.path.join(str(tmpdir), "aml_model")
@@ -772,14 +783,15 @@
         artifacts={"sk_model": sklearn_model_path},
         aml_model=main_scoped_model_class(predict_fn=None),
         conda_env=_conda_env(),
     )
     _assert_pip_requirements(pyfunc_model_path, azureml_mlflow.aml.get_default_pip_requirements())
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_log_model_without_specified_conda_env_uses_default_env_with_expected_dependencies(
         sklearn_knn_model, main_scoped_model_class
 ):
     sklearn_artifact_path = "sk_model"
     with azureml_mlflow.start_run():
         azureml_mlflow.sklearn.log_model(sk_model=sklearn_knn_model, artifact_path=sklearn_artifact_path)
         sklearn_run_id = azureml_mlflow.active_run().info.run_id
@@ -795,14 +807,15 @@
             },
             aml_model=main_scoped_model_class(predict_fn=None),
         )
         model_uri = azureml_mlflow.get_artifact_uri(pyfunc_artifact_path)
     _assert_pip_requirements(model_uri, azureml_mlflow.aml.get_default_pip_requirements())
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_save_model_correctly_resolves_directory_artifact_with_nested_contents(
         tmpdir, model_path, iris_data
 ):
     directory_artifact_path = os.path.join(str(tmpdir), "directory_artifact")
     nested_file_relative_path = os.path.join(
         "my", "somewhat", "heavily", "nested", "directory", "myfile.txt"
     )
@@ -830,14 +843,15 @@
         conda_env=_conda_env(),
     )
 
     loaded_model = azureml_mlflow.aml.load_model(model_uri=model_path, model_type="classifier")
     assert loaded_model.predict(iris_data[0])
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_save_model_with_no_artifacts_does_not_produce_artifacts_dir(model_path):
     azureml_mlflow.aml.save_model(
         path=model_path,
         aml_model=ModuleScopedSklearnModel(predict_fn=None),
         artifacts=None,
         conda_env=_conda_env(),
     )
@@ -846,27 +860,29 @@
     assert "artifacts" not in os.listdir(model_path)
     pyfunc_conf = _get_flavor_configuration(
         model_path=model_path, flavor_name=azureml_mlflow.aml.FLAVOR_NAME
     )
     assert azureml_mlflow.aml.model.CONFIG_KEY_ARTIFACTS not in pyfunc_conf
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_save_model_with_python_model_argument_of_invalid_type_raises_exeption(tmpdir):
     match = "aml_model` must be a subclass of `AzureMLModel`"
     with pytest.raises(MlflowException, match=match):
         azureml_mlflow.aml.save_model(
             path=os.path.join(str(tmpdir), "model1"), aml_model="not the right type"
         )
 
     with pytest.raises(MlflowException, match=match):
         azureml_mlflow.aml.save_model(
             path=os.path.join(str(tmpdir), "model2"), aml_model="not the right type"
         )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_save_model_with_unsupported_argument_combinations_throws_exception(model_path):
     with pytest.raises(
             MlflowException, match="Either `loader_module` or `aml_model` must be specified"
     ) as exc_info:
         azureml_mlflow.aml.save_model(
             path=model_path, artifacts={"artifact": "/path/to/artifact"}, aml_model=None
         )
@@ -894,14 +910,15 @@
 
     with pytest.raises(
             MlflowException, match="Either `loader_module` or `aml_model` must be specified"
     ):
         azureml_mlflow.aml.save_model(path=model_path, aml_model=None, loader_module=None)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_log_model_with_unsupported_argument_combinations_throws_exception():
     match = (
         "Either `loader_module` or `aml_model` must be specified. A `loader_module` "
         "should be a python module. A `aml_model` should be a subclass of "
         "AzureMLModel"
     )
     with azureml_mlflow.start_run(), pytest.raises(MlflowException, match=match):
@@ -937,14 +954,15 @@
 
     with azureml_mlflow.start_run(), pytest.raises(
             MlflowException, match="Either `loader_module` or `aml_model` must be specified"
     ):
         azureml_mlflow.aml.log_model(artifact_path="aml_model", aml_model=None, loader_module=None)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_repr_can_be_called_withtout_run_id_or_artifact_path():
     model_meta = Model(
         artifact_path=None,
         run_id=None,
         flavors={"aml": {"loader_module": "someFlavour"}},
     )
 
@@ -953,14 +971,15 @@
             return model_input
 
     model_impl = TestModel()
 
     assert "flavor: someFlavour" in azureml_mlflow.aml.AMLModel(model_meta, model_impl).__repr__()
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_load_model_with_differing_cloudpickle_version_at_micro_granularity_logs_warning(
         model_path,
 ):
     class TestModel(azureml.evaluate.mlflow.aml.AzureMLModel):
         def predict(self, context, model_input):
             return model_input
 
@@ -990,14 +1009,15 @@
         "differs from the version of CloudPickle that is currently running" in log_message
         and saver_cloudpickle_version in log_message
         and loader_cloudpickle_version in log_message
         for log_message in log_messages
     )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_load_model_with_missing_cloudpickle_version_logs_warning(model_path):
     class TestModel(azureml.evaluate.mlflow.aml.AzureMLModel):
         def predict(self, context, model_input):
             return model_input
 
     azureml_mlflow.aml.save_model(path=model_path, aml_model=TestModel())
     model_config_path = os.path.join(model_path, "MLmodel")
@@ -1058,14 +1078,15 @@
 
     def predict(self, context, model_input):
         import custom_module
 
         return custom_module.predict()
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_model_with_code_path_does_not_use_cached_module(tmp_path):
     dir1 = tmp_path.joinpath("1")
     dir2 = tmp_path.joinpath("2")
     dir1.mkdir()
     dir2.mkdir()
     mod1 = dir1.joinpath("custom_module.py")
     mod2 = dir2.joinpath("custom_module.py")
@@ -1104,14 +1125,15 @@
 
 class TestModel:
     @staticmethod
     def predict(pdf):
         return pdf
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_column_schema_enforcement():
     m = Model()
     input_schema = Schema(
         [
             ColSpec("integer", "a"),
             ColSpec("long", "b"),
             ColSpec("float", "c"),
```

## tests/aml/test_model_export_with_loader_module_and_data_path.py

```diff
@@ -86,14 +86,15 @@
 
 
 @pytest.fixture
 def model_path(tmpdir):
     return os.path.join(str(tmpdir), "model")
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_model_save_load(sklearn_knn_model, iris_data, tmpdir, model_path):
     sk_model_path = os.path.join(str(tmpdir), "knn.pkl")
     with open(sk_model_path, "wb") as f:
         pickle.dump(sklearn_knn_model, f)
 
     model_config = Model(run_id="test", artifact_path="testtest")
     azureml_mlflow.aml.save_model(
@@ -110,14 +111,15 @@
     assert azureml_mlflow.aml.PY_VERSION in reloaded_model_config.flavors[azureml_mlflow.aml.FLAVOR_NAME]
     reloaded_model = azureml_mlflow.aml.load_model(model_path)
     np.testing.assert_array_equal(
         sklearn_knn_model.predict(iris_data[0]), reloaded_model.predict(iris_data[0])
     )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_signature_and_examples_are_saved_correctly(sklearn_knn_model, iris_data):
     data = iris_data
     signature_ = infer_signature(*data)
     example_ = data[0][:3, ]
     for signature in (None, signature_):
         for example in (None, example_):
             with TempDir() as tmp:
@@ -136,14 +138,15 @@
                 assert signature == mlflow_model.signature
                 if example is None:
                     assert mlflow_model.saved_input_example_info is None
                 else:
                     assert np.array_equal(_read_example(mlflow_model, path), example)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_column_schema_enforcement():
     m = Model()
     input_schema = Schema(
         [
             ColSpec("integer", "a"),
             ColSpec("long", "b"),
             ColSpec("float", "c"),
@@ -331,14 +334,15 @@
 def _compare_exact_tensor_dict_input(d1, d2):
     """Return whether two dicts of np arrays are exactly equal"""
     if d1.keys() != d2.keys():
         return False
     return all(np.array_equal(d1[key], d2[key]) for key in d1)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_tensor_multi_named_schema_enforcement():
     m = Model()
     input_schema = Schema(
         [
             TensorSpec(np.dtype(np.uint64), (-1, 5), "a"),
             TensorSpec(np.dtype(np.short), (-1, 2), "b"),
             TensorSpec(np.dtype(np.float32), (2, -1, 2), "c"),
@@ -437,14 +441,15 @@
     with pytest.raises(
             MlflowException,
             match=re.escape("Shape of input (1,) does not match expected shape (-1, 5)"),
     ):
         aml_model.predict(pdf)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_schema_enforcement_single_named_tensor_schema():
     m = Model()
     input_schema = Schema([TensorSpec(np.dtype(np.uint64), (-1, 2), "a")])
     m.signature = ModelSignature(inputs=input_schema)
     aml_model = AMLGenericModel(model_meta=m, model_impl=TestModel())
     inp = {
         "a": np.array([[0, 0], [1, 1]], dtype=np.uint64),
@@ -465,14 +470,15 @@
     assert expected_types == actual_types
 
     # test list does not work
     with pytest.raises(MlflowException, match="Model is missing inputs"):
         aml_model.predict([[0, 0], [1, 1]])
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_schema_enforcement_named_tensor_schema_1d():
     m = Model()
     input_schema = Schema(
         [TensorSpec(np.dtype(np.uint64), (-1,), "a"), TensorSpec(np.dtype(np.float32), (-1,), "b")]
     )
     m.signature = ModelSignature(inputs=input_schema)
     aml_model = AMLGenericModel(model_meta=m, model_impl=TestModel())
@@ -495,14 +501,15 @@
     res = aml_model.predict(d_inp)
     assert res == d_inp
     expected_types = dict(zip(input_schema.input_names(), input_schema.input_types()))
     actual_types = {k: v.dtype for k, v in res.items()}
     assert expected_types == actual_types
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_missing_value_hint_is_displayed_when_it_should():
     m = Model()
     input_schema = Schema([ColSpec("integer", "a")])
     m.signature = ModelSignature(inputs=input_schema)
     aml_model = AMLGenericModel(model_meta=m, model_impl=TestModel())
     pdf = pd.DataFrame(data=[[1], [None]], columns=["a"])
     match = "Incompatible input types"
@@ -516,14 +523,15 @@
     assert hint not in str(ex.value.message)
     pdf = pd.DataFrame(data=[[1], [2]], columns=["a"], dtype=np.float64)
     with pytest.raises(MlflowException, match=match) as ex:
         aml_model.predict(pdf)
     assert hint not in str(ex.value.message)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_column_schema_enforcement_no_col_names():
     m = Model()
     input_schema = Schema([ColSpec("double"), ColSpec("double"), ColSpec("double")])
     m.signature = ModelSignature(inputs=input_schema)
     aml_model = AMLGenericModel(model_meta=m, model_impl=TestModel())
     test_data = [[1.0, 2.0, 3.0]]
 
@@ -553,14 +561,15 @@
         aml_model.predict(set([1, 2, 3]))
 
     # 9. dictionaries of str -> list/nparray work
     d = {"a": [1.0], "b": [2.0], "c": [3.0]}
     assert aml_model.predict(d).equals(pd.DataFrame(d))
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_tensor_schema_enforcement_no_col_names():
     m = Model()
     input_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 3))])
     m.signature = ModelSignature(inputs=input_schema)
     aml_model = AMLGenericModel(model_meta=m, model_impl=TestModel())
     test_data = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=np.float32)
 
@@ -604,14 +613,15 @@
     # Can not call with an empty ndarray
     with pytest.raises(
             MlflowException, match=re.escape("Shape of input () does not match expected shape (-1, 3)")
     ):
         aml_model.predict(np.ndarray([]))
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_model_log_load(sklearn_knn_model, iris_data, tmpdir):
     sk_model_path = os.path.join(str(tmpdir), "knn.pkl")
     with open(sk_model_path, "wb") as f:
         pickle.dump(sklearn_knn_model, f)
 
     aml_artifact_path = "aml_model"
     with mlflow.start_run():
@@ -633,14 +643,15 @@
     reloaded_model = azureml_mlflow.aml.load_model(aml_model_path)
     assert model_config.to_yaml() == reloaded_model.metadata.to_yaml()
     np.testing.assert_array_equal(
         sklearn_knn_model.predict(iris_data[0]), reloaded_model.predict(iris_data[0])
     )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_model_log_load_no_active_run(sklearn_knn_model, iris_data, tmpdir):
     sk_model_path = os.path.join(str(tmpdir), "knn.pkl")
     with open(sk_model_path, "wb") as f:
         pickle.dump(sklearn_knn_model, f)
 
     aml_artifact_path = "aml_model"
     assert azureml_mlflow.active_run() is None
@@ -662,28 +673,31 @@
     reloaded_model = azureml_mlflow.aml.load_model(aml_model_path)
     np.testing.assert_array_equal(
         sklearn_knn_model.predict(iris_data[0]), reloaded_model.predict(iris_data[0])
     )
     azureml_mlflow.end_run()
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_save_model_with_unsupported_argument_combinations_throws_exception(model_path):
     with pytest.raises(
             MlflowException, match="Either `loader_module` or `aml_model` must be specified"
     ):
         azureml_mlflow.aml.save_model(path=model_path, data_path="/path/to/data")
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_log_model_with_unsupported_argument_combinations_throws_exception():
     with mlflow.start_run(), pytest.raises(
             MlflowException, match="Either `loader_module` or `aml_model` must be specified"
     ):
         azureml_mlflow.aml.log_model(artifact_path="aml_model", data_path="/path/to/data")
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_log_model_persists_specified_conda_env_file_in_mlflow_model_directory(
         sklearn_knn_model, tmpdir, aml_custom_env_file
 ):
     sk_model_path = os.path.join(str(tmpdir), "knn.pkl")
     with open(sk_model_path, "wb") as f:
         pickle.dump(sklearn_knn_model, f)
 
@@ -712,14 +726,15 @@
     with open(aml_custom_env_file, "r") as f:
         aml_custom_env_parsed = yaml.safe_load(f)
     with open(saved_conda_env_path, "r") as f:
         saved_conda_env_parsed = yaml.safe_load(f)
     assert saved_conda_env_parsed == aml_custom_env_parsed
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_log_model_persists_specified_conda_env_dict_in_mlflow_model_directory(
         sklearn_knn_model, tmpdir, aml_custom_env_dict
 ):
     sk_model_path = os.path.join(str(tmpdir), "knn.pkl")
     with open(sk_model_path, "wb") as f:
         pickle.dump(sklearn_knn_model, f)
 
@@ -745,14 +760,15 @@
     assert os.path.exists(saved_conda_env_path)
 
     with open(saved_conda_env_path, "r") as f:
         saved_conda_env_parsed = yaml.safe_load(f)
     assert saved_conda_env_parsed == aml_custom_env_dict
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_log_model_persists_requirements_in_mlflow_model_directory(
         sklearn_knn_model, tmpdir, aml_custom_env_dict
 ):
     sk_model_path = os.path.join(str(tmpdir), "knn.pkl")
     with open(sk_model_path, "wb") as f:
         pickle.dump(sklearn_knn_model, f)
 
@@ -776,14 +792,15 @@
 
     with open(saved_pip_req_path, "r") as f:
         requirements = f.read().split("\n")
 
     assert aml_custom_env_dict["dependencies"][-1]["pip"] == requirements
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_log_model_without_specified_conda_env_uses_default_env_with_expected_dependencies(
         sklearn_knn_model, tmpdir
 ):
     sk_model_path = os.path.join(str(tmpdir), "knn.pkl")
     with open(sk_model_path, "wb") as f:
         pickle.dump(sklearn_knn_model, f)
```

## tests/hftransformers/test_hf_load_pipeline.py

```diff
@@ -19,14 +19,15 @@
     return os.path.join(str(tmpdir), subdir)
 
 
 def extra_files():
     pass
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_multiclass_model_save_load_pipeline(newsgroup_dataset, model_path):  # noqa: F811
     config = AutoConfig.from_pretrained("bert-base-uncased", num_labels=4, output_attentions=False,
                                         output_hidden_states=False,
                                         train_label_list=np.unique(newsgroup_dataset._constructor_args['targets']))
     model = AutoModelForSequenceClassification.from_pretrained(
         "bert-base-uncased",
         config=config
@@ -63,14 +64,15 @@
     for i in range(len(test_dataset["context"])):
         context.append(test_dataset["context"][i])
         questions.append(test_dataset["question"][i])
     df = pd.DataFrame({'question': questions, 'context': context})
     return df
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_hf_qna_model_save_load_predict(dataset_squad_qna, model_path):
     model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'
     config = AutoConfig.from_pretrained(model_name)
     model = AutoModelForQuestionAnswering.from_pretrained(model_name, config=config)
     tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True)
     task_type = 'question-answering'
     predictor = pipeline(task=task_type, model=model, tokenizer=tokenizer, config=config)
```

## tests/hftransformers/test_hf_model_export.py

```diff
@@ -22,23 +22,25 @@
 import pandas as pd
 from transformers import pipeline
 from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
 import torch
 import io
 import base64
 
+
 @pytest.fixture
 def model_path(tmpdir, subdir="model"):
     return os.path.join(str(tmpdir), subdir)
 
 
 def extra_files():
     pass
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_multiclass_model_save_load(newsgroup_dataset, model_path):  # noqa: F811
     config = AutoConfig.from_pretrained("bert-base-uncased", num_labels=4, output_attentions=False,
                                         output_hidden_states=False,
                                         train_label_list=np.unique(newsgroup_dataset._constructor_args['targets']))
     model = AutoModelForSequenceClassification.from_pretrained(
         "bert-base-uncased",
         config=config
@@ -76,14 +78,15 @@
     pyfunc_preds = pyfunc_preds[pyfunc_preds.columns[0]].values
     # ToDo: Check this, not working after latest change
     # np.testing.assert_array_equal(
     #     pyfunc_preds, predicted_labels
     # )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_multiclass_model_save_load_with_extra_files(newsgroup_dataset, model_path):  # noqa: F811
     config = AutoConfig.from_pretrained("bert-base-uncased", num_labels=4, output_attentions=False,
                                         output_hidden_states=False,
                                         train_label_list=np.unique(newsgroup_dataset._constructor_args['targets']))
     model = AutoModelForSequenceClassification.from_pretrained(
         "bert-base-uncased",
         config=config
@@ -160,14 +163,15 @@
     for i in range(len(test_dataset["context"])):
         context.append(test_dataset["context"][i])
         questions.append(test_dataset["question"][i])
     df = pd.DataFrame({'question': questions, 'context': context})
     return df
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_hf_translation_model_save_load_predict(dataset_opus, model_path):
     model_name = 't5-small'
     source_lang, target_lang = 'en', 'fr'
     config = AutoConfig.from_pretrained(model_name)
     model = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config)
     tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True)
     # df = newsgroup_dataset._constructor_args["data"]
@@ -194,14 +198,15 @@
     pyfunc_loaded = mlflow.pyfunc.load_model(model_path)
     pyfunc_preds = pyfunc_loaded.predict(dataset_opus['X'])
     np.testing.assert_array_equal(
         pyfunc_preds, result1
     )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_hf_summarization_model_save_load_predict(dataset_billsum, model_path):
     model_name = 't5-small'
     config = AutoConfig.from_pretrained(model_name)
     model = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config)
     tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True)
     task_type = 'summarization'
     predictor = pipeline(task=task_type, model=model, tokenizer=tokenizer, config=config)
@@ -222,14 +227,15 @@
     pyfunc_loaded = mlflow.pyfunc.load_model(model_path)
     pyfunc_preds = pyfunc_loaded.predict(dataset_billsum)
     np.testing.assert_array_equal(
         pyfunc_preds[pyfunc_preds.columns[0]].to_list(), result1
     )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_hf_qna_model_save_load_predict(dataset_squad_qna, model_path):
     model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'
     config = AutoConfig.from_pretrained(model_name)
     model = AutoModelForQuestionAnswering.from_pretrained(model_name, config=config)
     tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True)
     task_type = 'question-answering'
     predictor = pipeline(task=task_type, model=model, tokenizer=tokenizer, config=config)
@@ -250,14 +256,15 @@
     pyfunc_loaded = mlflow.pyfunc.load_model(model_path)
     pyfunc_preds = pyfunc_loaded.predict(dataset_squad_qna)
     np.testing.assert_array_equal(
         pyfunc_preds[pyfunc_preds.columns[0]].to_list(), result1
     )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_hf_fill_mask_model_save_load_predict(model_path):
     model_name = "distilbert-base-cased"
     config = AutoConfig.from_pretrained(model_name)
     model = AutoModelForMaskedLM.from_pretrained(model_name, config=config)
     tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True)
     data = pd.DataFrame({
         'X': [f"HuggingFace is creating a {tokenizer.mask_token} that the community uses to solve NLP tasks.",
@@ -286,14 +293,15 @@
         pyfunc_preds[pyfunc_preds.columns[0]].to_list(), result1
     )
     np.testing.assert_array_equal(
         result1, pyfunc_preds1
     )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_hf_fill_mask_model_save_load_predict_single_example(model_path):
     model_name = "distilbert-base-cased"
     config = AutoConfig.from_pretrained(model_name)
     model = AutoModelForMaskedLM.from_pretrained(model_name, config=config)
     tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True)
     data = pd.DataFrame({
         'X': [f"HuggingFace is creating a {tokenizer.mask_token} that the community uses to solve NLP tasks."]
@@ -320,14 +328,15 @@
         pyfunc_preds[pyfunc_preds.columns[0]].to_list(), result1
     )
     np.testing.assert_array_equal(
         result1, pyfunc_preds1
     )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_hf_fill_mask_model_save_load_predict_multi_example_multi_mask(model_path):
     model_name = "distilbert-base-cased"
     config = AutoConfig.from_pretrained(model_name)
     model = AutoModelForMaskedLM.from_pretrained(model_name, config=config)
     tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True)
     data = pd.DataFrame({
         'X': [
@@ -358,14 +367,15 @@
     #     pyfunc_preds[pyfunc_preds.columns[0]].to_list(), result1
     # )
     # np.testing.assert_array_equal(
     #     result1, pyfunc_preds1
     # )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_hf_text_generation_model_save_load_predict(model_path):
     model_name = "xlnet-base-cased"
     config = AutoConfig.from_pretrained(model_name)
     model = AutoModelForCausalLM.from_pretrained(model_name, config=config)
     tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True)
     task_type = 'text-generation'
     data = pd.DataFrame({'text': ["As far as I am concerned, I will",
@@ -387,14 +397,15 @@
     pyfunc_loaded = mlflow.pyfunc.load_model(model_path)
     pyfunc_preds = pyfunc_loaded.predict(data['text'])
     # np.testing.assert_array_equal(
     #     pyfunc_preds, outputs2
     # )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_multiclass_model_save_load_with_paths(newsgroup_dataset, model_path):  # noqa: F811
     config = AutoConfig.from_pretrained("bert-base-uncased", num_labels=4, output_attentions=False,
                                         output_hidden_states=False,
                                         train_label_list=np.unique(newsgroup_dataset._constructor_args['targets']))
     model = AutoModelForSequenceClassification.from_pretrained(
         "bert-base-uncased",
         config=config
@@ -443,14 +454,15 @@
     temp_dir.cleanup()
     # ToDo: Check this, not working after latest change
     # np.testing.assert_array_equal(
     #     pyfunc_preds, predicted_labels
     # )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_multiclass_model_save_load_without_task_and_single_path(newsgroup_dataset, model_path):  # noqa: F811
     config = AutoConfig.from_pretrained("bert-base-uncased", num_labels=4, output_attentions=False,
                                         output_hidden_states=False,
                                         train_label_list=np.unique(newsgroup_dataset._constructor_args['targets']))
     model = AutoModelForSequenceClassification.from_pretrained(
         "bert-base-uncased",
         config=config
@@ -488,14 +500,15 @@
     pyfunc_loaded = mlflow.pyfunc.load_model(model_path)
     with pytest.raises(Exception) as exc:
         pyfunc_loaded.predict(dataset_wrapper.data)
     assert str(exc.value) == "task=None not supported"
     temp_dir.cleanup()
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_multiclass_model_predict_with_preprocess_script(newsgroup_dataset, model_path):  # noqa: F811
     config = AutoConfig.from_pretrained("bert-base-uncased", num_labels=4, output_attentions=False,
                                         output_hidden_states=False)
     model = AutoModelForSequenceClassification.from_pretrained(
         "bert-base-uncased",
         config=config
     )
@@ -533,14 +546,15 @@
     pyfunc_preds = pyfunc_loaded.predict(dataset_wrapper.data)
     pyfunc_preds = pyfunc_preds[pyfunc_preds.columns[0]].values
     np.testing.assert_array_equal(
         pyfunc_preds, predicted_labels
     )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_multiclass_model_predict_with_preprocess_script(newsgroup_dataset, model_path):  # noqa: F811
     config = AutoConfig.from_pretrained("bert-base-uncased", num_labels=4, output_attentions=False,
                                         output_hidden_states=False)
     model = AutoModelForSequenceClassification.from_pretrained(
         "bert-base-uncased",
         config=config
     )
@@ -578,14 +592,15 @@
     pyfunc_preds = pyfunc_loaded.predict(dataset_wrapper.data)
     pyfunc_preds = pyfunc_preds[pyfunc_preds.columns[0]].values
     np.testing.assert_array_equal(
         pyfunc_preds, predicted_labels
     )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_multiclass_model_predict_with_predict_script(newsgroup_dataset, model_path):  # noqa: F811
     config = AutoConfig.from_pretrained("bert-base-uncased", num_labels=4, output_attentions=False,
                                         output_hidden_states=False)
     model = AutoModelForSequenceClassification.from_pretrained(
         "bert-base-uncased",
         config=config
     )
@@ -624,46 +639,50 @@
     pyfunc_preds = pyfunc_loaded.predict(dataset_wrapper.data)
     pyfunc_preds = pyfunc_preds[pyfunc_preds.columns[0]].values
     np.testing.assert_array_equal(
         pyfunc_preds, predicted_labels
     )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_hf_tts_model_save_load_predict(model_path):
     model_name = "openai/whisper-large"
     ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
     from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig
     config = WhisperConfig.from_pretrained(model_name)
     model = WhisperForConditionalGeneration.from_pretrained(model_name, config=config)
     processor = WhisperProcessor.from_pretrained(model_name, padding=True, truncation=True)
     task_type = 'automatic-speech-recognition'
     audio = ds["audio"][:5]
     data = pd.DataFrame([{'raw': item["array"], 'sampling_rate': item["sampling_rate"]} for item in audio])
-    predictor = pipeline(task=task_type, model=model, tokenizer=processor.tokenizer, config=config, feature_extractor=processor.feature_extractor)
+    predictor = pipeline(task=task_type, model=model, tokenizer=processor.tokenizer, config=config,
+                         feature_extractor=processor.feature_extractor)
     outputs = predictor(data.to_dict('records'))
     hf_conf = {
         'task_type': task_type
     }
     mlflow.hftransformers.save_model(model, model_path, tokenizer=processor, config=config, hf_conf=hf_conf)
     pyfunc_loaded = mlflow.pyfunc.load_model(model_path)
     pyfunc_preds = pyfunc_loaded.predict(data)
 
     reloaded_task_type, r_model, r_processor, r_config = mlflow.hftransformers.load_model(model_path)
-    predictor = pipeline(task=task_type, model=r_model, tokenizer=r_processor.tokenizer, config=r_config, feature_extractor=r_processor.feature_extractor)
+    predictor = pipeline(task=task_type, model=r_model, tokenizer=r_processor.tokenizer, config=r_config,
+                         feature_extractor=r_processor.feature_extractor)
     outputs2 = predictor(data.to_dict('records'))
     result2 = [output['text'] for output in outputs2]
 
     np.testing.assert_array_equal(
         pyfunc_preds[pyfunc_preds.columns[0]].to_list(), result2
     )
     np.testing.assert_array_equal(
         outputs2, outputs
     )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_hf_text_2_image_model_save_load_predict(model_path):
     model_name = "CompVis/stable-diffusion-v1-4"
     dataset = load_dataset("lambdalabs/pokemon-blip-captions")
 
     task_type = "text-to-image"
     hf_conf = {
         'task_type': task_type,
```

## tests/models/test_azureml_evaluator.py

```diff
@@ -1,13 +1,15 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 # flake8: noqa
 import ast
 
+import pytest
+
 from azureml.metrics.azureml_regression_metrics import AzureMLRegressionMetrics
 import numpy as np
 import pandas as pd
 from azureml.metrics import compute_metrics, constants
 import azureml.evaluate.mlflow as mlflow
 
 from azureml.evaluate.mlflow.models.evaluation import evaluate
@@ -49,21 +51,21 @@
 )
 from azureml.metrics.azureml_forecasting_metrics import AzureMLForecastingMetrics
 from azureml.evaluate.mlflow.constants import ForecastFlavors
 from azureml.evaluate.mlflow.models.evaluation.base import EvaluationDataset
 from pandas.tseries.frequencies import to_offset
 
 
-
 def assert_dict_equal(d1, d2, rtol):
     for k in d1:
         assert k in d2
         assert np.isclose(d1[k], d2[k], rtol=rtol)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_regressor_evaluation(linear_regressor_model_uri, diabetes_dataset):  # noqa: F811
     '''
     format of result needs to be changed
     2) non-scalar metrics should be part of metrics? -> residuals, predicted_true
     '''
     # mlflow.set_tracking_uri("azureml://eastus2.api.azureml.ms/mlflow/v1.0/subscriptions/72c03bf3-4e69-41af-9532"
     #                         "-dfcdc3eefef4/resourceGroups/shared-model-evaluation-rg/providers/Microsoft"
@@ -91,14 +93,15 @@
             assert np.isclose(
                 expected_metrics[metric_key],
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_regressor_evaluation_mlflow_model(linear_regressor_model_uri,
                                            diabetes_dataset):  # noqa: F811
     '''
     format of result needs to be changed
     2) non-scalar metrics should be part of metrics? -> residuals, predicted_true
     '''
     linear_regressor_model_mlflow = mlflow.aml.load_model(linear_regressor_model_uri)
@@ -121,17 +124,18 @@
         if np.isscalar(expected_metrics[metric_key]):
             assert np.isclose(
                 expected_metrics[metric_key],
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
+
 def _do_test_forecaster_evaluation(forecaster_model_uri,
                                    forecast_flavour
-                                   ): # noqa: F811
+                                   ):  # noqa: F811
     """Test the forecasting evaluation using model uri."""
     X_train, y_train, X_test, y_test = get_diabetes_dataset_timeseries()
     cfg = {
         ForecastFlavors.FLAVOUR: forecast_flavour,
         'X_train': X_train,
         'y_train': y_train,
     }
@@ -145,15 +149,15 @@
         np.random.seed(42)
         y_test = np.concatenate([y_test, y_test + np.random.rand(len(y_test))])
     X_test['y'] = y_test
 
     constructor_args = {"data": X_test, "targets": "y", "name": "diabetes_dataset_timeseries"}
     diabetes_dataset_timeseries = EvaluationDataset(**constructor_args)
     diabetes_dataset_timeseries._constructor_args = constructor_args
-    #diabetes_dataset_timeseries = 
+    # diabetes_dataset_timeseries =
     with mlflow.start_run() as run:
         result = evaluate(
             forecaster_model_uri,
             diabetes_dataset_timeseries._constructor_args["data"],
             model_type="forecaster",
             targets=diabetes_dataset_timeseries._constructor_args["targets"],
             dataset_name=diabetes_dataset_timeseries.name,
@@ -168,42 +172,48 @@
     y = diabetes_dataset_timeseries.labels_data
     if forecast_flavour == ForecastFlavors.ROLLING_FORECAST:
         X_test = model.rolling_forecast(diabetes_dataset_timeseries.features_data, y, step=2)
         y_pred = X_test.pop(model._model_impl.forecast_column_name).values
         y = X_test.pop(model._model_impl.actual_column_name).values
     else:
         y_pred, _ = model.forecast(diabetes_dataset_timeseries.features_data)
-        
+
     metric = AzureMLForecastingMetrics(
         X_train=X_train,
         y_train=y_train,
         y_std=np.std(y_train),
         time_column_name='date'
-        )
+    )
     expected_metrics = metric.compute(
         y_test=y, y_pred=y_pred,
         X_test=X_test)
     for metric_key in expected_metrics:
         if np.isscalar(expected_metrics[metric_key]):
             assert np.isclose(
                 expected_metrics[metric_key],
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
-def test_forecaster_evaluation(forecaster_model_uri): # noqa: F811):
+
+@pytest.mark.usefixtures("new_clean_dir")
+def test_forecaster_evaluation(forecaster_model_uri):  # noqa: F811):
     """Test evaluation on regular forecast."""
     _do_test_forecaster_evaluation(
         forecaster_model_uri, ForecastFlavors.RECURSIVE_FORECAST)
 
-def test_forecaster_evaluation_rolling(forecaster_model_uri): # noqa: F811):
+
+@pytest.mark.usefixtures("new_clean_dir")
+def test_forecaster_evaluation_rolling(forecaster_model_uri):  # noqa: F811):
     """Test evaluation on regular forecast."""
     _do_test_forecaster_evaluation(
         forecaster_model_uri, ForecastFlavors.ROLLING_FORECAST)
 
+
+@pytest.mark.usefixtures("new_clean_dir")
 def test_forecaster_evaluation_mlflow_model(forecaster_model_uri,
                                             diabetes_dataset_timeseries):
     """Test the forecasting evaluation using model instance."""
     forecaster_model_mlflow = mlflow.aml.load_model(forecaster_model_uri)
     with mlflow.start_run() as run:
         result = evaluate(
             forecaster_model_mlflow,
@@ -226,14 +236,15 @@
             assert np.isclose(
                 expected_metrics[metric_key],
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_multi_classifier_evaluation(multiclass_logistic_regressor_model_uri,
                                      iris_dataset):  # noqa: F811
     metrics_args = {
         "class_labels": np.unique(iris_dataset.labels_data),
         "train_labels": np.unique(iris_dataset.labels_data)
     }
     with mlflow.start_run() as run:
@@ -262,14 +273,15 @@
             assert np.isclose(
                 expected_metrics[metric_key],
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_multiclass_llm_evaluation(multiclass_llm_model_uri, newsgroup_dataset):  # noqa: F811
     metrics_args = {
         "class_labels": np.unique(newsgroup_dataset.labels_data),
         "train_labels": np.unique(newsgroup_dataset.labels_data)
     }
     with mlflow.start_run() as run:
         result = evaluate(
@@ -298,14 +310,15 @@
             assert np.isclose(
                 expected_metrics[metric_key],
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_multiclass_llm_evaluation_text_pair(multiclass_llm_model_uri, newsgroup_dataset_text_pair):  # noqa: F811
     metrics_args = {
         "class_labels": np.unique(newsgroup_dataset_text_pair.labels_data),
         "train_labels": np.unique(newsgroup_dataset_text_pair.labels_data)
     }
     with mlflow.start_run() as run:
         result = evaluate(
@@ -334,14 +347,15 @@
             assert np.isclose(
                 expected_metrics[metric_key],
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_multilabel_llm_evaluation(multilabel_llm_model_uri, arxiv_dataset,
                                    y_transformer_arxiv):  # noqa: F811
     metrics_args = {
         # "class_labels": np.array(y_transformer_arxiv.classes_),
         # "train_labels": np.array(y_transformer_arxiv.classes_),
         "multilabel": True
         # "y_transformer": y_transformer_arxiv
@@ -372,14 +386,15 @@
             assert np.isclose(
                 expected_metrics[metric_key],
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_ner_llm_evaluation(ner_llm_model_uri, ner_dataset):  # noqa: F811
     _, _, labels_list = get_connll_dataset()
     metrics_args = {
         'train_label_list': labels_list,
         'label_list': labels_list
     }
     with mlflow.start_run() as run:
@@ -406,28 +421,30 @@
             assert np.isclose(
                 expected_metrics[metric_key],
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_parse_aml_tracking_uri():
     current_tracking_uri = mlflow.get_tracking_uri()
     mlflow.set_tracking_uri("azureml://eastus2.api.azureml.ms/mlflow/v1.0/subscriptions/72c03bf3-4e69-41af-9532"
                             "-dfcdc3eefef4/resourceGroups/shared-model-evaluation-rg/providers/Microsoft"
                             ".MachineLearningServices/workspaces/aml-shared-model-evaluation-ws")
     from azureml.evaluate.mlflow.models.evaluation.azureml.azureml_evaluator import AzureMLEvaluator
     azureml_evaluator = AzureMLEvaluator()
     workspace, resource_group, subscription = azureml_evaluator._parse_aml_tracking_uri()
     assert workspace == "aml-shared-model-evaluation-ws"
     assert resource_group == "shared-model-evaluation-rg"
     assert subscription == "72c03bf3-4e69-41af-9532-dfcdc3eefef4"
     mlflow.set_tracking_uri(current_tracking_uri)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_summarization_llm_evaluation(summarization_llm_model_uri, billsum_dataset):  # noqa: F811
     metrics_args = {
 
     }
     with mlflow.start_run() as run:
         result = evaluate(
             summarization_llm_model_uri,
@@ -454,14 +471,15 @@
             assert np.isclose(
                 expected_metrics[metric_key],
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_translation_llm_evaluation(translation_llm_model_uri, opus_dataset):  # noqa: F811
     metrics_args = {
 
     }
     with mlflow.start_run() as run:
         result = evaluate(
             translation_llm_model_uri,
@@ -489,14 +507,15 @@
             assert np.isclose(
                 expected_metrics[metric_key],
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_qna_llm_evaluation(qna_llm_model_uri, squad_qna_dataset):  # noqa: F811
     metrics_args = {
 
     }
     with mlflow.start_run() as run:
         result = evaluate(
             qna_llm_model_uri,
@@ -522,14 +541,16 @@
         if np.isscalar(expected_metrics[metric_key]):
             assert np.isclose(
                 expected_metrics[metric_key],
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
+
+@pytest.mark.usefixtures("new_clean_dir")
 def test_text_generation_llm_evaluation(text_generation_llm_model_uri, text_gen_data):  # noqa: F811
     metrics_args = {
 
     }
     with mlflow.start_run() as run:
         result = evaluate(
             text_generation_llm_model_uri,
@@ -555,14 +576,16 @@
         if np.isscalar(expected_metrics[metric_key]):
             assert np.isclose(
                 expected_metrics[metric_key],
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
+
+@pytest.mark.usefixtures("new_clean_dir")
 def test_fill_mask_llm_evaluation(fill_mask_llm_model_uri, wiki_mask):  # noqa: F811
     metrics_args = {
 
     }
     with mlflow.start_run() as run:
         result = evaluate(
             fill_mask_llm_model_uri,
@@ -589,14 +612,15 @@
             assert np.isclose(
                 expected_metrics[metric_key],
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_image_classifier_evaluation(image_mc_classification_model_uri, image_net):  # noqa: F811
     with mlflow.start_run() as run:
         result = evaluate(
             image_mc_classification_model_uri,
             image_net._constructor_args["data"],
             model_type="image-classifier",
             targets=image_net._constructor_args["targets"],
@@ -615,27 +639,28 @@
             assert np.isclose(
                 expected_metric_value,
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_image_classifier_multilabel_evaluation(image_ml_classification_model_uri, fridge_multi_label):  # noqa: F811
     with mlflow.start_run() as run:
         metrics_args = {
             "multilabel": True
         }
         result = evaluate(
             image_ml_classification_model_uri,
             fridge_multi_label._constructor_args["data"],
             model_type="image-classifier-multilabel",
             targets=fridge_multi_label._constructor_args["targets"],
             dataset_name=fridge_multi_label.name,
             evaluators="azureml",
-            evaluator_config = metrics_args
+            evaluator_config=metrics_args
         )
 
     model = mlflow.aml.load_model(image_ml_classification_model_uri, "image-classifier-multilabel")
 
     y = fridge_multi_label.labels_data
     y_pred = model.predict(fridge_multi_label.features_data)["labels"]
 
@@ -651,13 +676,15 @@
             assert np.isclose(
                 expected_metric_value,
                 result.metrics[metric_key],
                 rtol=1e-3,
             )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_load_model():
     pass
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_log_predictions():
     pass
```

## tests/models/test_default_evaluator.py

```diff
@@ -59,14 +59,15 @@
 
 def assert_dict_equal(d1, d2, rtol):
     for k in d1:
         assert k in d2
         assert np.isclose(d1[k], d2[k], rtol=rtol)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_regressor_evaluation(linear_regressor_model_uri, diabetes_dataset):  # pylint: disable=unused-import
     with mlflow.start_run() as run:
         result = evaluate(
             linear_regressor_model_uri,
             diabetes_dataset._constructor_args["data"],
             model_type="regressor",
             targets=diabetes_dataset._constructor_args["targets"],
@@ -105,14 +106,15 @@
     # assert result.artifacts.keys() == {
     #     "shap_beeswarm_plot",
     #     "shap_feature_importance_plot",
     #     "shap_summary_plot",
     # }
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_multi_classifier_evaluation(multiclass_logistic_regressor_model_uri,
                                      iris_dataset):  # pylint: disable=unused-import
     with mlflow.start_run() as run:
         result = evaluate(
             multiclass_logistic_regressor_model_uri,
             iris_dataset._constructor_args["data"],
             model_type="classifier",
@@ -162,14 +164,15 @@
     #     "confusion_matrix",
     #     'shap_beeswarm_plot',
     #     'shap_summary_plot',
     #     'shap_feature_importance_plot'
     # }
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_bin_classifier_evaluation(binary_logistic_regressor_model_uri,
                                    breast_cancer_dataset):  # pylint: disable=unused-import
     with mlflow.start_run() as run:
         result = evaluate(
             binary_logistic_regressor_model_uri,
             breast_cancer_dataset._constructor_args["data"],
             model_type="classifier",
@@ -260,14 +263,15 @@
 #         {**diabetes_spark_dataset._metadata, "model": model.metadata.model_uuid}
 #     ]
 #
 #     assert set(artifacts) == set()
 #     assert result.artifacts == {}
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_svm_classifier_evaluation(svm_model_uri, breast_cancer_dataset):  # pylint: disable=unused-import
     with mlflow.start_run() as run:
         result = evaluate(
             svm_model_uri,
             breast_cancer_dataset._constructor_args["data"],
             model_type="classifier",
             targets=breast_cancer_dataset._constructor_args["targets"],
@@ -311,14 +315,15 @@
     #     "confusion_matrix",
     #     'shap_beeswarm_plot',
     #     'shap_summary_plot',
     #     'shap_feature_importance_plot'
     # }
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_pipeline_model_kernel_explainer_on_categorical_features(pipeline_model_uri):  # pylint: disable=unused-import
     data, target_col = get_pipeline_model_dataset()
     with mlflow.start_run() as run:
         evaluate(
             pipeline_model_uri,
             data[0::3],
             model_type="classifier",
@@ -331,14 +336,15 @@
     assert {
         "shap_beeswarm_plot_on_data_pipeline_model_dataset.png",
         "shap_feature_importance_plot_on_data_pipeline_model_dataset.png",
         "shap_summary_plot_on_data_pipeline_model_dataset.png",
     }.issubset(run_data.artifacts)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_compute_df_mode_or_mean():
     df = pd.DataFrame(
         {
             "a": [2.0, 2.0, 5.0],
             "b": [3, 3, 5],
             "c": [2.0, 2.0, 6.5],
             "d": [True, False, True],
@@ -375,23 +381,25 @@
             "d": [True, False, True],
             "g": ["ab", "ab", None],
         }
     )
     assert _compute_df_mode_or_mean(df2) == {"d": True, "g": "ab"}
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_infer_model_type_by_labels():
     assert _infer_model_type_by_labels(["a", "b"]) == "classifier"
     assert _infer_model_type_by_labels([True, False]) == "classifier"
     assert _infer_model_type_by_labels([1, 2.5]) == "regressor"
     assert _infer_model_type_by_labels(pd.Series(["a", "b"], dtype="category")) == "classifier"
     assert _infer_model_type_by_labels(pd.Series([1.5, 2.5], dtype="category")) == "classifier"
     assert _infer_model_type_by_labels([1, 2, 3]) is None
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_extract_raw_model_and_predict_fn(
         binary_logistic_regressor_model_uri, breast_cancer_dataset  # pylint: disable=unused-import
 ):
     model = mlflow.pyfunc.load_model(binary_logistic_regressor_model_uri)
 
     model_loader_module, raw_model = _extract_raw_model(model)
     predict_fn, predict_proba_fn = _extract_predict_fn(model, raw_model)
@@ -404,14 +412,15 @@
     )
     assert np.allclose(
         predict_proba_fn(breast_cancer_dataset.features_data),
         raw_model.predict_proba(breast_cancer_dataset.features_data),
     )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_get_regressor_metrics():
     y = [1.1, 2.1, -3.5]
     y_pred = [1.5, 2.0, -3.0]
 
     metrics = _get_regressor_metrics(y, y_pred)
     expected_metrics = {
         "example_count": 3,
@@ -423,14 +432,15 @@
         "r2_score": 0.976457399103139,
         "max_error": 0.5,
         "mean_absolute_percentage_error": 0.18470418470418468,
     }
     assert_dict_equal(metrics, expected_metrics, rtol=1e-3)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_get_binary_sum_up_label_pred_prob():
     y = [0, 1, 2]
     y_pred = [0, 2, 1]
     y_probs = [[0.7, 0.1, 0.2], [0.2, 0.3, 0.5], [0.25, 0.4, 0.35]]
 
     results = []
     for idx, label in enumerate([0, 1, 2]):
@@ -442,14 +452,15 @@
     assert results == [
         ([1, 0, 0], [1, 0, 0], [0.7, 0.2, 0.25]),
         ([0, 1, 0], [0, 0, 1], [0.1, 0.3, 0.4]),
         ([0, 0, 1], [0, 1, 0], [0.2, 0.5, 0.35]),
     ]
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_get_classifier_per_class_metrics():
     y = [0, 1, 0, 1, 0, 1, 0, 1, 1, 0]
     y_pred = [0, 1, 1, 0, 1, 1, 0, 1, 1, 0]
 
     expected_metrics = {
         "true_negatives": 3,
         "false_positives": 2,
@@ -459,14 +470,15 @@
         "precision": 0.6666666666666666,
         "f1_score": 0.7272727272727272,
     }
     metrics = _get_classifier_per_class_metrics(y, y_pred)
     assert_dict_equal(metrics, expected_metrics, rtol=1e-3)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_multiclass_get_classifier_global_metrics():
     y = [0, 1, 2, 1, 2]
     y_pred = [0, 2, 1, 1, 0]
     y_probs = [
         [0.7, 0.1, 0.2],
         [0.2, 0.3, 0.5],
         [0.25, 0.4, 0.35],
@@ -483,26 +495,28 @@
         "f1_score_micro": 0.4,
         "f1_score_macro": 0.38888888888888884,
         "log_loss": 1.1658691395263094,
     }
     assert_dict_equal(metrics, expected_metrics, 1e-3)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_binary_get_classifier_global_metrics():
     y = [0, 1, 0, 1, 0, 1, 0, 1, 1, 0]
     y_pred = [0, 1, 1, 0, 1, 1, 0, 1, 1, 0]
     y_prob = [0.1, 0.9, 0.8, 0.2, 0.7, 0.8, 0.3, 0.6, 0.65, 0.4]
     y_probs = [[1 - p, p] for p in y_prob]
     metrics = _get_classifier_global_metrics(
         is_binomial=True, y=y, y_pred=y_pred, y_probs=y_probs, labels=[0, 1]
     )
     expected_metrics = {"accuracy": 0.7, "example_count": 10, "log_loss": 0.6665822319387167}
     assert_dict_equal(metrics, expected_metrics, 1e-3)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_gen_binary_precision_recall_curve():
     y = [0, 1, 0, 1, 0, 1, 0, 1, 1, 0]
     y_prob = [0.1, 0.9, 0.8, 0.2, 0.7, 0.8, 0.3, 0.6, 0.65, 0.4]
 
     results = _gen_classifier_curve(
         is_binomial=True, y=y, y_probs=y_prob, labels=[0, 1], curve_type="pr"
     )
@@ -518,14 +532,15 @@
     )
     assert results.plot_fn_args["xlabel"] == "recall"
     assert results.plot_fn_args["ylabel"] == "precision"
     assert results.plot_fn_args["line_kwargs"] == {"drawstyle": "steps-post", "linewidth": 1}
     assert np.isclose(results.auc, 0.7088888888888889, rtol=1e-3)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_gen_binary_roc_curve():
     y = [0, 1, 0, 1, 0, 1, 0, 1, 1, 0]
     y_prob = [0.1, 0.9, 0.8, 0.2, 0.7, 0.8, 0.3, 0.6, 0.65, 0.4]
 
     results = _gen_classifier_curve(
         is_binomial=True, y=y, y_probs=y_prob, labels=[0, 1], curve_type="roc"
     )
@@ -541,14 +556,15 @@
     )
     assert results.plot_fn_args["xlabel"] == "False Positive Rate"
     assert results.plot_fn_args["ylabel"] == "True Positive Rate"
     assert results.plot_fn_args["line_kwargs"] == {"drawstyle": "steps-post", "linewidth": 1}
     assert np.isclose(results.auc, 0.66, rtol=1e-3)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_gen_multiclass_precision_recall_curve():
     y = [0, 1, 2, 1, 2]
     y_probs = [
         [0.7, 0.1, 0.2],
         [0.2, 0.3, 0.5],
         [0.25, 0.4, 0.35],
         [0.3, 0.4, 0.3],
@@ -574,14 +590,15 @@
     assert results.plot_fn_args["ylabel"] == "precision"
     assert results.plot_fn_args["line_kwargs"] == {"drawstyle": "steps-post", "linewidth": 1}
 
     expected_auc = [0.25, 0.6666666666666666, 0.2875]
     assert np.allclose(results.auc, expected_auc, rtol=1e-3)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_gen_multiclass_roc_curve():
     y = [0, 1, 2, 1, 2]
     y_probs = [
         [0.7, 0.1, 0.2],
         [0.2, 0.3, 0.5],
         [0.25, 0.4, 0.35],
         [0.3, 0.4, 0.3],
@@ -608,14 +625,15 @@
     assert results.plot_fn_args["ylabel"] == "True Positive Rate"
     assert results.plot_fn_args["line_kwargs"] == {"drawstyle": "steps-post", "linewidth": 1}
 
     expected_auc = [0.75, 0.75, 0.3333]
     assert np.allclose(results.auc, expected_auc, rtol=1e-3)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_evaluate_custom_metric_incorrect_return_formats():
     eval_df = pd.DataFrame({"prediction": [1.2, 1.9, 3.2], "target": [1, 2, 3]})
     metrics = _get_regressor_metrics(eval_df["target"], eval_df["prediction"])
 
     def dummy_fn(*_):
         pass
 
@@ -682,21 +700,23 @@
     [
         (lambda eval_df, _: {"pred_sum": sum(eval_df["prediction"])}, does_not_raise()),
         (lambda eval_df, builtin_metrics: ({"test": 1.1}, {"a_list": [1, 2, 3]}), does_not_raise()),
         (lambda _, __: 3, pytest.raises(MlflowException, match="'<lambda>' (.*) did not return in an expected "
                                                                "format", ),),
     ],
 )
+@pytest.mark.usefixtures("new_clean_dir")
 def test_evaluate_custom_metric_lambda(fn, expectation):
     eval_df = pd.DataFrame({"prediction": [1.2, 1.9, 3.2], "target": [1, 2, 3]})
     metrics = _get_regressor_metrics(eval_df["target"], eval_df["prediction"])
     with expectation:
         _evaluate_custom_metric(_CustomMetric(fn, "<lambda>", 0, ""), eval_df, metrics)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_evaluate_custom_metric_success():
     eval_df = pd.DataFrame({"prediction": [1.2, 1.9, 3.2], "target": [1, 2, 3]})
     metrics = _get_regressor_metrics(eval_df["target"], eval_df["prediction"])
 
     def example_custom_metric(_, given_metrics):
         return {
             "example_count_times_1_point_5": given_metrics["example_count"] * 1.5,
@@ -762,14 +782,15 @@
             evaluators="default",
             custom_metrics=custom_metrics,
         )
     _, metrics, _, artifacts = get_run_data(run.info.run_id)
     return result, metrics, artifacts
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_custom_metric_produced_multiple_artifacts_with_same_name_throw_exception(
         binary_logistic_regressor_model_uri, breast_cancer_dataset  # pylint: disable=unused-import
 ):
     def example_custom_metric_1(_, __):
         return {}, {"test_json_artifact": {"a": 2, "b": [1, 2]}}
 
     def example_custom_metric_2(_, __):
@@ -782,14 +803,15 @@
         _get_results_for_custom_metrics_tests(
             binary_logistic_regressor_model_uri,
             breast_cancer_dataset,
             [example_custom_metric_1, example_custom_metric_2],
         )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_custom_metric_mixed(binary_logistic_regressor_model_uri,
                              breast_cancer_dataset):  # pylint: disable=unused-import
     def example_custom_metric(eval_df, given_metrics, tmp_path):
         example_metrics = {
             "true_count": given_metrics["true_negatives"] + given_metrics["true_positives"],
             "positive_count": np.sum(eval_df["prediction"]),
         }
@@ -844,14 +866,15 @@
 
     assert "test_npy_artifact" in result.artifacts
     assert "test_npy_artifact_on_data_breast_cancer_dataset.npy" in artifacts
     assert isinstance(result.artifacts["test_npy_artifact"], NumpyEvaluationArtifact)
     assert np.array_equal(result.artifacts["test_npy_artifact"].content, np.array([1, 2, 3, 4, 5]))
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_custom_metric_logs_artifacts_from_paths(
         binary_logistic_regressor_model_uri, breast_cancer_dataset  # pylint: disable=unused-import
 ):
     fig_x = 8.0
     fig_y = 5.0
     fig_dpi = 100.0
     img_formats = ("png", "jpeg", "jpg")
@@ -951,14 +974,15 @@
         self.a = [1, 2, 3]
         self.b = "hello"
 
     def __eq__(self, o: object) -> bool:
         return self.a == o.a and self.b == self.b
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_custom_metric_logs_artifacts_from_objects(
         binary_logistic_regressor_model_uri, breast_cancer_dataset  # pylint: disable=unused-import
 ):
     fig = plt.figure()
     plt.plot([1, 2, 3])
     buf = io.BytesIO()
     fig.savefig(buf)
@@ -1007,14 +1031,15 @@
 
     assert "test_pickled_artifact" in result.artifacts
     assert "test_pickled_artifact_on_data_breast_cancer_dataset.pickle" in artifacts
     assert isinstance(result.artifacts["test_pickled_artifact"], PickleEvaluationArtifact)
     assert result.artifacts["test_pickled_artifact"].content == _ExampleToBePickledObject()
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_evaluate_sklearn_model_score_skip_when_not_scorable(
         linear_regressor_model_uri, diabetes_dataset  # pylint: disable=unused-import
 ):
     with mock.patch(
             "sklearn.linear_model.LinearRegression.score",
             side_effect=RuntimeError("LinearRegression.score failed"),
     ) as mock_score:
@@ -1058,14 +1083,15 @@
 #             if len(matched_keys) > 1:
 #                 duplicate_metrics += matched_keys
 #         assert duplicate_metrics == []
 #     finally:
 #         mlflow.sklearn.autolog(disable=True)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_truncation_works_for_long_feature_names(linear_regressor_model_uri,
                                                  diabetes_dataset):  # noqa: F811
     evaluate(
         linear_regressor_model_uri,
         diabetes_dataset._constructor_args["data"],
         model_type="regressor",
         targets=diabetes_dataset._constructor_args["targets"],
```

## tests/models/test_evaluation.py

```diff
@@ -382,14 +382,15 @@
             "f4": eval_X[:, 3],
             "y": eval_y,
         }
     )
     return EvaluationDataset(data=data, targets="y", name="iris_pandas_df_dataset")
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_classifier_evaluate(multiclass_logistic_regressor_model_uri, iris_dataset):
     y_true = iris_dataset.labels_data
     classifier_model = mlflow.pyfunc.load_model(multiclass_logistic_regressor_model_uri)
     y_pred = classifier_model.predict(iris_dataset.features_data)
     expected_accuracy_score = accuracy_score(y_true, y_pred)
     expected_metrics = {
         "accuracy_score": expected_accuracy_score,
@@ -539,14 +540,15 @@
 #                 evaluators="dummy_evaluator",
 #             )
 #         _, saved_metrics, _, _ = get_run_data(run.info.run_id)
 #         # assert saved_metrics == expected_saved_metrics
 #         # assert eval_result.metrics == expected_metrics
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_pandas_df_regressor_evaluation(linear_regressor_model_uri):
     data = sklearn.datasets.load_diabetes()
     df = pd.DataFrame(data.data, columns=data.feature_names)
     df["y"] = data.target
 
     regressor_model = mlflow.pyfunc.load_model(linear_regressor_model_uri)
 
@@ -565,32 +567,35 @@
         _, saved_metrics, _, _ = get_run_data(run.info.run_id)
 
     augment_name = f"_on_data_{dataset_name}"
     for k, v in eval_result.metrics.items():
         assert v == saved_metrics[f"{k}{augment_name}"]
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_dataset_name():
     X, y = get_iris()
     d1 = EvaluationDataset(data=X, targets=y, name="a1")
     assert d1.name == "a1"
     d2 = EvaluationDataset(data=X, targets=y)
     assert d2.name == d2.hash
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_dataset_metadata():
     X, y = get_iris()
     d1 = EvaluationDataset(data=X, targets=y, name="a1", path="/path/to/a1")
     assert d1._metadata == {
         "hash": "6bdf4e119bf1a37e7907dfd9f0e68733",
         "name": "a1",
         "path": "/path/to/a1",
     }
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_gen_md5_for_arraylike_obj():
     def get_md5(data):
         md5_gen = hashlib.md5()
         _gen_md5_for_arraylike_obj(md5_gen, data)
         return md5_gen.hexdigest()
 
     list0 = list(range(20))
@@ -606,14 +611,15 @@
 
 # def test_dataset_hash(iris_dataset, iris_pandas_df_dataset, diabetes_spark_dataset):
 #     assert iris_dataset.hash == "99329a790dc483e7382c0d1d27aac3f3"
 #     assert iris_pandas_df_dataset.hash == "799d4f50e2e353127f94a0e5300add06"
 #     assert diabetes_spark_dataset.hash == "e646b03e976240bd0c79c6bcc1ae0bda"
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_dataset_with_pandas_dataframe():
     data = pd.DataFrame({"f1": [1, 2], "f2": [3, 4], "f3": [5, 6], "label": [0, 1]})
     eval_dataset = EvaluationDataset(data=data, targets="label")
 
     assert list(eval_dataset.features_data.columns) == ["f1", "f2", "f3"]
     assert np.array_equal(eval_dataset.features_data.f1.to_numpy(), [1, 2])
     assert np.array_equal(eval_dataset.features_data.f2.to_numpy(), [3, 4])
@@ -622,14 +628,15 @@
 
     eval_dataset2 = EvaluationDataset(data=data, targets="label", feature_names=["f3", "f2"])
     assert list(eval_dataset2.features_data.columns) == ["f3", "f2"]
     assert np.array_equal(eval_dataset2.features_data.f2.to_numpy(), [3, 4])
     assert np.array_equal(eval_dataset2.features_data.f3.to_numpy(), [5, 6])
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_dataset_with_array_data():
     features = [[1, 2], [3, 4]]
     labels = [0, 1]
 
     for input_data in [features, np.array(features)]:
         eval_dataset1 = EvaluationDataset(data=input_data, targets=labels)
         assert np.array_equal(eval_dataset1.features_data, features)
@@ -640,14 +647,15 @@
         data=input_data, targets=labels, feature_names=["a", "b"]
     ).feature_names == ["a", "b"]
 
     with pytest.raises(ValueError, match="all element must has the same length"):
         EvaluationDataset(data=[[1, 2], [3, 4, 5]], targets=labels)
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_dataset_autogen_feature_names():
     labels = [0]
     eval_dataset2 = EvaluationDataset(data=[list(range(9))], targets=labels)
     assert eval_dataset2.feature_names == [f"feature_{i + 1}" for i in range(9)]
 
     eval_dataset2 = EvaluationDataset(data=[list(range(10))], targets=labels)
     assert eval_dataset2.feature_names == [f"feature_{i + 1:02d}" for i in range(10)]
@@ -670,14 +678,15 @@
 #         dataset = EvaluationDataset(spark_df, targets="y")
 #         assert list(dataset.features_data.columns) == ["f1", "f2"]
 #         assert list(dataset.features_data["f1"]) == [1.0] * 5
 #         assert list(dataset.features_data["f2"]) == [2.0] * 5
 #         assert list(dataset.labels_data) == [3.0] * 5
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_log_dataset_tag(iris_dataset, iris_pandas_df_dataset):
     model_uuid = uuid.uuid4().hex
     with mlflow.start_run() as run:
         client = mlflow.tracking.MlflowClient()
         iris_dataset._log_dataset_tag(client, run.info.run_id, model_uuid=model_uuid)
         _, _, tags, _ = get_run_data(run.info.run_id)
 
@@ -734,14 +743,15 @@
     def _save(self, output_artifact_path):
         raise RuntimeError()
 
     def _load_content_from_file(self, local_artifact_path):
         raise RuntimeError()
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_evaluator_interface(multiclass_logistic_regressor_model_uri, iris_dataset):
     with mock.patch.object(
             _model_evaluation_registry, "_registry", {"test_evaluator1": FakeEvauator1}
     ):
         evaluator1_config = {"eval1_confg_a": 3, "eval1_confg_b": 4}
         evaluator1_return_value = EvaluationResult(
             metrics={"m1": 5, "m2": 6},
@@ -799,14 +809,15 @@
                     dataset=iris_dataset,
                     run_id=run.info.run_id,
                     evaluator_config=evaluator1_config,
                     custom_metrics=None,
                 )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_evaluate_with_multi_evaluators(multiclass_logistic_regressor_model_uri, iris_dataset):
     with mock.patch.object(
             _model_evaluation_registry,
             "_registry",
             {"test_evaluator1": FakeEvauator1, "test_evaluator2": FakeEvauator2},
     ):
         evaluator1_config = {"eval1_confg": 3}
@@ -874,28 +885,30 @@
                         dataset=iris_dataset,
                         run_id=run.info.run_id,
                         evaluator_config=evaluator2_config,
                         custom_metrics=None,
                     )
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_start_run_or_reuse_active_run():
     with _start_run_or_reuse_active_run() as run_id:
         assert mlflow.active_run().info.run_id == run_id
 
     with mlflow.start_run() as run:
         active_run_id = run.info.run_id
 
         with _start_run_or_reuse_active_run() as run_id:
             assert run_id == active_run_id
 
         with _start_run_or_reuse_active_run() as run_id:
             assert run_id == active_run_id
 
 
+@pytest.mark.usefixtures("new_clean_dir")
 def test_normalize_evaluators_and_evaluator_config_args():
     from azureml.evaluate.mlflow.models.evaluation.default_evaluator import DefaultEvaluator
     from azureml.evaluate.mlflow.models.evaluation.azureml.azureml_evaluator import AzureMLEvaluator
 
     with mock.patch.object(
             _model_evaluation_registry,
             "_registry",
```

## Comparing `azureml_evaluate_mlflow-0.0.7.dist-info/LICENSE.txt` & `azureml_evaluate_mlflow-0.0.8.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `azureml_evaluate_mlflow-0.0.7.dist-info/METADATA` & `azureml_evaluate_mlflow-0.0.8.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: azureml-evaluate-mlflow
-Version: 0.0.7
+Version: 0.0.8
 Summary: Contains the integration code of AzureML Evaluate with Mlflow.
 Home-page: UNKNOWN
 Author: Microsoft Corp
 License: Proprietary https://aka.ms/azureml-preview-sdk-license 
 Platform: UNKNOWN
 Classifier: Development Status :: 4 - Beta
 Classifier: Intended Audience :: Developers
@@ -30,15 +30,18 @@
 Requires-Dist: scikit-learn (>=0.24.0)
 Requires-Dist: cryptography
 Requires-Dist: python-dateutil (<3.0.0,>=2.7.3)
 Requires-Dist: datasets (==2.11.0)
 Requires-Dist: soundfile (>=0.12.1)
 Requires-Dist: librosa
 Requires-Dist: diffusers (==0.14.0)
-Requires-Dist: transformers (==4.26.1)
+Requires-Dist: sentencepiece
+Requires-Dist: transformers (>=4.28.1)
+Requires-Dist: torch (>=1.9.0)
+Requires-Dist: accelerate (>=0.19.0)
 Requires-Dist: Pillow (==9.4.0)
 Provides-Extra: deployments
 Requires-Dist: flask ; extra == 'deployments'
 Requires-Dist: numpy ; extra == 'deployments'
 Requires-Dist: pandas ; extra == 'deployments'
 
 UNKNOWN
```

## Comparing `azureml_evaluate_mlflow-0.0.7.dist-info/RECORD` & `azureml_evaluate_mlflow-0.0.8.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 azureml/__init__.py,sha256=n0xtZ3iWcoVg5Qognsb7InYAUVAK8s3iaVeHB5GOaNA,251
 azureml/evaluate/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
 azureml/evaluate/mlflow/NOTICE,sha256=9yhw1yr8BF0uDAkvycPPtWKgGRZQ1qa-NjWjh3M9Ptw,477828
 azureml/evaluate/mlflow/__init__.py,sha256=XiZqhd8Bmmiq_2UNYiLBkzEKBKB2xEFF4Uq8nAsL7dg,7071
-azureml/evaluate/mlflow/_version.py,sha256=8iO5GQcMv0IT7Iil5TDihy-fhGMWbAFFV8FZLU2C8A4,19
+azureml/evaluate/mlflow/_version.py,sha256=T-gjHA28WYCer9VwfphBWRKzryc5qfuoSdQFyJ6lUQg,19
 azureml/evaluate/mlflow/constants.py,sha256=I4FeqGzLnBhlpkMc5IjqtZWGrphEwmNOQ5XHYuNpQ_s,695
 azureml/evaluate/mlflow/exceptions.py,sha256=IRI5it-nGfea4Mz2szTZe2JOPHbZQ_tJHoX4yFYz9-s,373
 azureml/evaluate/mlflow/_loader_modules/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
-azureml/evaluate/mlflow/_loader_modules/hftransformers.py,sha256=ifpkefsthCK_T7rdOQcmVK4Hbw9zdvYHKevCCUZnpA8,2769
+azureml/evaluate/mlflow/_loader_modules/hftransformers.py,sha256=UDdTr69MLpZgtKiwz7dqjRcpHRijKl25eP_TcIeORmY,2867
 azureml/evaluate/mlflow/_loader_modules/sklearn.py,sha256=3ucm1zLUfJYJ83SftdC28tGw4tHuLUGtxofFI8PAJJ8,457
 azureml/evaluate/mlflow/aml/__init__.py,sha256=xSVmSnGxtNRAuna8m8YS8zQj2hka73Emx3Nj29RTzxw,33109
 azureml/evaluate/mlflow/aml/constants.py,sha256=A6sNVbQ4w26UnwtOGKmHPrglaaCXsiJuYSxogeT0Lwc,930
 azureml/evaluate/mlflow/aml/model.py,sha256=rkH_gplcQiFyj5Za0e9IB7i9-ghggcUejOd3Ensp4Dg,14964
 azureml/evaluate/mlflow/aml/utils.py,sha256=T1xdnR0z1Kyi8R8q8pQDE2DQ_RX9mxr8ZJ41hV9bXLc,12432
-azureml/evaluate/mlflow/hftransformers/__init__.py,sha256=lptqB6nKjjpbEqqIgzG-srkZJLElsI1hQbK2d2Z9sjY,38122
+azureml/evaluate/mlflow/hftransformers/__init__.py,sha256=Yo20XSe7dSUFKlnUG_PJ7a7GSV9kTShAD_s11Qnxo84,39007
 azureml/evaluate/mlflow/hftransformers/_task_based_pipeline_predictors.py,sha256=sIBsDjCKNgQaulAgvGTGxbQylGPwVu684hSOiBAvZs8,3895
-azureml/evaluate/mlflow/hftransformers/_task_based_predictors.py,sha256=YhXYE5jqIkuP9DdbsWe__09uZn3TGtpCzlwbAxT3p58,32935
+azureml/evaluate/mlflow/hftransformers/_task_based_predictors.py,sha256=mDyv3Khr0G8jyjbgewmGvQyL5-PTqX8tqpqtPTbBazg,33130
 azureml/evaluate/mlflow/hftransformers/constants.py,sha256=mmUQrpNvJ45MA6ySgEVEUsLOPGgzgqF-kACi3Abj7Yo,1050
 azureml/evaluate/mlflow/hftransformers/dataset_wrappers.py,sha256=bf5RGChxfTGVgamSAZxntTiHVvCLAqWM-49WRABJ8dQ,4875
 azureml/evaluate/mlflow/hftransformers/utils.py,sha256=hm5YudTVzi5LESzY2Cor9AC32D8PzgJ3XAItzKnztQo,2675
 azureml/evaluate/mlflow/models/__init__.py,sha256=M8_B_jWlt2-Rb_8T8do5suYOA7d87fsRizwgqDGl0ag,1832
 azureml/evaluate/mlflow/models/evaluation/__init__.py,sha256=-OltsA-899LKiGGG7giMGOyjf_pH5kXoXNyiJT_n9mQ,540
 azureml/evaluate/mlflow/models/evaluation/base.py,sha256=2HAWAa5VKryaznaMAvstg45LWqDGlPv9OlGPM9nVupM,42463
 azureml/evaluate/mlflow/models/evaluation/constants.py,sha256=xvRcxjq0fXd4In7nh08TegQfn9r_WUVQk2g-9AfPKiY,375
@@ -36,26 +36,26 @@
 azureml/evaluate/mlflow/models/evaluation/azureml/_summarization_evaluator.py,sha256=hTWA8Zca-OR5dNTuQ-Y8zE_iQr1-nTFluKlORFp7y0k,1270
 azureml/evaluate/mlflow/models/evaluation/azureml/_task_evaluator.py,sha256=xzpidupudXfOVCd_nqEZzx6nYWDyNj31MRTx4VGfgH4,802
 azureml/evaluate/mlflow/models/evaluation/azureml/_task_evaluator_factory.py,sha256=pKT8msOyAkb0A5AcZ7mZNFGwr98AKpULYimlqK7NSc4,2561
 azureml/evaluate/mlflow/models/evaluation/azureml/_text_generation_evaluator.py,sha256=iXX8xVjrPWQJiSi5l7y_waz9OiaktUITxGiMinpRt9M,1273
 azureml/evaluate/mlflow/models/evaluation/azureml/_translation_evaluator.py,sha256=YY7MqnscGwI2iMgaWm3FCgj9tFtwNFeuV_kDvYrseP0,1266
 azureml/evaluate/mlflow/models/evaluation/azureml/azureml_evaluator.py,sha256=a6cn60NfR95_lp_XdcOitiEJKAAAnuh47RnuDo0NuNg,12303
 tests/aml/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
-tests/aml/test_model_export_with_class_and_artifacts.py,sha256=cwkX8I96Lpe3uj7pZRGt1_vTWalVSjig2C51v_kRkXo,51203
-tests/aml/test_model_export_with_loader_module_and_data_path.py,sha256=vAwplwJ1D-ssopmYOEucWuBETM8b4hCi1ZXLqrtL9fA,30719
+tests/aml/test_model_export_with_class_and_artifacts.py,sha256=cuKWAamiLKQdgreb81HILL-UqaUQZdeCVV_3l64nYuo,52149
+tests/aml/test_model_export_with_loader_module_and_data_path.py,sha256=CRmXxEdn6MxfmbbTKlgQsyqsP4DssKYuIU4PR92TLAY,31450
 tests/hftransformers/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
 tests/hftransformers/hf_test_predict.py,sha256=XtvT129GmnU4bD5Ykd-Dh--x4PtP4j07aS0-ZBlxn6o,317
 tests/hftransformers/preprocess.py,sha256=a8IqSESoy8Gcpt2pQLHZKjwWLUZfbAp1bSCzEQ0Prvc,154
-tests/hftransformers/test_hf_load_pipeline.py,sha256=CV3sEQEAU2j10D8-jbdTXEiHl1wMgWrl8D9Xtb7VDlU,4019
-tests/hftransformers/test_hf_model_export.py,sha256=oqlcnS88e2dlqeOdYfeKfd4nSEcme2iySwcQrHxT-Og,32623
+tests/hftransformers/test_hf_load_pipeline.py,sha256=r_7nAur0EhAKF2AU2BTnNU5M0g5VGMRn0tP-yidkvsc,4105
+tests/hftransformers/test_hf_model_export.py,sha256=zhIY8oC-D6STrKZtvbbwAqG86Mmz0GqoZuHE7lb0ZDA,33365
 tests/models/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
 tests/models/dummy_evaluator.py,sha256=KpWJfNo9cTpDzhBeszCKSU032j2BU0l7OrkSYoT2H6Q,4537
-tests/models/test_azureml_evaluator.py,sha256=lhJvkz1lkwtyt0LBy-UolV3fRs4NnrNYJ8oQgyQVYvw,27305
-tests/models/test_default_evaluator.py,sha256=J63xqa_1C2Tood7LB0d4VV7H1eJ2tKMBSY8_yzf8Gv8,43050
-tests/models/test_evaluation.py,sha256=qI4ieoSslO24d46tXFQyBlmk2WNQs3VdOcoPsQWwdRw,37145
+tests/models/test_azureml_evaluator.py,sha256=saCgnBkPQp2UDF-xGnByPCoX3_oBRV5rGxpo6jKtRvE,28181
+tests/models/test_default_evaluator.py,sha256=JEDh5mQTm_HmHgbIO5GyJY7sE9NgDUgqvACqkI1np4w,44168
+tests/models/test_evaluation.py,sha256=GRDGZ7JdcTmWXMqztZIF7dxtI2bIRcmTckGShVNDYZ8,37704
 tests/models/test_forecaster_evaluator.py,sha256=bInHaNFuHoHgnLH-_6s2uCOt3Q_bTonbYGXz8hBXDRM,13542
 tests/models/utils.py,sha256=P4775IKBgXY7qGtHYyuDdQsqooAWEIJ09Fm7B9GBMms,50782
-azureml_evaluate_mlflow-0.0.7.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
-azureml_evaluate_mlflow-0.0.7.dist-info/METADATA,sha256=YjmrtLbnuHGL4lX1lRJb_dRfiAYDUm_0HPG4Wjzc5ZE,1686
-azureml_evaluate_mlflow-0.0.7.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
-azureml_evaluate_mlflow-0.0.7.dist-info/top_level.txt,sha256=YGbVVonfOvHBwpsDBTpLWbK7KEdGg0m7LlGFY7j0SCw,14
-azureml_evaluate_mlflow-0.0.7.dist-info/RECORD,,
+azureml_evaluate_mlflow-0.0.8.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
+azureml_evaluate_mlflow-0.0.8.dist-info/METADATA,sha256=eNeplocBOjifUQ31XHTECTcpKeBSFlhKZUnSOPQFUXc,1783
+azureml_evaluate_mlflow-0.0.8.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
+azureml_evaluate_mlflow-0.0.8.dist-info/top_level.txt,sha256=YGbVVonfOvHBwpsDBTpLWbK7KEdGg0m7LlGFY7j0SCw,14
+azureml_evaluate_mlflow-0.0.8.dist-info/RECORD,,
```

