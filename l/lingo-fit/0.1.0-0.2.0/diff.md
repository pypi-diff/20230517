# Comparing `tmp/lingo_fit-0.1.0-py3-none-any.whl.zip` & `tmp/lingo_fit-0.2.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,27 +1,27 @@
-Zip file size: 18817 bytes, number of entries: 25
+Zip file size: 18969 bytes, number of entries: 25
 -rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 lingo_fit/__init__.py
 -rwxr-xr-x  2.0 unx       54 b- defN 80-Jan-01 00:00 lingo_fit/api/__init__.py
--rw-r--r--  2.0 unx     1071 b- defN 80-Jan-01 00:00 lingo_fit/api/__main__.py
+-rw-r--r--  2.0 unx     1404 b- defN 80-Jan-01 00:00 lingo_fit/api/__main__.py
 -rw-r--r--  2.0 unx      150 b- defN 80-Jan-01 00:00 lingo_fit/api/endpoints/__init__.py
 -rw-r--r--  2.0 unx      553 b- defN 80-Jan-01 00:00 lingo_fit/api/endpoints/predict.py
--rw-r--r--  2.0 unx      519 b- defN 80-Jan-01 00:00 lingo_fit/api/endpoints/train.py
--rw-r--r--  2.0 unx     1008 b- defN 80-Jan-01 00:00 lingo_fit/api/features.py
+-rw-r--r--  2.0 unx      701 b- defN 80-Jan-01 00:00 lingo_fit/api/endpoints/train.py
+-rw-r--r--  2.0 unx     1005 b- defN 80-Jan-01 00:00 lingo_fit/api/features.py
 -rwxr-xr-x  2.0 unx        0 b- defN 80-Jan-01 00:00 lingo_fit/api/py.typed
 -rw-r--r--  2.0 unx      359 b- defN 80-Jan-01 00:00 lingo_fit/api/recommendation_api.py
--rw-r--r--  2.0 unx     2838 b- defN 80-Jan-01 00:00 lingo_fit/api/utils.py
+-rw-r--r--  2.0 unx     2804 b- defN 80-Jan-01 00:00 lingo_fit/api/utils.py
 -rw-r--r--  2.0 unx      780 b- defN 80-Jan-01 00:00 lingo_fit/data/__init__.py
 -rw-r--r--  2.0 unx     1637 b- defN 80-Jan-01 00:00 lingo_fit/data/exercises.py
 -rw-r--r--  2.0 unx     2608 b- defN 80-Jan-01 00:00 lingo_fit/data/experiences.py
 -rw-r--r--  2.0 unx      542 b- defN 80-Jan-01 00:00 lingo_fit/data/languages.py
 -rw-r--r--  2.0 unx      886 b- defN 80-Jan-01 00:00 lingo_fit/data/learners.py
 -rw-r--r--  2.0 unx      833 b- defN 80-Jan-01 00:00 lingo_fit/data/lessons.py
 -rw-r--r--  2.0 unx      876 b- defN 80-Jan-01 00:00 lingo_fit/data/progress.py
 -rw-r--r--  2.0 unx      642 b- defN 80-Jan-01 00:00 lingo_fit/data/subscriptions.py
 -rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 lingo_fit/model/__init__.py
--rw-r--r--  2.0 unx     5014 b- defN 80-Jan-01 00:00 lingo_fit/model/thompson.py
+-rw-r--r--  2.0 unx     5004 b- defN 80-Jan-01 00:00 lingo_fit/model/thompson.py
 -rw-r--r--  2.0 unx        0 b- defN 80-Jan-01 00:00 lingo_fit/py.typed
--rw-r--r--  2.0 unx    11357 b- defN 80-Jan-01 00:00 lingo_fit-0.1.0.dist-info/LICENSE
--rw-r--r--  2.0 unx     6768 b- defN 80-Jan-01 00:00 lingo_fit-0.1.0.dist-info/METADATA
--rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 lingo_fit-0.1.0.dist-info/WHEEL
-?rw-r--r--  2.0 unx     2041 b- defN 16-Jan-01 00:00 lingo_fit-0.1.0.dist-info/RECORD
-25 files, 40624 bytes uncompressed, 15505 bytes compressed:  61.8%
+-rw-r--r--  2.0 unx    11357 b- defN 80-Jan-01 00:00 lingo_fit-0.2.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     6835 b- defN 80-Jan-01 00:00 lingo_fit-0.2.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       88 b- defN 80-Jan-01 00:00 lingo_fit-0.2.0.dist-info/WHEEL
+?rw-r--r--  2.0 unx     2041 b- defN 16-Jan-01 00:00 lingo_fit-0.2.0.dist-info/RECORD
+25 files, 41159 bytes uncompressed, 15657 bytes compressed:  62.0%
```

## zipnote {}

```diff
@@ -57,20 +57,20 @@
 
 Filename: lingo_fit/model/thompson.py
 Comment: 
 
 Filename: lingo_fit/py.typed
 Comment: 
 
-Filename: lingo_fit-0.1.0.dist-info/LICENSE
+Filename: lingo_fit-0.2.0.dist-info/LICENSE
 Comment: 
 
-Filename: lingo_fit-0.1.0.dist-info/METADATA
+Filename: lingo_fit-0.2.0.dist-info/METADATA
 Comment: 
 
-Filename: lingo_fit-0.1.0.dist-info/WHEEL
+Filename: lingo_fit-0.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: lingo_fit-0.1.0.dist-info/RECORD
+Filename: lingo_fit-0.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## lingo_fit/api/__main__.py

```diff
@@ -1,31 +1,49 @@
 import argparse
 import os
 
 import uvicorn
 
-if __name__ == "__main__":
-    # Parse command-line arguments
+
+def parse_arguments():
     parser = argparse.ArgumentParser(description="Thompson Sampling API")
     parser.add_argument(
         "--host", type=str, default="0.0.0.0", help="Host IP address"
     )
     parser.add_argument("--port", type=int, default=8000, help="Port number")
     parser.add_argument("--priors", type=str, help="where priors are located")
     parser.add_argument(
-        "--bucket", type=str, help="S3 bucket name for the current service"
+        "--to_dump_priors",
+        default="no",
+        type=str,
+        help="yes - to dump priors, no - to not save priorfs",
+    )
+    parser.add_argument(
+        "--bucket",
+        default="personalization-service",
+        type=str,
+        help="S3 bucket name for the current service",
     )
     parser.add_argument("--region", type=str, help="AWS region")
     args = parser.parse_args()
+    os.environ["S3_BUCKET"] = args.bucket
+    os.environ["REGION"] = args.region
+    os.environ["BUCKET"] = args.bucket
+    os.environ["TO_DUMP_PRIORS"] = str(args.to_dump_priors)
+    return args
+
+
+def main():
     log_config = uvicorn.config.LOGGING_CONFIG
     log_config["formatters"]["access"][
         "fmt"
     ] = "%(asctime)s - %(levelname)s - %(message)s"
 
-    os.environ["S3_BUCKET"] = args.bucket
-    os.environ["REGION"] = args.region
-    os.environ["ARMS_PRIOR_PATH"] = args.priors
-
+    args = parse_arguments()
     from .recommendation_api import create_app
 
     app = create_app()
     uvicorn.run(app, host=args.host, port=args.port, log_config=log_config)
+
+
+if __name__ == "__main__":
+    main()
```

## lingo_fit/api/endpoints/train.py

```diff
@@ -1,22 +1,26 @@
 import logging
-from typing import List
+import os
 
 import numpy as np
 from fastapi import APIRouter
 
 from ..features import UserExperience
 from ..utils import dump_priors, get_ts
 
 router = APIRouter()
 ts = get_ts()
 router = APIRouter()
 
 
 @router.post("/train")
-def train(user_experience: List[UserExperience]):
-    context = np.array(user_experience.dict().values())
-    ts.sample(context)
-    dump_priors(ts.prior_params)
+def train(user_experience: UserExperience):
+    user_experience_dict = user_experience.dict()
+    learner_id = user_experience_dict.get("learner_id")
+    context = np.array(user_experience_dict.values())
+    ts.sample(context, learner_id)
+    to_dump_priors = os.environ["TO_DUMP_PRIORS"]
+    if to_dump_priors == "yes":
+        dump_priors(ts.prior_params)
     response = {"STATUS_CODE": 200}
     logging.info(f"returning response:{response}")
     return response
```

## lingo_fit/api/features.py

```diff
@@ -1,15 +1,15 @@
 """
 The InferenceFeatures class is a Pydantic BaseModel
 that represents the features used for venue rating inference.
 """
 from pydantic import BaseModel, Field
 
 
-class InferenceFeatures(BaseModel):
+class UserExperience(BaseModel):
     learner_id: str = Field(
         "4202398962129790175",
         description="ID of the venue being recommended",
         example="4202398962129790175",
     )
     lesson_id: str = Field(
         "spanish_sangria",
```

## lingo_fit/api/utils.py

```diff
@@ -21,17 +21,16 @@
     if "ARMS_PRIOR_PATH" in os.environ:
         arms_priors_path = os.environ["ARMS_PRIOR_PATH"]
         arms_priors = load_arms_priors(arms_priors_path)
         return ThompsonSampling(
             reward_function=reward_function, arms_priors=arms_priors
         )
 
-    return ThompsonSampling(
-            reward_function=reward_function
-        )
+    return ThompsonSampling(reward_function=reward_function, arms_priors=None)
+
 
 def parse_s3_path(s3_path: str) -> (str, str):
     s3_path = s3_path[5:]  # Remove the "s3://" prefix
     parts = s3_path.split("/", 1)
     bucket_name = parts[0]
     object_key = parts[1]
     return bucket_name, object_key
@@ -63,41 +62,42 @@
         # Load from local file
         with open(arms_priors_path) as file:
             arms_priors = json.load(file)
 
     return arms_priors
 
 
-def generate_timestamp_path(bucket: str, region: str, object_name: str) -> str:
+def generate_timestamp_path(region: str, object_name: str) -> str:
     timestamp = f"{datetime.now()}"
     timestamped_path = (
         f"{object_name}/"
         f"region={region}/"
         f"timestamp={timestamp}/"
         f"{object_name}.json"
     )
     return timestamped_path
 
 
-def dump_priors(prior_params):
+def dump_priors(prior_params: dict):
     """
     compute regret here
     :return:
     """
+
     bucket_name = os.environ["BUCKET"]
     region = os.environ["REGION"]
 
     # Serialize prior_params to JSON
     prior_params_json = json.dumps(prior_params)
 
     # Create an S3 client
     s3_client = boto3.client("s3", region_name=region)
 
     # Define the S3 object key
     s3_object_key = generate_timestamp_path(
-        bucket_name=bucket_name, region=region, object_name="priors"
+        region=region, object_name="priors"
     )
 
     # Upload the JSON data to S3 bucket
     s3_client.put_object(
         Body=prior_params_json, Bucket=bucket_name, Key=s3_object_key
     )
```

## lingo_fit/model/thompson.py

```diff
@@ -14,53 +14,53 @@
 
 class ThompsonSampling:
     def __init__(
         self,
         reward_function: Callable,
         arms_priors: Tuple[float, float],
     ):
-        is_valid_prior = validate_priors(arms_priors)
+        is_valid_prior = validate_priors(arms_priors) if arms_priors else False
         self.prior_params = (
             arms_priors
             if is_valid_prior
             else {
                 exercise_type: (
                     np.random.uniform(0, 1),
                     np.random.uniform(0, 1),
                 )
                 for exercise_type in EXERCISE_TYPES
             }
         )
-        import json
 
-        json.dump(open("generated_priors.json"), self.prior_params)
         info_message = """
         Insufficient number of prior parameters supplied.
         Therefore, priors were generated randomly
         """
         if not is_valid_prior:
             logging.info(info_message)
 
         self.n_arms = len(EXERCISE_TYPES)
         self.best_arm = None
         if not isinstance(reward_function, Callable):
             raise ValueError("reward_function is expected to be 'Callable'")
         self.reward_function = reward_function
 
-    def sample(self, context: np.array) -> int:
+    def sample(self, context: np.array, learner_id: str) -> int:
         """Samples an arm using the Thompson Sampling algorithm.
 
         Parameters:
             context (np.ndarray): The context for which to choose an arm.
 
         Returns:
             int: The index of the arm to pull.
         """
 
-        expected_rewards, best_arm = self._compute_expected_rewards(context)
+        expected_rewards, best_arm = self._compute_expected_rewards(
+            context, learner_id
+        )
 
         # Update counts, successes, and failures for chosen arm
         highest_expected_reward = expected_rewards[best_arm]
         self.update_prior(best_arm, highest_expected_reward)
 
         self.best_arm = best_arm
 
@@ -139,12 +139,12 @@
         to the observed current reward {highest_reward}.
         The updated values are:
         new_alpha: {alpha}, new_beta: {beta}
         """
         logging.info(info_message)
         self.prior_params[arm] = (alpha, beta)
 
-    def regret(self):
+    def regrets(self):
         """
         compute regret here
         :return:
         """
```

## Comparing `lingo_fit-0.1.0.dist-info/LICENSE` & `lingo_fit-0.2.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `lingo_fit-0.1.0.dist-info/METADATA` & `lingo_fit-0.2.0.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: lingo-fit
-Version: 0.1.0
+Version: 0.2.0
 Summary: 
 Author: rauan_akylzhanov
 Author-email: akylzhanov.r@gmail.com
 Requires-Python: >=3.8.1,<3.12
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
@@ -12,14 +12,15 @@
 Requires-Dist: faker (>=18.7.0,<19.0.0)
 Requires-Dist: fastapi (>=0.95.1,<0.96.0)
 Requires-Dist: numpy (>=1.24.3,<2.0.0)
 Requires-Dist: pre-commit (>=3.3.1,<4.0.0)
 Requires-Dist: pydantic (>=1.10.7,<2.0.0)
 Requires-Dist: sagemaker (>=2.155.0,<3.0.0)
 Requires-Dist: scipy (>=1.10.1,<2.0.0)
+Requires-Dist: twine (>=4.0.2,<5.0.0)
 Requires-Dist: uvicorn (>=0.22.0,<0.23.0)
 Description-Content-Type: text/markdown
 
 # aws-ml-inference-demo
 
 we aim at building a product that can truly teach languages to our learners.
 To do this we need to provide a personalized experience that suits each learner's needs.
@@ -59,16 +60,14 @@
 So each user requires maximum 12 recommendations per day. That makes 12 million recommendation in any 2 hour window
 during the day.
 
 ## endpoints
 With these assumptions we deploy 15 AWS sagemaker lambda functions into each of the 15 global regions.
 The traffic between zones is rerouted automatically to the step function.
 
-
-
 We will have two API endpoints:
 
     POST /inference: This endpoint will accept a learner UUID in the request body and respond with a recommendation for an exercise type based on the Thompson Sampling algorithm.
 
     POST /training: This endpoint will accept a learner UUID and the learner's response in the request body. The Lambda function will use the response to update the underlying models in the Thompson Sampling algorithm.
 # About training
 The training in batches will result in the same sequence of prior param values as
@@ -94,22 +93,22 @@
 we have around 10 columns of user activity in app that we record, it makes 40 bytes per record
 Each exercise completion generates a record in an sql table, we expect
 40 bytes * 12 million recommendation request 0.48 Gb per day of new data.
 We might store last 30 days of user interaction records, that will be 14 Gb per region
 
 ## ingesting user experience data
 1. As soon as the learner studied 1/4, 2/4, 3/4, 4/4 of the lesson, the AWS Lambda function
-gets triggered and calls Kinesis Data Stream.
+gets triggered and calls Kinesis Data Stream, a possible events.json is attached
 2. Kinesis Data stream calls the Firehose and store the file on S3 bucket.
 3. As soon as the file upload to the bucket, another Lambda gets trigger and call the Glue Job.
-4. AWS Glue job runs and convert the JSON file format to Parquet file format
-5. Glue Job place this new file to the destination bucket.
-6. Snowpipe configure on the destination bucket and trigger once file uploads to Bucket.
-7. On the event of the parquet file being added, we trigger "train" endpoint to produce new priors
-7. Data gets ingest to the Snowflake.
+4. As soon as the file upload to the bucket, we trigger "train" endpoint to produce new priors
+5. AWS Glue job runs and convert the JSON file format to Parquet file format
+6. Glue Job place this new file to the destination bucket.
+7. SnowPipe configure on the destination bucket and trigger once file uploads to Bucket.
+8. Data gets ingest to the Snowflake.
 
 ## Modelling Problem:
 We want to model probabilities that the suggested exercise type is useful for the user and he/she will start and finish the exercise.
 This probability shall be represented by the Beta-Bernoulli distribution
 We want to model the relevancy of each of the exercises as a multi-armed bandit with one arm corresponding to suggested exercise type.
 The probability of the completion of any of the four exercise types is represented by the beta bernoulli model.
 In other words, for each user uuid + possible attached features vector of features is classified by four labels: "listening", "speaking", "writing", or "flashcard",
```

## Comparing `lingo_fit-0.1.0.dist-info/RECORD` & `lingo_fit-0.2.0.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 lingo_fit/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 lingo_fit/api/__init__.py,sha256=OTTvOYHeDD-bmcg2BLvHw2DCwVu5SiJGPVADSYjnx8o,54
-lingo_fit/api/__main__.py,sha256=p-f0f6E1W2-3PElGs0P2qt5PmzL9EPRyDsrZYbF7wBc,1071
+lingo_fit/api/__main__.py,sha256=UYoj8efMeulH1xQWEukN73eV7WHgtMcYxwfZ8d09G1s,1404
 lingo_fit/api/endpoints/__init__.py,sha256=3aSDIf-9dn7QOoRwdDu0mQgd25O2FbJyuw_o5RE_f2k,150
 lingo_fit/api/endpoints/predict.py,sha256=1fln738uSP78O0bM7wanX-UNeCo6noa8EQAhoU6XKAk,553
-lingo_fit/api/endpoints/train.py,sha256=24Y8g-73clU845Ws3gE5P_rIuFlEBSaK_-dOCovl94E,519
-lingo_fit/api/features.py,sha256=WGuTJwO0kj7LGiHwD5JmPfarbFjnkTgH5vzSaWDsdWQ,1008
+lingo_fit/api/endpoints/train.py,sha256=5o2bb9i6wj0C2xKY0j0QO5CCyEwONHNL3QkpS_RmFuk,701
+lingo_fit/api/features.py,sha256=9NHObbHBOMpMlzPKrBGsKB4kpnyrJ6cGuym8KC_8F_Y,1005
 lingo_fit/api/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 lingo_fit/api/recommendation_api.py,sha256=osIIcP5bflnfT9WKk2MzXan3UdAKZlLHKSxFyDWVCQk,359
-lingo_fit/api/utils.py,sha256=dB6yajFkNpdwsXnCvtTleCHhI8Am1MI_i-LuBm0II1c,2838
+lingo_fit/api/utils.py,sha256=_w8F9eivFUzzDcKj8vQhtg8TrRA2bPvR47P6V0w_7Rc,2804
 lingo_fit/data/__init__.py,sha256=Mj4raoeBk9vwAtmGlcKTXO_k0q_h2wk8LbuLLQ-PYVQ,780
 lingo_fit/data/exercises.py,sha256=TfvzXX5GH-p4-A3qKBQlEbPrqUctgtKymcYGOtbMqTY,1637
 lingo_fit/data/experiences.py,sha256=p65yZhuROhqIibKuWAZ1EXNrRg22bs2QRWpMAhDVGdw,2608
 lingo_fit/data/languages.py,sha256=1iC1TelndL-0KgiuR8YcnA9yU2SeITZRGwK__ciLGuE,542
 lingo_fit/data/learners.py,sha256=GBurYNtxmvzUx-AOdrOZxlVHhb-M_CdBGMXa5k-sDtg,886
 lingo_fit/data/lessons.py,sha256=Gw6-7d6s58wiO0duI6-4OkHBEjA8nbrDV7t7ROfdjAs,833
 lingo_fit/data/progress.py,sha256=ECLSxwuj7TPr1zj8OeCF5W0clSfOaRvLeoQtJN3NNG4,876
 lingo_fit/data/subscriptions.py,sha256=musWp83dEy1yOmKNa8kMVyhuy3uJQjD_WzsKrtIiZOM,642
 lingo_fit/model/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-lingo_fit/model/thompson.py,sha256=p_d43b-QfqZHt6-0DdhH1RrsFRfmD24g6lPLLm_T_v8,5014
+lingo_fit/model/thompson.py,sha256=x1yS0-Cb8gWqFPDXyhfCDtcY7rf1sY3mi0CyyyKHKdI,5004
 lingo_fit/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-lingo_fit-0.1.0.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-lingo_fit-0.1.0.dist-info/METADATA,sha256=ol9X8bkJk6BHFBrmFFID7TJoW8xk2N8VVRq-d3p4vRo,6768
-lingo_fit-0.1.0.dist-info/WHEEL,sha256=7Z8_27uaHI_UZAc4Uox4PpBhQ9Y5_modZXWMxtUi4NU,88
-lingo_fit-0.1.0.dist-info/RECORD,,
+lingo_fit-0.2.0.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+lingo_fit-0.2.0.dist-info/METADATA,sha256=MnSVp-0YdJ7giqxIEwC69E1egkeAP3fCvuPp8-46g50,6835
+lingo_fit-0.2.0.dist-info/WHEEL,sha256=7Z8_27uaHI_UZAc4Uox4PpBhQ9Y5_modZXWMxtUi4NU,88
+lingo_fit-0.2.0.dist-info/RECORD,,
```

