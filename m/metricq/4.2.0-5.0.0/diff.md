# Comparing `tmp/metricq-4.2.0-py3-none-any.whl.zip` & `tmp/metricq-5.0.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,35 +1,38 @@
-Zip file size: 65820 bytes, number of entries: 33
--rw-r--r--  2.0 unx     2499 b- defN 22-Aug-25 11:45 metricq/__init__.py
--rw-r--r--  2.0 unx       72 b- defN 22-Aug-25 11:45 metricq/_protobuf_version.py
--rw-r--r--  2.0 unx    25488 b- defN 22-Aug-25 11:45 metricq/agent.py
--rw-r--r--  2.0 unx     9182 b- defN 22-Aug-25 11:45 metricq/client.py
--rw-r--r--  2.0 unx     5340 b- defN 22-Aug-25 11:45 metricq/connection_watchdog.py
--rw-r--r--  2.0 unx     5509 b- defN 22-Aug-25 11:45 metricq/data_client.py
--rw-r--r--  2.0 unx      999 b- defN 22-Aug-25 11:45 metricq/datachunk_pb2.py
--rw-r--r--  2.0 unx     1082 b- defN 22-Aug-25 11:45 metricq/datachunk_pb2.pyi
--rw-r--r--  2.0 unx     4535 b- defN 22-Aug-25 11:45 metricq/drain.py
--rw-r--r--  2.0 unx     3716 b- defN 22-Aug-25 11:45 metricq/exceptions.py
--rw-r--r--  2.0 unx    30618 b- defN 22-Aug-25 11:45 metricq/history_client.py
--rw-r--r--  2.0 unx     2685 b- defN 22-Aug-25 11:45 metricq/history_pb2.py
--rw-r--r--  2.0 unx     6142 b- defN 22-Aug-25 11:45 metricq/history_pb2.pyi
--rw-r--r--  2.0 unx     7446 b- defN 22-Aug-25 11:45 metricq/interval_source.py
--rw-r--r--  2.0 unx     1383 b- defN 22-Aug-25 11:45 metricq/logging.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-25 11:45 metricq/py.typed
--rw-r--r--  2.0 unx     5771 b- defN 22-Aug-25 11:45 metricq/rpc.py
--rw-r--r--  2.0 unx     8865 b- defN 22-Aug-25 11:45 metricq/sink.py
--rw-r--r--  2.0 unx    10625 b- defN 22-Aug-25 11:45 metricq/source.py
--rw-r--r--  2.0 unx     4168 b- defN 22-Aug-25 11:45 metricq/source_metric.py
--rw-r--r--  2.0 unx     4684 b- defN 22-Aug-25 11:45 metricq/subscription.py
--rw-r--r--  2.0 unx     5519 b- defN 22-Aug-25 11:45 metricq/synchronous_source.py
--rw-r--r--  2.0 unx    28067 b- defN 22-Aug-25 11:45 metricq/types.py
--rw-r--r--  2.0 unx     1700 b- defN 22-Aug-25 11:45 metricq/version.py
--rw-r--r--  2.0 unx     1559 b- defN 22-Aug-25 11:45 metricq_proto/LICENSE
--rw-r--r--  2.0 unx      417 b- defN 22-Aug-25 11:45 metricq_proto/README.md
--rw-r--r--  2.0 unx     1768 b- defN 22-Aug-25 11:45 metricq_proto/datachunk.proto
--rw-r--r--  2.0 unx     2858 b- defN 22-Aug-25 11:45 metricq_proto/history.proto
--rw-r--r--  2.0 unx     1559 b- defN 22-Aug-25 11:45 metricq-4.2.0.dist-info/LICENSE
--rw-r--r--  2.0 unx     6611 b- defN 22-Aug-25 11:45 metricq-4.2.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 22-Aug-25 11:45 metricq-4.2.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       22 b- defN 22-Aug-25 11:45 metricq-4.2.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2583 b- defN 22-Aug-25 11:45 metricq-4.2.0.dist-info/RECORD
-33 files, 193564 bytes uncompressed, 61764 bytes compressed:  68.1%
+Zip file size: 72099 bytes, number of entries: 36
+-rw-r--r--  2.0 unx     2680 b- defN 23-May-17 09:05 metricq/__init__.py
+-rw-r--r--  2.0 unx      124 b- defN 23-May-17 09:06 metricq/_protobuf_version.py
+-rw-r--r--  2.0 unx    31105 b- defN 23-May-17 09:05 metricq/agent.py
+-rw-r--r--  2.0 unx     9409 b- defN 23-May-17 09:05 metricq/client.py
+-rw-r--r--  2.0 unx     4990 b- defN 23-May-17 09:05 metricq/connection_watchdog.py
+-rw-r--r--  2.0 unx     5945 b- defN 23-May-17 09:05 metricq/data_client.py
+-rw-r--r--  2.0 unx     2394 b- defN 23-May-17 09:06 metricq/datachunk_pb2.py
+-rw-r--r--  2.0 unx     2776 b- defN 23-May-17 09:06 metricq/datachunk_pb2.pyi
+-rw-r--r--  2.0 unx     4600 b- defN 23-May-17 09:05 metricq/drain.py
+-rw-r--r--  2.0 unx     3716 b- defN 23-May-17 09:05 metricq/exceptions.py
+-rw-r--r--  2.0 unx    31510 b- defN 23-May-17 09:05 metricq/history_client.py
+-rw-r--r--  2.0 unx    13343 b- defN 23-May-17 09:06 metricq/history_pb2.py
+-rw-r--r--  2.0 unx     7949 b- defN 23-May-17 09:06 metricq/history_pb2.pyi
+-rw-r--r--  2.0 unx     7625 b- defN 23-May-17 09:05 metricq/interval_source.py
+-rw-r--r--  2.0 unx     1272 b- defN 23-May-17 09:05 metricq/logging.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-17 09:05 metricq/py.typed
+-rw-r--r--  2.0 unx     5606 b- defN 23-May-17 09:05 metricq/rpc.py
+-rw-r--r--  2.0 unx     9718 b- defN 23-May-17 09:05 metricq/sink.py
+-rw-r--r--  2.0 unx    11512 b- defN 23-May-17 09:05 metricq/source.py
+-rw-r--r--  2.0 unx     4173 b- defN 23-May-17 09:05 metricq/source_metric.py
+-rw-r--r--  2.0 unx     4658 b- defN 23-May-17 09:05 metricq/subscription.py
+-rw-r--r--  2.0 unx     7261 b- defN 23-May-17 09:05 metricq/synchronous_source.py
+-rw-r--r--  2.0 unx     1700 b- defN 23-May-17 09:05 metricq/version.py
+-rw-r--r--  2.0 unx       90 b- defN 23-May-17 09:05 metricq/pandas/__init__.py
+-rw-r--r--  2.0 unx     4463 b- defN 23-May-17 09:05 metricq/pandas/pandas_history_client.py
+-rw-r--r--  2.0 unx      330 b- defN 23-May-17 09:05 metricq/timeseries/__init__.py
+-rw-r--r--  2.0 unx      253 b- defN 23-May-17 09:05 metricq/timeseries/extras.py
+-rw-r--r--  2.0 unx     5060 b- defN 23-May-17 09:05 metricq/timeseries/time_aggregate.py
+-rw-r--r--  2.0 unx     1103 b- defN 23-May-17 09:05 metricq/timeseries/time_value.py
+-rw-r--r--  2.0 unx    15354 b- defN 23-May-17 09:05 metricq/timeseries/timedelta.py
+-rw-r--r--  2.0 unx     8467 b- defN 23-May-17 09:05 metricq/timeseries/timestamp.py
+-rw-r--r--  2.0 unx     1559 b- defN 23-May-17 09:06 metricq-5.0.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     6559 b- defN 23-May-17 09:06 metricq-5.0.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-May-17 09:06 metricq-5.0.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        8 b- defN 23-May-17 09:06 metricq-5.0.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2890 b- defN 23-May-17 09:06 metricq-5.0.0.dist-info/RECORD
+36 files, 220294 bytes uncompressed, 67541 bytes compressed:  69.3%
```

## zipnote {}

```diff
@@ -60,41 +60,50 @@
 
 Filename: metricq/subscription.py
 Comment: 
 
 Filename: metricq/synchronous_source.py
 Comment: 
 
-Filename: metricq/types.py
+Filename: metricq/version.py
 Comment: 
 
-Filename: metricq/version.py
+Filename: metricq/pandas/__init__.py
+Comment: 
+
+Filename: metricq/pandas/pandas_history_client.py
+Comment: 
+
+Filename: metricq/timeseries/__init__.py
+Comment: 
+
+Filename: metricq/timeseries/extras.py
 Comment: 
 
-Filename: metricq_proto/LICENSE
+Filename: metricq/timeseries/time_aggregate.py
 Comment: 
 
-Filename: metricq_proto/README.md
+Filename: metricq/timeseries/time_value.py
 Comment: 
 
-Filename: metricq_proto/datachunk.proto
+Filename: metricq/timeseries/timedelta.py
 Comment: 
 
-Filename: metricq_proto/history.proto
+Filename: metricq/timeseries/timestamp.py
 Comment: 
 
-Filename: metricq-4.2.0.dist-info/LICENSE
+Filename: metricq-5.0.0.dist-info/LICENSE
 Comment: 
 
-Filename: metricq-4.2.0.dist-info/METADATA
+Filename: metricq-5.0.0.dist-info/METADATA
 Comment: 
 
-Filename: metricq-4.2.0.dist-info/WHEEL
+Filename: metricq-5.0.0.dist-info/WHEEL
 Comment: 
 
-Filename: metricq-4.2.0.dist-info/top_level.txt
+Filename: metricq-5.0.0.dist-info/top_level.txt
 Comment: 
 
-Filename: metricq-4.2.0.dist-info/RECORD
+Filename: metricq-5.0.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## metricq/__init__.py

```diff
@@ -37,31 +37,43 @@
 from .interval_source import IntervalSource
 from .logging import get_logger
 from .rpc import rpc_handler
 from .sink import DurableSink, Sink
 from .source import Source
 from .subscription import Subscriber
 from .synchronous_source import SynchronousSource
-from .types import TimeAggregate, Timedelta, Timestamp, TimeValue
+from .timeseries import (
+    JsonDict,
+    MetadataDict,
+    Metric,
+    TimeAggregate,
+    Timedelta,
+    Timestamp,
+    TimeValue,
+)
 from .version import __version__
 
+# Please keep sorted alphabetically to avoid merge conflicts
 __all__ = [
-    "exceptions",
     "Agent",
     "Client",
     "DataClient",
+    "Drain",
     "DurableSink",
+    "exceptions",
+    "get_logger",
     "HistoryClient",
     "IntervalSource",
+    "JsonDict",
+    "MetadataDict",
+    "Metric",
+    "rpc_handler",
     "Sink",
     "Source",
     "Subscriber",
-    "Drain",
     "SynchronousSource",
+    "TimeAggregate",
     "Timedelta",
     "Timestamp",
     "TimeValue",
-    "TimeAggregate",
-    "get_logger",
-    "rpc_handler",
     "__version__",
 ]
```

## metricq/_protobuf_version.py

```diff
@@ -1,2 +1,3 @@
-_protobuf_version = "3.20.1"
-_protobuf_requirement = "protobuf~=3.20.0"
+# file generated on 2023-05-17 09:06:30.258581
+_protobuf_version = "3.12.4"
+_protobuf_requirement = "protobuf>=3.12, <3.13"
```

## metricq/agent.py

```diff
@@ -28,101 +28,159 @@
 # NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 import asyncio
 import functools
 import json
 import signal
-import ssl
 import textwrap
 import threading
 import time
 import traceback
 import uuid
+from collections.abc import Callable, Iterable, Mapping
 from contextlib import suppress
-from typing import (
-    Any,
-    Callable,
-    Dict,
-    Iterable,
-    List,
-    Optional,
-    Tuple,
-    TypeVar,
-    Union,
-    cast,
-)
+from itertools import chain
+from typing import Any, Optional, TypeVar
+from warnings import warn
 
 import aio_pika
-from aio_pika.exceptions import ChannelInvalidStateError
 from yarl import URL
 
 from .connection_watchdog import ConnectionWatchdog
 from .exceptions import (
     AgentStopped,
     ConnectFailed,
     PublishFailed,
     ReceivedSignal,
     ReconnectTimeout,
     RPCError,
 )
 from .logging import get_logger
 from .rpc import RPCDispatcher
-from .types import JsonDict
+from .timeseries import JsonDict
 from .version import __version__
 
 logger = get_logger(__name__)
 timer = time.monotonic
 
 T = TypeVar("T")
 
 _global_thread_lock = threading.Lock()
 
 
 class Agent(RPCDispatcher):
+    """
+    Base class for all MetricQ agents - i.e. clients that are connected to the
+    RabbitMQ MetricQ network.
+
+    The lifetime of an agent works as follows:
+
+    * create an instance
+    * ``await`` :meth:`connect`
+    * ``await`` :meth:`stop`
+
+    :meth:`stop` is automatically invoked if the connection is lost and the reconnect
+    timeout is exceeded. It may also be called manually to stop the agent.
+
+    To indefinitely wait for the agent to stop, use :meth:`stopped`.
+
+    Usually, :meth:`connect` and  :meth:`stop` are not called directly, but instead:
+
+    * The class (via its child :class:`Client`) is used as an asynchronous context
+      manager, e.g. ``async with Client(...) as agent:``. In that case,
+      :meth:`connect` and :meth:`stop` are called as part of the context.
+
+    * The synchronous :meth:`run` method is called, which:
+
+      * sets up a signal handler for ``SIGINT`` and ``SIGTERM`` that calls :meth:`stop`.
+      * sets up a loop exception handler that calls :meth:`stop`.
+      * calls :meth:`connect` and :meth:`stopped` to run indefinitely until
+        :meth:`stop` is called.
+
+    Within :meth:`stop`, the Agent will invoke :meth:`close` to allow the all child
+    classes to perform any necessary cleanup. Implementations of :meth:`teardown` should
+    call ``super().teardown()``, possibly in an ``asyncio.gather``. :meth:`stop`
+    wraps the invocation of :meth:`close` in a timeout (``_close_timeout``) and logs any
+    errors during cleanup, possibly passing it to anyone waiting for :meth:`stopped`.
+    """
+
     LOG_MAX_WIDTH = 200
 
     def __init__(
         self,
         token: str,
-        management_url: str,
+        url: Optional[str] = None,
         *,
-        connection_timeout: Union[int, float] = 600,
+        connection_timeout: int | float = 600,
         add_uuid: bool = False,
+        management_url: Optional[str] = None,
     ):
+        """
+        Args:
+            token: The token of the agent.
+            url:
+                The amqp(s) URL of the MetricQ management network.
+            add_uuid:
+                Whether to add a UUID to the token. This is used for transient clients
+                without centralized configuration.
+            connection_timeout:
+                The timeout (in seconds) for reconnecting.
+        """
         self.token = f"{token}.{uuid.uuid4().hex}" if add_uuid else token
 
+        if management_url is not None:
+            warn(
+                "using deprecated 'management_url' argument, please use 'url' instead",
+                DeprecationWarning,
+            )
+            if url is not None:
+                raise TypeError(
+                    "cannot use both 'url' and 'management_url', use only 'url'"
+                )
+            url = management_url
+
+        if url is None:
+            raise TypeError("missing required positional argument 'url'")
+
         self._loop: Optional[asyncio.AbstractEventLoop] = None
         self._stop_in_progress = False
-        self._stop_future: Optional[asyncio.Future[None]] = None
         self._cancel_on_exception = False
+        self._close_timeout = connection_timeout
+
+        # Cannot create it here because creating a future requires a running event loop
+        self.__stop_future: Optional[asyncio.Future[None]] = None
 
-        self._management_url = management_url
+        self._management_url = url
         self._management_broadcast_exchange_name = "metricq.broadcast"
         self._management_exchange_name = "metricq.management"
 
-        self._management_connection: Optional[aio_pika.RobustConnection] = None
+        self._management_connection: Optional[
+            aio_pika.abc.AbstractRobustConnection
+        ] = None
         self._management_connection_watchdog = ConnectionWatchdog(
             on_timeout_callback=lambda watchdog: self._schedule_stop(
                 ReconnectTimeout(
                     f"Failed to reestablish {watchdog.connection_name} after {watchdog.timeout} seconds"
                 )
             ),
             timeout=connection_timeout,
             connection_name="management connection",
         )
-        self._management_channel: Optional[aio_pika.RobustChannel] = None
+        self._management_channel: Optional[aio_pika.abc.AbstractChannel] = None
 
-        self.management_rpc_queue: Optional[aio_pika.Queue] = None
+        self.management_rpc_queue: Optional[aio_pika.abc.AbstractQueue] = None
 
-        self._management_broadcast_exchange: Optional[aio_pika.Exchange] = None
-        self._management_exchange: Optional[aio_pika.Exchange] = None
+        self._management_broadcast_exchange: Optional[
+            aio_pika.abc.AbstractExchange
+        ] = None
+        self._management_exchange: Optional[aio_pika.abc.AbstractExchange] = None
 
-        self._rpc_response_handlers: Dict[
-            str, Tuple[Callable[..., None], bool]
+        self._rpc_response_handlers: dict[
+            str, tuple[Callable[..., None], bool]
         ] = dict()
         logger.info(
             "Initialized Agent `{}` (running version `metricq=={}`)",
             type(self).__qualname__,
             __version__,
         )
 
@@ -149,70 +207,72 @@
             with _global_thread_lock:
                 if self._loop is None:
                     self._loop = loop
         if loop is not self._loop:
             raise RuntimeError(f"{self!r} is bound to a different event loop")
         return loop
 
+    @property
+    def _stop_future(self) -> asyncio.Future[None]:
+        if self.__stop_future is None:
+            self.__stop_future = self._event_loop.create_future()
+        return self.__stop_future
+
     async def make_connection(
-        self, url: str, connection_name: Optional[str] = None
-    ) -> aio_pika.RobustConnection:
-        ssl_options = None
-
-        if url.startswith("amqps"):
-            ssl_options = {
-                "cert_reqs": ssl.CERT_REQUIRED,
-                "ssl_version": ssl.PROTOCOL_TLS | ssl.OP_NO_SSLv2 | ssl.OP_NO_SSLv3,
-            }
-
-        client_properties = None
-        if connection_name:
-            # TODO remove outer client_properties with aiormq >= 5.1.1
-            client_properties = {
-                "client_properties": {"connection_name": connection_name}
-            }
-
-        connection: aio_pika.RobustConnection = await aio_pika.connect_robust(
-            url,
-            reconnect_interval=30,
-            ssl_options=cast(Dict[Any, Any], ssl_options),
-            client_properties=cast(Dict[Any, Any], client_properties),
+        self, url: str, connection_name: str
+    ) -> aio_pika.abc.AbstractRobustConnection:
+        url_obj = URL(url).with_query(
+            {"reconnect_interval": 5, "fail_fast": 1, "name": connection_name}
+        )
+        connection = await aio_pika.connect_robust(
+            url_obj,
+            # If the following bugfix released, we can MAYBE use the following code again
+            # instead of hacking the url query parameters
+            # https://github.com/mosquito/aio-pika/pull/531
+            # reconnect_interval=30,
+            # fail_fast=True,
+            # name=connection_name,
         )
 
         # How stupid that we can't easily add the handlers *before* actually connecting.
         # We could make our own RobustConnection object, but then we loose url parsing convenience
-        connection.add_reconnect_callback(self._on_reconnect)  # type: ignore
-        connection.add_close_callback(self._on_close)
+        connection.reconnect_callbacks.add(self._on_reconnect)
+        connection.close_callbacks.add(self._on_close)
 
         return connection
 
     async def connect(self) -> None:
         """Connect to the MetricQ network"""
         logger.info(
             "establishing management connection to {}",
-            URL(self._management_url).with_password("***"),
-        )
-
-        self._management_connection = await self.make_connection(
-            self._management_url,
-            connection_name="management connection {}".format(self.token),
-        )
-        self._management_connection.add_close_callback(
-            self._on_management_connection_close
-        )
-
-        self._management_connection.add_reconnect_callback(
-            self._on_management_connection_reconnect  # type: ignore
+            URL(self._management_url).with_password("******"),
         )
 
-        self._management_channel = await self._management_connection.channel()
-        assert self._management_channel is not None
-        self.management_rpc_queue = await self._management_channel.declare_queue(
-            "{}-rpc".format(self.token), exclusive=True
-        )
+        try:
+            connection = await self.make_connection(
+                self._management_url,
+                connection_name="management connection {}".format(self.token),
+            )
+            self._management_connection = connection
+            connection.close_callbacks.add(self._on_management_connection_close)
+            connection.reconnect_callbacks.add(self._on_management_connection_reconnect)
+
+            self._management_channel = await connection.channel()
+            assert self._management_channel is not None
+            self.management_rpc_queue = await self._management_channel.declare_queue(
+                "{}-rpc".format(self.token), exclusive=True
+            )
+        except Exception as e:
+            logger.error(
+                "Failed to connect {}: {} ({})",
+                type(self).__qualname__,
+                e,
+                type(e).__qualname__,
+            )
+            raise ConnectFailed("Failed to connect Agent") from e
 
         self._management_connection_watchdog.start()
         self._management_connection_watchdog.set_established()
 
     def run(
         self,
         catch_signals: Iterable[str] = ("SIGINT", "SIGTERM"),
@@ -222,15 +282,15 @@
 
         If :meth:`connect` raises an exception, :exc:`.ConnectFailed` is
         raised, with the offending exception attached as a cause.  Any
         exception passed to :meth:`stop` is reraised.
 
         Args:
             catch_signals:
-                Call :meth:`on_signal` if any of theses signals were raised.
+                Call :meth:`on_signal` if any of these signals were raised.
             cancel_on_exception:
                 Stop the running Agent when an unhandled exception occurs.
                 The exception is reraised from this method.
 
         Raises:
             ConnectFailed:
                 Failed to :meth:`connect` to the MetricQ network.
@@ -282,15 +342,15 @@
             # If the Agent was stopped explicitly, return `None`.  If it was
             # stopped because of an exception, reraise it.
             if stopped_task in done:
                 return stopped_task.result()
 
     async def rpc(
         self,
-        exchange: aio_pika.Exchange,
+        exchange: aio_pika.abc.AbstractExchange,
         routing_key: str,
         function: str,
         response_callback: Optional[Callable[..., None]] = None,
         timeout: float = 60,
         cleanup_on_response: bool = True,
         **kwargs: Any,
     ) -> Optional[JsonDict]:
@@ -370,15 +430,15 @@
 
         request_future = None
 
         if response_callback is None:
             request_future = self._event_loop.create_future()
 
             if not cleanup_on_response:
-                # We must cleanup when we use the future otherwise we get errors
+                # We must clean up when we use the future otherwise we get errors
                 # trying to set the future result multiple times ... after the future was
                 # already evaluated
                 raise TypeError(
                     "Neither a response_callback was given nor cleanup_on_response was set."
                 )
 
             def default_response_callback(**response_kwargs: Any) -> None:
@@ -388,22 +448,25 @@
                 if "error" in response_kwargs:
                     request_future.set_exception(RPCError(response_kwargs["error"]))
                 else:
                     request_future.set_result(response_kwargs)
 
             response_callback = default_response_callback
 
+        # May VTTI be with you ༼ つ ╹ ╹ ༽つ
+        assert callable(response_callback)
+
         self._rpc_response_handlers[correlation_id] = (
             response_callback,
             cleanup_on_response,
         )
 
         try:
             await exchange.publish(msg, routing_key=routing_key)
-        except ChannelInvalidStateError as e:
+        except aio_pika.exceptions.ChannelInvalidStateError as e:
             errmsg = (
                 f"Failed to issue RPC request '{function!r}' to exchange ''{exchange}'"
             )
             logger.error("{}: {}", errmsg, e)
             raise PublishFailed(errmsg) from e
 
         def cleanup() -> None:
@@ -419,152 +482,199 @@
                 cleanup()
                 raise te
         elif timeout:
             self._event_loop.call_later(timeout, cleanup)
 
         return None
 
-    async def rpc_consume(self, extra_queues: List[aio_pika.Queue] = []) -> None:
+    async def rpc_consume(self, extra_queues: Iterable[aio_pika.Queue] = []) -> None:
         """Start consuming RPCs
 
         :meta private:
 
-        Typically this is called at the end of :meth:`Client.connect` once the Agent is prepared to handle RPCs.
+        Typically, this is called at the end of :meth:`Client.connect` once the
+        Agent is prepared to handle RPCs.
 
         Args:
             extra_queues: additional queues on which to receive RPCs
         """
         logger.info("starting RPC consume")
         assert self.management_rpc_queue is not None
-        queues = [self.management_rpc_queue] + extra_queues
+        queues = chain([self.management_rpc_queue], extra_queues)
         await asyncio.gather(
             *[queue.consume(self._on_management_message) for queue in queues]
         )
 
     def on_signal(self, signal: str) -> None:
         """Callback invoked when a signal is received.
 
         Override this method for custom signal handling.
-        By default it schedules the Client to stop by calling :meth:`stop`.
+        By default, it schedules the Client to stop by calling :meth:`stop`.
 
         Args:
-            signal: Name of the signal that occurred, e.g. :code:`"SIGTERM"`, :code:`"SIGINT"`, etc.
+            signal: Name of the signal that occurred, e.g. :code:`"SIGTERM"`,
+                    :code:`"SIGINT"`, etc.
         """
         logger.info("Received signal {}, stopping...", signal)
         self._schedule_stop(
             exception=None if signal == "SIGINT" else ReceivedSignal(signal)
         )
 
     def on_exception(
-        self, loop: asyncio.AbstractEventLoop, context: Dict[str, Any]
+        self, loop: asyncio.AbstractEventLoop, context: Mapping[str, Any]
     ) -> None:
         logger.error("Exception in event loop: {}".format(context["message"]))
+        if loop.is_closed():
+            logger.error("Received exception in closed loop.")
+        elif loop != self._event_loop:
+            logger.error(
+                f"Exception happened in a loop {loop} "
+                f"other than the internal loop {self._event_loop}. "
+                "This should never happen."
+            )
 
         with suppress(KeyError):
             logger.error("Future: {}", context["future"])
 
         with suppress(KeyError):
             logger.error("Handle: {}", context["handle"])
 
         ex: Optional[Exception] = context.get("exception")
         if ex is not None:
             is_keyboard_interrupt = isinstance(ex, KeyboardInterrupt)
-            if self._cancel_on_exception or is_keyboard_interrupt:
+            if (
+                self._cancel_on_exception or is_keyboard_interrupt
+            ) and loop.is_running():
                 if not is_keyboard_interrupt:
                     logger.error(
                         "Stopping Agent on unhandled exception ({})",
                         type(ex).__qualname__,
                     )
-                self._schedule_stop(exception=ex, loop=loop)
+                self._schedule_stop(exception=ex)
             else:
                 logger.error(
                     f"Agent {type(self).__qualname__} encountered an unhandled exception",
                     exc_info=(ex.__class__, ex, ex.__traceback__),
                 )
 
     def _schedule_stop(
         self,
         exception: Optional[Exception] = None,
-        loop: Optional[asyncio.AbstractEventLoop] = None,
     ) -> None:
-        loop = self._event_loop if loop is None else loop
-        loop.create_task(self.stop(exception=exception))
+        self._event_loop.create_task(self.stop(exception=exception, silent=True))
 
-    async def stop(self, exception: Optional[Exception] = None) -> None:
-        """Stop a running Agent.
+    async def stop(
+        self, exception: Optional[BaseException] = None, silent: bool = False
+    ) -> None:
+        """
+        Stop a running Agent. When calling stop multiple times, all but the
+        first call will be ignored. It will inform anyone waiting for
+        :meth:`stopped` about the completion or the given exception.
 
         Args:
             exception:
                 An optional exception that will be raised by :meth:`run` if given.
                 If the Agent was not started from :meth:`run`, see :meth:`stopped`
                 how to retrieve this exception.
+            silent:
+                If set to :code:`True`, a passed exception will not be raised.
+
+        Raises:
+            AgentStopped:
+                If an ``exception`` is given or an exception occurred while closing
+                the connection(s) and ``silent==False``.
         """
         if self._stop_in_progress:
-            logger.debug("Stop in progress! ({})", exception)
+            logger.warning(
+                "Stop in progress, ignoring (exception: {}, silent: {})",
+                exception,
+                silent,
+            )
             return
-        else:
-            self._stop_in_progress = True
 
-            logger.info("Stopping Agent {} ({})...", type(self).__qualname__, exception)
+        self._stop_in_progress = True
 
-            await asyncio.shield(self._close())
+        logger.info("Stopping Agent {} ({})...", type(self).__qualname__, exception)
 
-            if self._stop_future is None:
-                # No task is waiting for the Agent to stop.
-                if exception is not None:
-                    # Wrap the exception (to preserve traceback information)
-                    # and reraise it.
-                    raise AgentStopped("Agent stopped unexpectedly") from exception
-                else:
-                    return
-            else:
-                assert not self._stop_future.done()
-                if exception is None:
-                    self._stop_future.set_result(None)
-                else:
-                    self._stop_future.set_exception(exception)
+        try:
+            # I tried so hard to `shield` this call
+            # But in the end, it doesn't even matter
+            # The task is canceled, the loop is closed
+            # But in the end, it doesn't even matter
+            #
+            # I tried to close connections clean,
+            # A perfect end, that's what I mean,
+            # But when the network grew stale,
+            # My efforts were to no avail.
+            #
+            # The close packets, they couldn't send,
+            # A deadlock near, I can't pretend,
+            # So I added a timeout here,
+            # To break free from the chains of fear.
+            await asyncio.wait_for(self.teardown(), self._close_timeout)
+        except BaseException as close_exception:
+            logger.error("Error while closing Agent: {}", close_exception)
+            if not exception:
+                exception = close_exception
+
+        assert not self._stop_future.done()
+        if exception is None:
+            self._stop_future.set_result(None)
+        else:
+            self._stop_future.set_exception(exception)
+
+        if not silent and exception is not None:
+            raise AgentStopped("Agent stopped with error") from exception
 
     async def stopped(self) -> None:
         """Wait for this Agent to stop.
 
         If the agent stopped unexpectedly, this method raises an exception.
 
         Raises:
             AgentStopped:
                 The Agent was stopped via :meth:`stop` and an exception was passed.
             Exception:
                 The Agent encountered any other unhandled exception.
         """
-        if self._stop_future is None:
-            self._stop_future = self._event_loop.create_future()
         await self._stop_future
 
-    async def _close(self) -> None:
+    async def teardown(self) -> None:
+        """
+        .. Important::
+            Do not call this function, it is called indirectly by :meth:`Agent.stop`.
+
+        Close all connections and channels. Child classes should implement this
+        method to close their own connections and channels and call
+        ``super().teardown()``.
+        """
         logger.info("Closing management channel and connection...")
         await self._management_connection_watchdog.stop()
         if self._management_channel:
-            await self._management_channel.close()  # type: ignore
+            await self._management_channel.close()
             self._management_channel = None
         if self._management_connection:
-            await self._management_connection.close()  # type: ignore
+            await self._management_connection.close()
             self._management_connection = None
         self._management_broadcast_exchange = None
         self._management_exchange = None
 
     def _make_correlation_id(self) -> str:
         return "metricq-rpc-py-{}-{}".format(self.token, uuid.uuid4().hex)
 
-    async def _on_management_message(self, message: aio_pika.IncomingMessage) -> None:
+    async def _on_management_message(
+        self, message: aio_pika.abc.AbstractIncomingMessage
+    ) -> None:
         """Callback invoked when a message is received.
 
         Args:
             message: Either an RPC request or an RPC response.
 
         Raises:
-            PublishError: The reply could not be published.
+            PublishFailed: The reply could not be published.
         """
         assert self._management_channel is not None
         assert self._management_channel.default_exchange is not None
 
         async with message.process(requeue=True):
             time_begin = timer()
             body = message.body.decode()
@@ -581,14 +691,20 @@
             )
             arguments = json.loads(body)
             arguments["from_token"] = from_token
 
             function = arguments.get("function")
             if function is not None:
                 logger.debug("message is an RPC")
+                if not message.reply_to:
+                    logger.warning(
+                        "RPC request from {} has no reply_to, ignoring",
+                        from_token,
+                    )
+                    return
                 try:
                     response = await self.rpc_dispatch(**arguments)
                 except Exception as e:
                     logger.error(
                         "error handling RPC {} ({}): {}",
                         function,
                         type(e),
@@ -614,57 +730,72 @@
                             body=body.encode(),
                             correlation_id=correlation_id,
                             content_type="application/json",
                             app_id=self.token,
                         ),
                         routing_key=message.reply_to,
                     )
-                except ChannelInvalidStateError as e:
+                except aio_pika.exceptions.ChannelInvalidStateError as e:
                     errmsg = (
                         "Failed to reply to '{message.reply_to}' for RPC '{function!r}'"
                     )
                     logger.error("{}: {}", errmsg, e)
                     raise PublishFailed(errmsg) from e
             else:
                 logger.debug("message is an RPC response")
+                if correlation_id is None:
+                    logger.error(
+                        "received RPC response from {} without correlation id, ignoring",
+                        from_token,
+                    )
+                    return
                 try:
                     handler, cleanup = self._rpc_response_handlers[correlation_id]
                 except KeyError:
                     logger.error(
                         "received RPC response with unknown correlation id {} from {}",
                         correlation_id,
                         from_token,
                     )
                     # We do not throw here, no requeue for this!
                     return
                 if cleanup:
                     del self._rpc_response_handlers[correlation_id]
 
-                if not handler:
-                    return
+                # I'd agree here with Mypy that handler is always truthy here. There should
+                # not be an entry in _rpc_response_handler with a handler of None.
+                # Either there is no entry in the first place, or the entry is valid.
+                # May VTTI be with you. \(^-^)/
+                assert handler is not None
 
                 # Allow simple handlers that are not coroutines
                 # But only None to not get any confusion
                 r = handler(**arguments)
                 if r is not None:
                     await r
 
-    def _on_reconnect(self, sender: Any, connection: aio_pika.RobustConnection) -> None:
-        logger.info("Reconnected to {}", connection)
+    def _on_reconnect(self, sender: aio_pika.abc.AbstractRobustConnection) -> None:
+        logger.info("Reconnected to {}", sender)
 
-    def _on_close(self, sender: Any, exception: Optional[BaseException]) -> None:
+    def _on_close(
+        self,
+        sender: aio_pika.abc.AbstractRobustConnection,
+        exception: Optional[BaseException],
+    ) -> None:
         if isinstance(exception, asyncio.CancelledError):
             logger.debug("Connection closed regularly")
             return
         logger.info(
             "Connection closed: {} ({})", exception, type(exception).__qualname__
         )
 
     def _on_management_connection_reconnect(
-        self, sender: Any, connection: aio_pika.RobustConnection
+        self, sender: aio_pika.abc.AbstractRobustConnection
     ) -> None:
         self._management_connection_watchdog.set_established()
 
     def _on_management_connection_close(
-        self, sender: Any, _exception: Optional[BaseException]
+        self,
+        sender: aio_pika.abc.AbstractRobustConnection,
+        _exception: Optional[BaseException],
     ) -> None:
         self._management_connection_watchdog.set_closed()
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## metricq/client.py

```diff
@@ -24,30 +24,29 @@
 # EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 # PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 # PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 # LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 # NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
+from collections.abc import Sequence
 from socket import gethostname
 from sys import version_info as sys_version
 from types import TracebackType
-from typing import Any, Dict, Optional, Sequence, Type, TypeVar, Union, cast
+from typing import Any, Optional, TypeVar, cast
 
 from .agent import Agent
 from .logging import get_logger
 from .rpc import rpc_handler
-from .types import JsonDict, Timestamp
+from .timeseries import JsonDict, Timestamp
 from .version import __version__
 
 logger = get_logger(__name__)
 
 
-_GetMetricsResult = Union[Sequence[str], Sequence[Dict[str, Any]]]
-
 # With Python 3.11 use typing.Self instead
 Self = TypeVar("Self", bound="Client")
 
 
 class Client(Agent):
     def __init__(self, *args: Any, client_version: Optional[str] = None, **kwargs: Any):
         super().__init__(*args, **kwargs)
@@ -105,15 +104,15 @@
         assert self.management_rpc_queue is not None
         await self.management_rpc_queue.bind(
             exchange=self._management_broadcast_exchange, routing_key="#"
         )
 
         await self.rpc_consume()
 
-    # The superclass has extra parameters, which we fill in in the overloads of the subclasses.
+    # The superclass has extra parameters, which we fill in the overloads of the subclasses.
     # So this is fine! But mypy complains and we carefully considered the feedback.
     async def rpc(  # type: ignore
         self, function: str, *args: Any, **kwargs: Any
     ) -> Optional[JsonDict]:
         """Invoke an RPC on the management exchange
 
         Args:
@@ -162,23 +161,23 @@
         if self._client_version is not None:
             response["version"] = self._client_version
 
         return response
 
     async def get_metrics(
         self,
-        selector: Union[str, Sequence[str], None] = None,
+        selector: str | Sequence[str] | None = None,
         metadata: bool = True,
         historic: Optional[bool] = None,
         timeout: Optional[float] = None,
         prefix: Optional[str] = None,
         infix: Optional[str] = None,
         limit: Optional[int] = None,
         hidden: Optional[bool] = None,
-    ) -> _GetMetricsResult:
+    ) -> dict[str, JsonDict]:
         """Retrieve information for metrics matching a selector pattern.
 
         Args:
             selector:
                 Either:
 
                 * a regex matching parts of the metric name
@@ -196,20 +195,19 @@
             limit:
                 Maximum number of matches to return.
             hidden:
                 Only include metrics where :literal:`hidden` is :literal:`True`/:literal:`False`. If not set return all
                 matching metrics.
 
         Returns:
-            *
-                a dictionary mapping matching metric names to their
-                :ref:`metadata<metric-metadata>` (if :code:`metadata=True`)
-            * otherwise, a sequence of matching metric names
+            * a dictionary mapping matching metric names to their
+              :ref:`metadata<metric-metadata>` ( :code:`if metadata==True`)
+            * otherwise, a dictionary mapping metric names to empty dicts
         """
-        arguments: Dict[str, Any] = {"format": "object" if metadata else "array"}
+        arguments: dict[str, Any] = {"format": "object" if metadata else "array"}
         if selector is not None:
             arguments["selector"] = selector
         if timeout is not None:
             arguments["timeout"] = timeout
         if historic is not None:
             arguments["historic"] = historic
         if prefix is not None:
@@ -221,31 +219,38 @@
         if hidden is not None:
             arguments["hidden"] = hidden
 
         # Note: checks are done in the manager (e.g. must not have prefix and historic/selector at the same time)
 
         result = await self.rpc("get_metrics", **arguments)
         assert result is not None
-        return cast(_GetMetricsResult, result["metrics"])
+        metrics = result["metrics"]
+        if not metadata:
+            assert isinstance(metrics, list)
+            metrics = {m: {} for m in metrics}
+        assert isinstance(metrics, dict)
+        assert all(isinstance(v, dict) for v in metrics.values())
+        assert all(isinstance(k, str) for k in metrics.keys())
+        return metrics
 
     async def __aenter__(self: Self) -> Self:
         """Allows to use the Client as a context manager.
 
-        The connection to MetricQ will automatically established and closed.
+        The connection to MetricQ will be automatically established and closed.
 
         Use it like this::
 
             async with MyClient(...) as client:
                 pass
 
         """
         await self.connect()
 
         return self
 
     async def __aexit__(
         self,
-        exc_type: Optional[Type[BaseException]],
+        exc_type: Optional[type[BaseException]],
         exc_value: Optional[BaseException],
         traceback: Optional[TracebackType],
     ) -> None:
         await self.stop()
```

## metricq/connection_watchdog.py

```diff
@@ -1,21 +1,22 @@
 import asyncio
 from asyncio import CancelledError, Event, Task, TimeoutError, wait_for
-from typing import Callable, Optional, Union
+from collections.abc import Callable
+from typing import Optional
 
 from .logging import get_logger
 
 logger = get_logger(__name__)
 
 
 class ConnectionWatchdog:
     def __init__(
         self,
         on_timeout_callback: Callable[["ConnectionWatchdog"], None],
-        timeout: Union[int, float],
+        timeout: int | float,
         connection_name: str = "connection",
     ):
         """Watch a connection, fire a callback if it failed to reconnect before
         the given timeout.
 
         This class wraps a watchdog task that asynchronously waits for
         established/closed events.  Use :py:meth:`start` to start the
@@ -28,65 +29,58 @@
                 Time duration given until the connection is considered to have failed to reconnect.
                 Use :meth:`set_established` to signal reconnection.
             connection_name:
                 Human readable name of the connection, used in log messages.
         """
         self.connection_name = connection_name
         self.timeout = timeout
-
         self._callback = on_timeout_callback
-
         self._closed_event: Optional[Event] = None
         self._established_event: Optional[Event] = None
         self._watchdog_task: Optional[Task[None]] = None
 
+    async def _run(self) -> None:
+        logger.debug("Started {} watchdog", self.connection_name)
+        try:
+            cap_connection_name = self.connection_name.capitalize()
+            while True:
+                try:
+                    await wait_for(self.established(), timeout=self.timeout)
+                    logger.debug("{} established", cap_connection_name)
+                except TimeoutError:
+                    logger.warning(
+                        "{} failed to reconnect after {} seconds",
+                        cap_connection_name,
+                        self.timeout,
+                    )
+                    self._callback(self)
+                    return
+
+                await self.closed()
+                logger.debug("{} was closed", cap_connection_name)
+
+        except CancelledError:
+            logger.debug("Cancelled {} watchdog", self.connection_name)
+            raise
+
     def start(self) -> None:
         """Start the connection watchdog task.
 
         A call to this method will have no effect if the task is already
         running.
         """
         if self._watchdog_task:
             logger.warning(
                 "ConnectionWatchdog for {} already started", self.connection_name
             )
             return
 
         self._closed_event = Event()
         self._established_event = Event()
-
-        async def watchdog() -> None:
-            logger.debug("Started {} watchdog", self.connection_name)
-            try:
-                cap_connection_name = self.connection_name.capitalize()
-                while True:
-                    try:
-                        assert self._established_event is not None
-                        await wait_for(
-                            self._established_event.wait(), timeout=self.timeout
-                        )
-                        logger.debug("{} established", cap_connection_name)
-                    except TimeoutError:
-                        logger.warning(
-                            "{} failed to reconnect after {} seconds",
-                            cap_connection_name,
-                            self.timeout,
-                        )
-                        self._callback(self)
-                        break
-
-                    assert self._closed_event is not None
-                    await self._closed_event.wait()
-                    logger.debug("{} was closed", cap_connection_name)
-
-            except CancelledError:
-                logger.debug("Cancelled {} watchdog", self.connection_name)
-                raise
-
-        self._watchdog_task = asyncio.get_running_loop().create_task(watchdog())
+        self._watchdog_task = asyncio.create_task(self._run())
 
     def set_established(self) -> None:
         """Signal that the connection has been established."""
         assert (
             self._closed_event is not None
             and self._established_event is not None
             and self._watchdog_task is not None
@@ -113,31 +107,33 @@
     async def established(self) -> None:
         """Asynchronously wait for the connection to be established."""
         assert self._established_event is not None
         await self._established_event.wait()
 
     async def stop(self) -> None:
         """Stop the connection watchdog task if it is running."""
-        if self._watchdog_task:
-            if self._watchdog_task.done():
-                try:
-                    logger.warn(
-                        "Watchdog task {} already done with result: {}",
-                        self.connection_name,
-                        self._watchdog_task.result(),
-                    )
-                except Exception as e:
-                    logger.error(
-                        "Watchdog task {} already done with exception: {}",
-                        self.connection_name,
-                        e,
-                    )
-
-            self._watchdog_task.cancel()
+        if not self._watchdog_task:
+            return
 
+        if self._watchdog_task.done():
             try:
-                await self._watchdog_task
-            except CancelledError:
-                logger.debug("Stopping {} watchdog complete", self.connection_name)
-            self._watchdog_task = None
-            self._established_event = None
-            self._closed_event = None
+                logger.warning(
+                    "Watchdog task {} already done with result: {}",
+                    self.connection_name,
+                    self._watchdog_task.result(),
+                )
+            except Exception as e:
+                logger.error(
+                    "Watchdog task {} already done with exception: {}",
+                    self.connection_name,
+                    e,
+                )
+
+        self._watchdog_task.cancel()
+
+        try:
+            await self._watchdog_task
+        except CancelledError:
+            logger.debug("Stopping {} watchdog complete", self.connection_name)
+        self._watchdog_task = None
+        self._established_event = None
+        self._closed_event = None
```

## metricq/data_client.py

```diff
@@ -22,17 +22,18 @@
 # EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 # PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 # PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 # LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 # NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
-from typing import Any, Optional, cast
+import asyncio
+from typing import Any, Optional
 
-import aio_pika
+import aio_pika.abc
 from yarl import URL
 
 from .client import Client
 from .connection_watchdog import ConnectionWatchdog
 from .exceptions import ReconnectTimeout
 from .logging import get_logger
 
@@ -40,17 +41,17 @@
 
 
 class DataClient(Client):
     def __init__(self, *args: Any, **kwargs: Any):
         super().__init__(*args, **kwargs)
 
         self.data_server_address: Optional[str] = None
-        self.data_connection: Optional[aio_pika.RobustConnection] = None
-        self.data_channel: Optional[aio_pika.RobustChannel] = None
-        self.data_exchange: Optional[aio_pika.Exchange] = None
+        self.data_connection: Optional[aio_pika.abc.AbstractRobustConnection] = None
+        self.data_channel: Optional[aio_pika.abc.AbstractRobustChannel] = None
+        self.data_exchange: Optional[aio_pika.abc.AbstractExchange] = None
         self._data_connection_watchdog = ConnectionWatchdog(
             on_timeout_callback=lambda watchdog: self._schedule_stop(
                 ReconnectTimeout(
                     f"Failed to reestablish {watchdog.connection_name} after {watchdog.timeout} seconds"
                 )
             ),
             timeout=kwargs.get("connection_timeout", 60),
@@ -81,46 +82,57 @@
             )
             self.data_server_address = dataServerAddress
             self.data_connection = await self.make_connection(
                 self.data_server_address,
                 connection_name="data connection {}".format(self.token),
             )
 
-            self.data_connection.add_close_callback(self._on_data_connection_close)
-            self.data_connection.add_reconnect_callback(
-                self._on_data_connection_reconnect  # type: ignore
+            self.data_connection.close_callbacks.add(self._on_data_connection_close)
+            self.data_connection.reconnect_callbacks.add(
+                self._on_data_connection_reconnect
             )
 
             # publisher confirms seem to be buggy, disable for now
-            self.data_channel = cast(
-                aio_pika.RobustChannel,
-                await self.data_connection.channel(publisher_confirms=False),
-            )
-
+            channel = await self.data_connection.channel(publisher_confirms=False)
+            assert isinstance(channel, aio_pika.abc.AbstractRobustChannel)
+            self.data_channel = channel
             # TODO configurable prefetch count
-            await self.data_channel.set_qos(prefetch_count=400)
+            await channel.set_qos(prefetch_count=400)
 
             self._data_connection_watchdog.start()
             self._data_connection_watchdog.set_established()
 
-    async def stop(self, exception: Optional[Exception] = None) -> None:
-        logger.info("closing data channel and connection.")
+    async def __close(self) -> None:
+        logger.debug("closing data channel and connection.")
         await self._data_connection_watchdog.stop()
         if self.data_channel:
-            await self.data_channel.close()  # type: ignore
+            await self.data_channel.close()
+            logger.debug("data channel closed")
             self.data_channel = None
         if self.data_connection:
             # We need not pass anything as exception to this close. It will only hurt.
-            await self.data_connection.close()  # type: ignore
+            await self.data_connection.close()
+            logger.debug("data connection closed")
             self.data_connection = None
         self.data_exchange = None
-        await super().stop(exception)
+
+    async def teardown(self) -> None:
+        """
+        .. Important::
+            Do not call this function, it is called indirectly by :meth:`Agent.stop`.
+
+        Closes the data connection and the data channel in addition to
+        :meth:`Agent.teardown()`.
+        """
+        await asyncio.gather(super().teardown(), self.__close()),
 
     def _on_data_connection_close(
-        self, sender: Any, _exception: Optional[BaseException]
+        self,
+        sender: aio_pika.abc.AbstractRobustConnection,
+        _exception: Optional[BaseException],
     ) -> None:
         self._data_connection_watchdog.set_closed()
 
     def _on_data_connection_reconnect(
-        self, sender: Any, connection: aio_pika.RobustConnection
+        self, sender: aio_pika.abc.AbstractRobustConnection
     ) -> None:
         self._data_connection_watchdog.set_established()
```

## metricq/datachunk_pb2.py

```diff
@@ -1,25 +1,77 @@
 # -*- coding: utf-8 -*-
 # Generated by the protocol buffer compiler.  DO NOT EDIT!
 # source: datachunk.proto
-"""Generated protocol buffer code."""
-from google.protobuf.internal import builder as _builder
+
 from google.protobuf import descriptor as _descriptor
-from google.protobuf import descriptor_pool as _descriptor_pool
+from google.protobuf import message as _message
+from google.protobuf import reflection as _reflection
 from google.protobuf import symbol_database as _symbol_database
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0f\x64\x61tachunk.proto\x12\x07metricq\".\n\tDataChunk\x12\x12\n\ntime_delta\x18\x01 \x03(\x03\x12\r\n\x05value\x18\x02 \x03(\x01\x62\x06proto3')
+DESCRIPTOR = _descriptor.FileDescriptor(
+  name='datachunk.proto',
+  package='metricq',
+  syntax='proto3',
+  serialized_options=None,
+  create_key=_descriptor._internal_create_key,
+  serialized_pb=b'\n\x0f\x64\x61tachunk.proto\x12\x07metricq\".\n\tDataChunk\x12\x12\n\ntime_delta\x18\x01 \x03(\x03\x12\r\n\x05value\x18\x02 \x03(\x01\x62\x06proto3'
+)
+
+
+
+
+_DATACHUNK = _descriptor.Descriptor(
+  name='DataChunk',
+  full_name='metricq.DataChunk',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  create_key=_descriptor._internal_create_key,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='time_delta', full_name='metricq.DataChunk.time_delta', index=0,
+      number=1, type=3, cpp_type=2, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='value', full_name='metricq.DataChunk.value', index=1,
+      number=2, type=1, cpp_type=5, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=28,
+  serialized_end=74,
+)
+
+DESCRIPTOR.message_types_by_name['DataChunk'] = _DATACHUNK
+_sym_db.RegisterFileDescriptor(DESCRIPTOR)
+
+DataChunk = _reflection.GeneratedProtocolMessageType('DataChunk', (_message.Message,), {
+  'DESCRIPTOR' : _DATACHUNK,
+  '__module__' : 'datachunk_pb2'
+  # @@protoc_insertion_point(class_scope:metricq.DataChunk)
+  })
+_sym_db.RegisterMessage(DataChunk)
+
 
-_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
-_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'datachunk_pb2', globals())
-if _descriptor._USE_C_DESCRIPTORS == False:
-
-  DESCRIPTOR._options = None
-  _DATACHUNK._serialized_start=28
-  _DATACHUNK._serialized_end=74
 # @@protoc_insertion_point(module_scope)
```

## metricq/datachunk_pb2.pyi

```diff
@@ -1,28 +1,66 @@
 """
 @generated by mypy-protobuf.  Do not edit manually!
 isort:skip_file
+Copyright (c) 2018, ZIH,
+Technische Universitaet Dresden,
+Federal Republic of Germany
+
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without modification,
+are permitted provided that the following conditions are met:
+
+    * Redistributions of source code must retain the above copyright notice,
+      this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright notice,
+      this list of conditions and the following disclaimer in the documentation
+      and/or other materials provided with the distribution.
+    * Neither the name of metricq nor the names of its contributors
+      may be used to endorse or promote products derived from this software
+      without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 """
 import builtins
+import collections.abc
 import google.protobuf.descriptor
 import google.protobuf.internal.containers
 import google.protobuf.message
-import typing
-import typing_extensions
+import sys
+
+if sys.version_info >= (3, 8):
+    import typing as typing_extensions
+else:
+    import typing_extensions
 
 DESCRIPTOR: google.protobuf.descriptor.FileDescriptor
 
+@typing_extensions.final
 class DataChunk(google.protobuf.message.Message):
     DESCRIPTOR: google.protobuf.descriptor.Descriptor
+
     TIME_DELTA_FIELD_NUMBER: builtins.int
     VALUE_FIELD_NUMBER: builtins.int
     @property
     def time_delta(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]: ...
     @property
     def value(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.float]: ...
-    def __init__(self,
+    def __init__(
+        self,
         *,
-        time_delta: typing.Optional[typing.Iterable[builtins.int]] = ...,
-        value: typing.Optional[typing.Iterable[builtins.float]] = ...,
-        ) -> None: ...
-    def ClearField(self, field_name: typing_extensions.Literal["time_delta",b"time_delta","value",b"value"]) -> None: ...
+        time_delta: collections.abc.Iterable[builtins.int] | None = ...,
+        value: collections.abc.Iterable[builtins.float] | None = ...,
+    ) -> None: ...
+    def ClearField(self, field_name: typing_extensions.Literal["time_delta", b"time_delta", "value", b"value"]) -> None: ...
+
 global___DataChunk = DataChunk
```

## metricq/drain.py

```diff
@@ -23,74 +23,77 @@
 # PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 # PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 # LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 # NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 import asyncio
+from collections.abc import Iterable
 from types import TracebackType
-from typing import Any, List, Optional, Tuple, Type, cast
+from typing import Any, Optional, cast
 
-import aio_pika
+import aio_pika.abc
 
 from .logging import get_logger
 from .sink import Sink
-from .types import Timestamp
+from .timeseries import Timestamp
 
 logger = get_logger(__name__)
 
 
 class Drain(Sink):
-    def __init__(self, *args: Any, queue: str, metrics: List[str], **kwargs: Any):
+    def __init__(self, *args: Any, queue: str, metrics: Iterable[str], **kwargs: Any):
         """Drain the given queue of all buffered metric data
 
         Args:
             queue: The name of the queue that contains the subscribed data.
-            metrics: List of metrics that you want to subscribe to.
+            metrics: Metrics that you want to subscribe to.
         """
         super().__init__(*args, add_uuid=True, **kwargs)
-        if not metrics:
-            raise ValueError("Metrics list must not be empty")
         if not queue:
             raise ValueError("Queue must not be empty")
         self._queue = queue
-        self._metrics = metrics
+        self._metrics = list(metrics)
+        if not self._metrics:
+            raise ValueError("Metrics must not be empty")
 
-        self._data: asyncio.Queue[Tuple[str, Timestamp, float]] = asyncio.Queue()
+        self._data: asyncio.Queue[tuple[str, Timestamp, float]] = asyncio.Queue()
 
     async def connect(self) -> None:
         await super().connect()
         assert len(self._metrics) > 0
 
         response = await self.rpc(
             "sink.unsubscribe", dataQueue=self._queue, metrics=self._metrics
         )
 
         assert response is not None
         assert len(self._queue) > 0
         await self.sink_config(**response)
 
-    async def _on_data_message(self, message: aio_pika.IncomingMessage) -> None:
+    async def _on_data_message(
+        self, message: aio_pika.abc.AbstractIncomingMessage
+    ) -> None:
         if message.type == "end":
             async with message.process():
                 logger.debug("received end message")
                 await self.rpc("sink.release", dataQueue=self._queue)
-                asyncio.create_task(self.stop())
-                await self._data.put(cast(Tuple[str, Timestamp, float], ()))
+                self._event_loop.create_task(self.stop())
+                await self._data.put(cast(tuple[str, Timestamp, float], ()))
 
                 return
 
         await super()._on_data_message(message)
 
     async def on_data(self, metric: str, time: Timestamp, value: float) -> None:
         await self._data.put((metric, time, value))
 
     async def __aexit__(
         self,
-        exc_type: Optional[Type[BaseException]],
+        exc_type: Optional[type[BaseException]],
         exc_value: Optional[BaseException],
         exc_traceback: Optional[TracebackType],
     ) -> None:
         # We don't need to `await self.stop()` here, but it already got scheduled in
         # `Drain._on_data_message()`. But we need to wait for the stop task to finish.
         await self.stopped()
 
@@ -101,15 +104,15 @@
 
             async for metric, time, value in my_drain:
                 pass
 
         """
         return self
 
-    async def __anext__(self) -> Tuple[str, Timestamp, float]:
+    async def __anext__(self) -> tuple[str, Timestamp, float]:
         try:
             metric, time, value = await self._data.get()
         except ValueError:
             # Value Error is part of control flow
             # -> raised on empty tuple at the end, inserted with the end message
             raise StopAsyncIteration()
         return metric, time, value
```

## metricq/history_client.py

```diff
@@ -26,32 +26,33 @@
 # NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 import asyncio
 import uuid
 from asyncio import CancelledError, Task
 from asyncio.futures import Future
+from collections.abc import Iterable, Iterator
 from enum import Enum, auto
-from typing import Any, Dict, Iterator, List, Optional
+from itertools import chain
+from typing import Any, Optional
 
 import aio_pika
-from aiormq import ChannelInvalidStateError
 
 from . import history_pb2
-from .client import Client, _GetMetricsResult
+from .client import Client
 from .connection_watchdog import ConnectionWatchdog
 from .exceptions import (
     HistoryError,
     InvalidHistoryResponse,
     PublishFailed,
     ReconnectTimeout,
 )
 from .logging import get_logger
 from .rpc import rpc_handler
-from .types import TimeAggregate, Timedelta, Timestamp, TimeValue
+from .timeseries import JsonDict, TimeAggregate, Timedelta, Timestamp, TimeValue
 from .version import __version__  # noqa: F401 - shut up flake8, automatic version str
 
 logger = get_logger(__name__)
 
 
 class HistoryRequestType(Enum):
     """The type of metric data to request.
@@ -171,18 +172,18 @@
     @property
     def mode(self) -> HistoryResponseType:
         """The type of response at hand.
 
         This determines the behavior of :meth:`aggregates` and :meth:`values`:
 
         :attr:`mode` is :attr:`~HistoryResponseType.VALUES`:
-            :meth:`values` will return a iterator of :class:`~metricq.TimeValue`.
+            :meth:`values` will return an iterator of :class:`~metricq.TimeValue`.
             :meth:`aggregates` will fail with :exc:`ValueError`, except if called with :code:`convert=True`.
         :attr:`mode` is :attr:`~HistoryResponseType.AGGREGATES`:
-            :meth:`aggregates` will return a iterator of :class:`~metricq.TimeAggregate`.
+            :meth:`aggregates` will return an iterator of :class:`~metricq.TimeAggregate`.
             :meth:`values` will fail with :exc:`ValueError`, except if called with :code:`convert=True`.
         :attr:`mode` is :attr:`~HistoryResponseType.EMPTY`:
             Both :meth:`values` and :meth:`aggregates` return an empty iterator.
         :attr:`mode` is :attr:`~HistoryResponseType.LEGACY`:
             Both :meth:`values` and :meth:`aggregates` will raise :exc:`ValueError` unless called with :code:`convert=True`.
 
         .. warning::
@@ -191,28 +192,27 @@
 
                 if response.mode is HistoryResponseType.VALUES:
                     ...
                 elif response.mode is HistoryResponseType.AGGREGATES:
                     ...
                 else:
                     # catch-all case, handle it cleanly
-
         """
         return self._mode
 
     def values(self, convert: bool = False) -> Iterator[TimeValue]:
         """An iterator over all data points included in this response.
 
         Args:
             convert:
                 Convert values transparently if response does not contain raw values.
                 If the response contains aggregates, this will yield the mean value of each aggregate.
 
         Raises:
-            :class:`ValueError`:
+            ValueError:
                 if :code:`convert=False` and the response does not contain raw values.
         """
         time_ns = 0
         if self._mode is HistoryResponseType.VALUES:
             for time_delta, value in zip(self._proto.time_delta, self._proto.value):
                 time_ns = time_ns + time_delta
                 yield TimeValue(Timestamp(time_ns), value)
@@ -326,79 +326,92 @@
 class HistoryClient(Client):
     """A MetricQ client to access historical metric data."""
 
     def __init__(self, *args: Any, **kwargs: Any):
         super().__init__(*args, **kwargs)
 
         self.data_server_address: Optional[str] = None
-        self.history_connection: Optional[aio_pika.RobustConnection] = None
-        self.history_channel: Optional[aio_pika.RobustChannel] = None
-        self.history_exchange: Optional[aio_pika.Exchange] = None
-        self.history_response_queue: Optional[aio_pika.Queue] = None
+        self.history_connection: Optional[aio_pika.abc.AbstractRobustConnection] = None
+        self.history_channel: Optional[aio_pika.abc.AbstractRobustChannel] = None
+        self.history_exchange: Optional[aio_pika.abc.AbstractExchange] = None
+        self.history_response_queue: Optional[aio_pika.abc.AbstractQueue] = None
 
         self._history_connection_watchdog = ConnectionWatchdog(
             on_timeout_callback=lambda watchdog: self._schedule_stop(
                 ReconnectTimeout(
                     f"Failed to reestablish {watchdog.connection_name} after {watchdog.timeout} seconds"
                 )
             ),
             timeout=kwargs.get("connection_timeout", 60),
             connection_name="history connection",
         )
 
-        self._request_futures: Dict[str, Future[HistoryResponse]] = dict()
+        self._request_futures: dict[str, Future[HistoryResponse]] = dict()
         self._reregister_task: Optional[Task[None]] = None
 
     async def connect(self) -> None:
-        """Connect to the MetricQ network and register this HistoryClient."""
+        """
+        Connect to the MetricQ network and register this HistoryClient.
+        You can either use this method, or use the class as an async context manager.
+        """
         await super().connect()
         response = await self.rpc("history.register")
         logger.debug("register response: {}", response)
 
         assert response is not None
 
         self.data_server_address = self.derive_address(response["dataServerAddress"])
         self.history_connection = await self.make_connection(
             self.data_server_address,
             connection_name="history connection {}".format(self.token),
         )
-        self.history_connection.add_close_callback(self._on_history_connection_close)
-        self.history_connection.add_reconnect_callback(
-            self._on_history_connection_reconnect  # type: ignore
+        self.history_connection.close_callbacks.add(self._on_history_connection_close)
+        self.history_connection.reconnect_callbacks.add(
+            self._on_history_connection_reconnect
         )
 
-        self.history_channel = await self.history_connection.channel()
-        assert self.history_channel is not None
-        self.history_exchange = await self.history_channel.declare_exchange(
+        channel = await self.history_connection.channel()
+        assert isinstance(channel, aio_pika.abc.AbstractRobustChannel)
+        self.history_channel = channel
+        self.history_exchange = await channel.declare_exchange(
             name=response["historyExchange"], passive=True
         )
         await self._declare_history_queue(response["historyQueue"])
 
         if "config" in response:
             await self.rpc_dispatch("config", **response["config"])
 
         self._history_connection_watchdog.start()
         self._history_connection_watchdog.set_established()
 
         await self._history_consume()
 
-    async def stop(self, exception: Optional[Exception] = None) -> None:
+    async def __close(self) -> None:
         logger.info("closing history channel and connection.")
         await self._history_connection_watchdog.stop()
         if self.history_channel:
-            await self.history_channel.close()  # type: ignore
+            await self.history_channel.close()
             self.history_channel = None
         if self.history_connection:
             # We need not pass anything as exception to this close. It will only hurt.
-            await self.history_connection.close()  # type: ignore
+            await self.history_connection.close()
             self.history_connection = None
         self.history_exchange = None
-        await super().stop(exception)
 
-    async def get_metrics(self, *args: Any, **kwargs: Any) -> _GetMetricsResult:
+    async def teardown(self) -> None:
+        """
+        .. Important::
+            Do not call this function, it is called indirectly by :meth:`Agent.stop`.
+
+        Closes the history connection and the channel in addition to
+        :meth:`Agent.teardown()`.
+        """
+        await asyncio.gather(super().teardown(), self.__close()),
+
+    async def get_metrics(self, *args: Any, **kwargs: Any) -> dict[str, JsonDict]:
         """Retrieve information for **historic** metrics matching a selector pattern.
 
         This is like :meth:`Client.get_metrics`, but sets :code:`historic=True` by default.
         See documentation there for a detailed description of the remaining arguments.
         """
         kwargs.setdefault("historic", True)
         return await super().get_metrics(*args, **kwargs)
@@ -472,15 +485,15 @@
         assert self.history_exchange is not None
         await self._history_connection_watchdog.established()
 
         try:
             # TOC/TOU hazard: by the time we publish, the data connection might
             # be gone again, even if we waited for it to be established before.
             await self.history_exchange.publish(msg, routing_key=metric)
-        except ChannelInvalidStateError as e:
+        except aio_pika.exceptions.ChannelInvalidStateError as e:
             # Trying to publish on a closed channel results in a ChannelInvalidStateError
             # from aiormq.  Let's wrap that in a more descriptive error.
             raise PublishFailed(
                 f"Failed to publish data chunk for metric '{metric!r}' "
                 f"on exchange '{self.history_exchange}' ({self.history_connection})"
             ) from e
 
@@ -658,28 +671,41 @@
         except ValueError:
             raise InvalidHistoryResponse("Response contained no values")
 
     @rpc_handler("config")
     async def _history_config(self, **kwargs: Any) -> None:
         logger.info("received config {}", kwargs)
 
-    async def _history_consume(self, extra_queues: List[aio_pika.Queue] = []) -> None:
+    async def _history_consume(
+        self, extra_queues: Iterable[aio_pika.abc.AbstractQueue] = []
+    ) -> None:
         logger.info("starting history consume")
         assert self.history_response_queue is not None
-        queues = [self.history_response_queue] + extra_queues
+        queues = chain([self.history_response_queue], extra_queues)
         await asyncio.gather(
             *[queue.consume(self._on_history_response) for queue in queues],
         )
 
-    async def _on_history_response(self, message: aio_pika.IncomingMessage) -> None:
+    async def _on_history_response(
+        self, message: aio_pika.abc.AbstractIncomingMessage
+    ) -> None:
         async with message.process(requeue=True):
             body = message.body
             from_token = message.app_id
             correlation_id = message.correlation_id
-            request_duration = float(message.headers.get("x-request-duration", "-1"))
+            if correlation_id is None:
+                logger.warning(
+                    "received history response with no correlation id from {}, ignoring",
+                    from_token,
+                )
+                return
+
+            request_duration_str = message.headers.get("x-request-duration", "-1")
+            assert isinstance(request_duration_str, (str, int, float))
+            request_duration = float(request_duration_str)
 
             logger.debug(
                 "received message from {}, correlation id: {}, reply_to: {}",
                 from_token,
                 correlation_id,
                 message.reply_to,
             )
@@ -717,32 +743,32 @@
                 logger.debug("message is a history response")
                 future.set_result(history_response)
             except HistoryError as e:
                 logger.debug("message is a history response containing an error: {}", e)
                 future.set_exception(e)
 
     def _on_history_connection_close(
-        self, sender: Any, _exception: Optional[BaseException]
+        self,
+        sender: aio_pika.abc.AbstractRobustConnection,
+        _exception: Optional[BaseException],
     ) -> None:
         self._history_connection_watchdog.set_closed()
 
     def _on_history_connection_reconnect(
-        self, sender: Any, connection: aio_pika.Connection
+        self, sender: aio_pika.abc.AbstractRobustConnection
     ) -> None:
-        logger.info("History connection ({}) reestablished!", connection)
+        logger.info("History connection ({}) reestablished!", sender)
 
         if self._reregister_task is not None and not self._reregister_task.done():
             logger.warning(
                 "History connection was reestablished, but another reregister task is still running!"
             )
             self._reregister_task.cancel()
 
-        self._reregister_task = self._event_loop.create_task(
-            self._reregister(connection)
-        )
+        self._reregister_task = self._event_loop.create_task(self._reregister(sender))
 
         def reregister_done(task: Task[None]) -> None:
             try:
                 exception = task.exception()
                 if exception is None:
                     self._history_connection_watchdog.set_established()
                 else:
@@ -751,15 +777,15 @@
                     )
                     raise exception
             except CancelledError:
                 logger.warning("Reregister task was cancelled!")
 
         self._reregister_task.add_done_callback(reregister_done)
 
-    async def _reregister(self, connection: aio_pika.Connection) -> None:
+    async def _reregister(self, connection: aio_pika.abc.AbstractConnection) -> None:
         logger.info(
             "Reregistering as history client...",
         )
         response = await self.rpc("history.register")
 
         assert response is not None
         assert self.history_exchange is not None
@@ -774,16 +800,16 @@
         if "config" in response:
             await self.rpc_dispatch("config", **response["config"])
 
         logger.debug("Restarting consume...")
         await self._history_consume()
 
     async def _declare_history_queue(self, name: str) -> None:
-        # The manager declares the queue and we only connect to that queue with passive=True
+        # The manager declares the queue, and we only connect to that queue with `passive=True`
         # But when a disconnect happens, the queue gets deleted. Therefore, there is no
         # way, how a robust connection could reconnect to that queue. Hence, we set
-        # robust=False and handle the reconnect ourselfs.
-        # (See self._on_history_connection_reconnect())
+        # robust=False and handle the reconnect ourselves.
+        # (See :meth:`_on_history_connection_reconnect`)
         assert self.history_channel is not None
         self.history_response_queue = await self.history_channel.declare_queue(
             name=name, passive=True, robust=False
         )
```

## metricq/history_pb2.py

```diff
@@ -1,37 +1,297 @@
 # -*- coding: utf-8 -*-
 # Generated by the protocol buffer compiler.  DO NOT EDIT!
 # source: history.proto
-"""Generated protocol buffer code."""
-from google.protobuf.internal import builder as _builder
+
 from google.protobuf import descriptor as _descriptor
-from google.protobuf import descriptor_pool as _descriptor_pool
+from google.protobuf import message as _message
+from google.protobuf import reflection as _reflection
 from google.protobuf import symbol_database as _symbol_database
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\rhistory.proto\x12\x07metricq\"\xd8\x01\n\x0eHistoryRequest\x12\x12\n\nstart_time\x18\x01 \x01(\x03\x12\x10\n\x08\x65nd_time\x18\x02 \x01(\x03\x12\x14\n\x0cinterval_max\x18\x03 \x01(\x03\x12\x31\n\x04type\x18\x04 \x01(\x0e\x32#.metricq.HistoryRequest.RequestType\"W\n\x0bRequestType\x12\x16\n\x12\x41GGREGATE_TIMELINE\x10\x00\x12\r\n\tAGGREGATE\x10\x01\x12\x0e\n\nLAST_VALUE\x10\x02\x12\x11\n\rFLEX_TIMELINE\x10\x03\"\xc1\x02\n\x0fHistoryResponse\x12\x0e\n\x06metric\x18\x01 \x01(\t\x12\x12\n\ntime_delta\x18\x02 \x03(\x03\x12\x15\n\tvalue_min\x18\x03 \x03(\x01\x42\x02\x18\x01\x12\x15\n\tvalue_max\x18\x04 \x03(\x01\x42\x02\x18\x01\x12\x15\n\tvalue_avg\x18\x05 \x03(\x01\x42\x02\x18\x01\x12\x35\n\taggregate\x18\x06 \x03(\x0b\x32\".metricq.HistoryResponse.Aggregate\x12\r\n\x05value\x18\x07 \x03(\x01\x12\r\n\x05\x65rror\x18\x08 \x01(\t\x1ap\n\tAggregate\x12\x0f\n\x07minimum\x18\x01 \x01(\x01\x12\x0f\n\x07maximum\x18\x02 \x01(\x01\x12\x0b\n\x03sum\x18\x03 \x01(\x01\x12\r\n\x05\x63ount\x18\x04 \x01(\x04\x12\x10\n\x08integral\x18\x05 \x01(\x01\x12\x13\n\x0b\x61\x63tive_time\x18\x06 \x01(\x03\x62\x06proto3')
+DESCRIPTOR = _descriptor.FileDescriptor(
+  name='history.proto',
+  package='metricq',
+  syntax='proto3',
+  serialized_options=None,
+  create_key=_descriptor._internal_create_key,
+  serialized_pb=b'\n\rhistory.proto\x12\x07metricq\"\xd8\x01\n\x0eHistoryRequest\x12\x12\n\nstart_time\x18\x01 \x01(\x03\x12\x10\n\x08\x65nd_time\x18\x02 \x01(\x03\x12\x14\n\x0cinterval_max\x18\x03 \x01(\x03\x12\x31\n\x04type\x18\x04 \x01(\x0e\x32#.metricq.HistoryRequest.RequestType\"W\n\x0bRequestType\x12\x16\n\x12\x41GGREGATE_TIMELINE\x10\x00\x12\r\n\tAGGREGATE\x10\x01\x12\x0e\n\nLAST_VALUE\x10\x02\x12\x11\n\rFLEX_TIMELINE\x10\x03\"\xc1\x02\n\x0fHistoryResponse\x12\x0e\n\x06metric\x18\x01 \x01(\t\x12\x12\n\ntime_delta\x18\x02 \x03(\x03\x12\x15\n\tvalue_min\x18\x03 \x03(\x01\x42\x02\x18\x01\x12\x15\n\tvalue_max\x18\x04 \x03(\x01\x42\x02\x18\x01\x12\x15\n\tvalue_avg\x18\x05 \x03(\x01\x42\x02\x18\x01\x12\x35\n\taggregate\x18\x06 \x03(\x0b\x32\".metricq.HistoryResponse.Aggregate\x12\r\n\x05value\x18\x07 \x03(\x01\x12\r\n\x05\x65rror\x18\x08 \x01(\t\x1ap\n\tAggregate\x12\x0f\n\x07minimum\x18\x01 \x01(\x01\x12\x0f\n\x07maximum\x18\x02 \x01(\x01\x12\x0b\n\x03sum\x18\x03 \x01(\x01\x12\r\n\x05\x63ount\x18\x04 \x01(\x04\x12\x10\n\x08integral\x18\x05 \x01(\x01\x12\x13\n\x0b\x61\x63tive_time\x18\x06 \x01(\x03\x62\x06proto3'
+)
+
+
+
+_HISTORYREQUEST_REQUESTTYPE = _descriptor.EnumDescriptor(
+  name='RequestType',
+  full_name='metricq.HistoryRequest.RequestType',
+  filename=None,
+  file=DESCRIPTOR,
+  create_key=_descriptor._internal_create_key,
+  values=[
+    _descriptor.EnumValueDescriptor(
+      name='AGGREGATE_TIMELINE', index=0, number=0,
+      serialized_options=None,
+      type=None,
+      create_key=_descriptor._internal_create_key),
+    _descriptor.EnumValueDescriptor(
+      name='AGGREGATE', index=1, number=1,
+      serialized_options=None,
+      type=None,
+      create_key=_descriptor._internal_create_key),
+    _descriptor.EnumValueDescriptor(
+      name='LAST_VALUE', index=2, number=2,
+      serialized_options=None,
+      type=None,
+      create_key=_descriptor._internal_create_key),
+    _descriptor.EnumValueDescriptor(
+      name='FLEX_TIMELINE', index=3, number=3,
+      serialized_options=None,
+      type=None,
+      create_key=_descriptor._internal_create_key),
+  ],
+  containing_type=None,
+  serialized_options=None,
+  serialized_start=156,
+  serialized_end=243,
+)
+_sym_db.RegisterEnumDescriptor(_HISTORYREQUEST_REQUESTTYPE)
+
+
+_HISTORYREQUEST = _descriptor.Descriptor(
+  name='HistoryRequest',
+  full_name='metricq.HistoryRequest',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  create_key=_descriptor._internal_create_key,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='start_time', full_name='metricq.HistoryRequest.start_time', index=0,
+      number=1, type=3, cpp_type=2, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='end_time', full_name='metricq.HistoryRequest.end_time', index=1,
+      number=2, type=3, cpp_type=2, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='interval_max', full_name='metricq.HistoryRequest.interval_max', index=2,
+      number=3, type=3, cpp_type=2, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='type', full_name='metricq.HistoryRequest.type', index=3,
+      number=4, type=14, cpp_type=8, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+    _HISTORYREQUEST_REQUESTTYPE,
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=27,
+  serialized_end=243,
+)
+
+
+_HISTORYRESPONSE_AGGREGATE = _descriptor.Descriptor(
+  name='Aggregate',
+  full_name='metricq.HistoryResponse.Aggregate',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  create_key=_descriptor._internal_create_key,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='minimum', full_name='metricq.HistoryResponse.Aggregate.minimum', index=0,
+      number=1, type=1, cpp_type=5, label=1,
+      has_default_value=False, default_value=float(0),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='maximum', full_name='metricq.HistoryResponse.Aggregate.maximum', index=1,
+      number=2, type=1, cpp_type=5, label=1,
+      has_default_value=False, default_value=float(0),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='sum', full_name='metricq.HistoryResponse.Aggregate.sum', index=2,
+      number=3, type=1, cpp_type=5, label=1,
+      has_default_value=False, default_value=float(0),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='count', full_name='metricq.HistoryResponse.Aggregate.count', index=3,
+      number=4, type=4, cpp_type=4, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='integral', full_name='metricq.HistoryResponse.Aggregate.integral', index=4,
+      number=5, type=1, cpp_type=5, label=1,
+      has_default_value=False, default_value=float(0),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='active_time', full_name='metricq.HistoryResponse.Aggregate.active_time', index=5,
+      number=6, type=3, cpp_type=2, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=455,
+  serialized_end=567,
+)
+
+_HISTORYRESPONSE = _descriptor.Descriptor(
+  name='HistoryResponse',
+  full_name='metricq.HistoryResponse',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  create_key=_descriptor._internal_create_key,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='metric', full_name='metricq.HistoryResponse.metric', index=0,
+      number=1, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='time_delta', full_name='metricq.HistoryResponse.time_delta', index=1,
+      number=2, type=3, cpp_type=2, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='value_min', full_name='metricq.HistoryResponse.value_min', index=2,
+      number=3, type=1, cpp_type=5, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=b'\030\001', file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='value_max', full_name='metricq.HistoryResponse.value_max', index=3,
+      number=4, type=1, cpp_type=5, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=b'\030\001', file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='value_avg', full_name='metricq.HistoryResponse.value_avg', index=4,
+      number=5, type=1, cpp_type=5, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=b'\030\001', file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='aggregate', full_name='metricq.HistoryResponse.aggregate', index=5,
+      number=6, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='value', full_name='metricq.HistoryResponse.value', index=6,
+      number=7, type=1, cpp_type=5, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+    _descriptor.FieldDescriptor(
+      name='error', full_name='metricq.HistoryResponse.error', index=7,
+      number=8, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=b"".decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      serialized_options=None, file=DESCRIPTOR,  create_key=_descriptor._internal_create_key),
+  ],
+  extensions=[
+  ],
+  nested_types=[_HISTORYRESPONSE_AGGREGATE, ],
+  enum_types=[
+  ],
+  serialized_options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=246,
+  serialized_end=567,
+)
+
+_HISTORYREQUEST.fields_by_name['type'].enum_type = _HISTORYREQUEST_REQUESTTYPE
+_HISTORYREQUEST_REQUESTTYPE.containing_type = _HISTORYREQUEST
+_HISTORYRESPONSE_AGGREGATE.containing_type = _HISTORYRESPONSE
+_HISTORYRESPONSE.fields_by_name['aggregate'].message_type = _HISTORYRESPONSE_AGGREGATE
+DESCRIPTOR.message_types_by_name['HistoryRequest'] = _HISTORYREQUEST
+DESCRIPTOR.message_types_by_name['HistoryResponse'] = _HISTORYRESPONSE
+_sym_db.RegisterFileDescriptor(DESCRIPTOR)
+
+HistoryRequest = _reflection.GeneratedProtocolMessageType('HistoryRequest', (_message.Message,), {
+  'DESCRIPTOR' : _HISTORYREQUEST,
+  '__module__' : 'history_pb2'
+  # @@protoc_insertion_point(class_scope:metricq.HistoryRequest)
+  })
+_sym_db.RegisterMessage(HistoryRequest)
+
+HistoryResponse = _reflection.GeneratedProtocolMessageType('HistoryResponse', (_message.Message,), {
+
+  'Aggregate' : _reflection.GeneratedProtocolMessageType('Aggregate', (_message.Message,), {
+    'DESCRIPTOR' : _HISTORYRESPONSE_AGGREGATE,
+    '__module__' : 'history_pb2'
+    # @@protoc_insertion_point(class_scope:metricq.HistoryResponse.Aggregate)
+    })
+  ,
+  'DESCRIPTOR' : _HISTORYRESPONSE,
+  '__module__' : 'history_pb2'
+  # @@protoc_insertion_point(class_scope:metricq.HistoryResponse)
+  })
+_sym_db.RegisterMessage(HistoryResponse)
+_sym_db.RegisterMessage(HistoryResponse.Aggregate)
+
 
-_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
-_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'history_pb2', globals())
-if _descriptor._USE_C_DESCRIPTORS == False:
-
-  DESCRIPTOR._options = None
-  _HISTORYRESPONSE.fields_by_name['value_min']._options = None
-  _HISTORYRESPONSE.fields_by_name['value_min']._serialized_options = b'\030\001'
-  _HISTORYRESPONSE.fields_by_name['value_max']._options = None
-  _HISTORYRESPONSE.fields_by_name['value_max']._serialized_options = b'\030\001'
-  _HISTORYRESPONSE.fields_by_name['value_avg']._options = None
-  _HISTORYRESPONSE.fields_by_name['value_avg']._serialized_options = b'\030\001'
-  _HISTORYREQUEST._serialized_start=27
-  _HISTORYREQUEST._serialized_end=243
-  _HISTORYREQUEST_REQUESTTYPE._serialized_start=156
-  _HISTORYREQUEST_REQUESTTYPE._serialized_end=243
-  _HISTORYRESPONSE._serialized_start=246
-  _HISTORYRESPONSE._serialized_end=567
-  _HISTORYRESPONSE_AGGREGATE._serialized_start=455
-  _HISTORYRESPONSE_AGGREGATE._serialized_end=567
+_HISTORYRESPONSE.fields_by_name['value_min']._options = None
+_HISTORYRESPONSE.fields_by_name['value_max']._options = None
+_HISTORYRESPONSE.fields_by_name['value_avg']._options = None
 # @@protoc_insertion_point(module_scope)
```

## metricq/history_pb2.pyi

```diff
@@ -1,131 +1,168 @@
 """
 @generated by mypy-protobuf.  Do not edit manually!
 isort:skip_file
+Copyright (c) 2018, ZIH,
+Technische Universitaet Dresden,
+Federal Republic of Germany
+
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without modification,
+are permitted provided that the following conditions are met:
+
+    * Redistributions of source code must retain the above copyright notice,
+      this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright notice,
+      this list of conditions and the following disclaimer in the documentation
+      and/or other materials provided with the distribution.
+    * Neither the name of metricq nor the names of its contributors
+      may be used to endorse or promote products derived from this software
+      without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 """
 import builtins
+import collections.abc
 import google.protobuf.descriptor
 import google.protobuf.internal.containers
 import google.protobuf.internal.enum_type_wrapper
 import google.protobuf.message
+import sys
 import typing
-import typing_extensions
+
+if sys.version_info >= (3, 10):
+    import typing as typing_extensions
+else:
+    import typing_extensions
 
 DESCRIPTOR: google.protobuf.descriptor.FileDescriptor
 
+@typing_extensions.final
 class HistoryRequest(google.protobuf.message.Message):
     DESCRIPTOR: google.protobuf.descriptor.Descriptor
+
     class _RequestType:
-        ValueType = typing.NewType('ValueType', builtins.int)
+        ValueType = typing.NewType("ValueType", builtins.int)
         V: typing_extensions.TypeAlias = ValueType
-    class _RequestTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[HistoryRequest._RequestType.ValueType], builtins.type):
+
+    class _RequestTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[HistoryRequest._RequestType.ValueType], builtins.type):  # noqa: F821
         DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
         AGGREGATE_TIMELINE: HistoryRequest._RequestType.ValueType  # 0
         """Timeline of aggregates over |start_time, end_time[ with the specified interval_max"""
-
         AGGREGATE: HistoryRequest._RequestType.ValueType  # 1
         """Single aggregate value over [start_time, end_time[ in aggregates"""
-
         LAST_VALUE: HistoryRequest._RequestType.ValueType  # 2
         """Single last value in values (0 values if no values in DB)"""
-
         FLEX_TIMELINE: HistoryRequest._RequestType.ValueType  # 3
         """Either aggregates or values in |start_time, end_time[ depending on the specified interval_max"""
 
-    class RequestType(_RequestType, metaclass=_RequestTypeEnumTypeWrapper):
-        pass
-
+    class RequestType(_RequestType, metaclass=_RequestTypeEnumTypeWrapper): ...
     AGGREGATE_TIMELINE: HistoryRequest.RequestType.ValueType  # 0
     """Timeline of aggregates over |start_time, end_time[ with the specified interval_max"""
-
     AGGREGATE: HistoryRequest.RequestType.ValueType  # 1
     """Single aggregate value over [start_time, end_time[ in aggregates"""
-
     LAST_VALUE: HistoryRequest.RequestType.ValueType  # 2
     """Single last value in values (0 values if no values in DB)"""
-
     FLEX_TIMELINE: HistoryRequest.RequestType.ValueType  # 3
     """Either aggregates or values in |start_time, end_time[ depending on the specified interval_max"""
 
-
     START_TIME_FIELD_NUMBER: builtins.int
     END_TIME_FIELD_NUMBER: builtins.int
     INTERVAL_MAX_FIELD_NUMBER: builtins.int
     TYPE_FIELD_NUMBER: builtins.int
     start_time: builtins.int
     end_time: builtins.int
     interval_max: builtins.int
     type: global___HistoryRequest.RequestType.ValueType
     """default: AGGREGATE_TIMELINE"""
-
-    def __init__(self,
+    def __init__(
+        self,
         *,
         start_time: builtins.int = ...,
         end_time: builtins.int = ...,
         interval_max: builtins.int = ...,
         type: global___HistoryRequest.RequestType.ValueType = ...,
-        ) -> None: ...
-    def ClearField(self, field_name: typing_extensions.Literal["end_time",b"end_time","interval_max",b"interval_max","start_time",b"start_time","type",b"type"]) -> None: ...
+    ) -> None: ...
+    def ClearField(self, field_name: typing_extensions.Literal["end_time", b"end_time", "interval_max", b"interval_max", "start_time", b"start_time", "type", b"type"]) -> None: ...
+
 global___HistoryRequest = HistoryRequest
 
+@typing_extensions.final
 class HistoryResponse(google.protobuf.message.Message):
     DESCRIPTOR: google.protobuf.descriptor.Descriptor
+
+    @typing_extensions.final
     class Aggregate(google.protobuf.message.Message):
         DESCRIPTOR: google.protobuf.descriptor.Descriptor
+
         MINIMUM_FIELD_NUMBER: builtins.int
         MAXIMUM_FIELD_NUMBER: builtins.int
         SUM_FIELD_NUMBER: builtins.int
         COUNT_FIELD_NUMBER: builtins.int
         INTEGRAL_FIELD_NUMBER: builtins.int
         ACTIVE_TIME_FIELD_NUMBER: builtins.int
         minimum: builtins.float
         maximum: builtins.float
         sum: builtins.float
         count: builtins.int
         integral: builtins.float
         active_time: builtins.int
-        def __init__(self,
+        def __init__(
+            self,
             *,
             minimum: builtins.float = ...,
             maximum: builtins.float = ...,
             sum: builtins.float = ...,
             count: builtins.int = ...,
             integral: builtins.float = ...,
             active_time: builtins.int = ...,
-            ) -> None: ...
-        def ClearField(self, field_name: typing_extensions.Literal["active_time",b"active_time","count",b"count","integral",b"integral","maximum",b"maximum","minimum",b"minimum","sum",b"sum"]) -> None: ...
+        ) -> None: ...
+        def ClearField(self, field_name: typing_extensions.Literal["active_time", b"active_time", "count", b"count", "integral", b"integral", "maximum", b"maximum", "minimum", b"minimum", "sum", b"sum"]) -> None: ...
 
     METRIC_FIELD_NUMBER: builtins.int
     TIME_DELTA_FIELD_NUMBER: builtins.int
     VALUE_MIN_FIELD_NUMBER: builtins.int
     VALUE_MAX_FIELD_NUMBER: builtins.int
     VALUE_AVG_FIELD_NUMBER: builtins.int
     AGGREGATE_FIELD_NUMBER: builtins.int
     VALUE_FIELD_NUMBER: builtins.int
     ERROR_FIELD_NUMBER: builtins.int
-    metric: typing.Text
+    metric: builtins.str
     @property
     def time_delta(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]: ...
     @property
     def value_min(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.float]: ...
     @property
     def value_max(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.float]: ...
     @property
     def value_avg(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.float]: ...
     @property
     def aggregate(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___HistoryResponse.Aggregate]: ...
     @property
     def value(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.float]: ...
-    error: typing.Text
-    def __init__(self,
+    error: builtins.str
+    def __init__(
+        self,
         *,
-        metric: typing.Text = ...,
-        time_delta: typing.Optional[typing.Iterable[builtins.int]] = ...,
-        value_min: typing.Optional[typing.Iterable[builtins.float]] = ...,
-        value_max: typing.Optional[typing.Iterable[builtins.float]] = ...,
-        value_avg: typing.Optional[typing.Iterable[builtins.float]] = ...,
-        aggregate: typing.Optional[typing.Iterable[global___HistoryResponse.Aggregate]] = ...,
-        value: typing.Optional[typing.Iterable[builtins.float]] = ...,
-        error: typing.Text = ...,
-        ) -> None: ...
-    def ClearField(self, field_name: typing_extensions.Literal["aggregate",b"aggregate","error",b"error","metric",b"metric","time_delta",b"time_delta","value",b"value","value_avg",b"value_avg","value_max",b"value_max","value_min",b"value_min"]) -> None: ...
+        metric: builtins.str = ...,
+        time_delta: collections.abc.Iterable[builtins.int] | None = ...,
+        value_min: collections.abc.Iterable[builtins.float] | None = ...,
+        value_max: collections.abc.Iterable[builtins.float] | None = ...,
+        value_avg: collections.abc.Iterable[builtins.float] | None = ...,
+        aggregate: collections.abc.Iterable[global___HistoryResponse.Aggregate] | None = ...,
+        value: collections.abc.Iterable[builtins.float] | None = ...,
+        error: builtins.str = ...,
+    ) -> None: ...
+    def ClearField(self, field_name: typing_extensions.Literal["aggregate", b"aggregate", "error", b"error", "metric", b"metric", "time_delta", b"time_delta", "value", b"value", "value_avg", b"value_avg", "value_max", b"value_max", "value_min", b"value_min"]) -> None: ...
+
 global___HistoryResponse = HistoryResponse
```

## metricq/interval_source.py

```diff
@@ -24,20 +24,20 @@
 # PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 # LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 # NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 import asyncio
 from abc import abstractmethod
-from typing import Any, Optional, Union
+from typing import Any, Optional
 
 from .exceptions import PublishFailed
 from .logging import get_logger
 from .source import Source
-from .types import Timedelta, Timestamp
+from .timeseries import Timedelta, Timestamp
 
 logger = get_logger(__name__)
 
 
 class IntervalSource(Source):
     """A :term:`Source` producing metrics at regular intervals of time.
 
@@ -71,15 +71,15 @@
 
                 async def update(self):
                     await self.send("example.counter", time=Timestamp.now(), value=self.counter)
                     self.counter += 1
     """
 
     def __init__(
-        self, *args: Any, period: Union[float, Timedelta, None] = None, **kwargs: Any
+        self, *args: Any, period: float | Timedelta | None = None, **kwargs: Any
     ):
         super().__init__(*args, **kwargs)
         self._period: Optional[Timedelta]
         if period is None:
             self._period = None
         else:
             self.period = period  # type: ignore # https://github.com/python/mypy/issues/3004
@@ -110,15 +110,15 @@
             Return period as :class:`Timedelta` instead of a number of seconds.
         """
         if self._period is None:
             return None
         return self._period
 
     @period.setter
-    def period(self, duration: Union[float, Timedelta]) -> None:
+    def period(self, duration: float | Timedelta) -> None:
         if isinstance(duration, Timedelta):
             self._period = duration
         elif duration is None:
             # Raise a descriptive exception if someone tries to reset the period.
             raise TypeError(
                 "Setting the IntervalSource update period to None is not supported"
             )
@@ -132,42 +132,45 @@
             try:
                 await self.update()
             except PublishFailed as e:
                 # This is a "normal" case, when we lost the connection.
                 # During the reconnection phase, we need to save the task from
                 # being cancelled.
                 logger.debug("Failed to send metric value: {}", e)
-
-            try:
-                if self._period is None:
-                    raise ValueError(
-                        "IntervalSource.period not set before running task"
-                    )
+            except Exception as e:
+                # Some exception escaped from the client, we want to stop the
+                # zombie apocalypse, so we request a stop here. If agents don't
+                # handle their exceptions, we will. Again, we need to save the
+                # task from being cancelled.
+                await self.stop(e)
+
+            if self._period is None:
+                raise ValueError("IntervalSource.period not set before running task")
+            deadline += self._period
+            now = Timestamp.now()
+            while now >= deadline:
+                logger.warning("Missed deadline {}, it is now {}", deadline, now)
                 deadline += self._period
-                now = Timestamp.now()
-                while now >= deadline:
-                    logger.warn("Missed deadline {}, it is now {}", deadline, now)
-                    deadline += self._period
-
-                timeout = (deadline - now).s
-                await asyncio.wait_for(
-                    asyncio.shield(self._interval_task_stop_future), timeout=timeout
-                )
-                self._interval_task_stop_future.result()
-                logger.info("stopping IntervalSource task")
-                break
-            except asyncio.TimeoutError:
-                # This is the normal case, just continue with the loop
-                continue
-
-    async def stop(self, exception: Optional[Exception] = None) -> None:
-        logger.debug("stop()")
-        if self._interval_task_stop_future is not None:
-            self._interval_task_stop_future.set_result(None)
-        await super().stop(exception)
+
+            timeout = (deadline - now).s
+            done, pending = await asyncio.wait(
+                (
+                    asyncio.create_task(asyncio.sleep(timeout)),
+                    self._interval_task_stop_future,
+                ),
+                return_when=asyncio.FIRST_COMPLETED,
+            )
+            if self.task_stop_future in done:
+                logger.info("IntervalSource task reached end")
+                # cancel pending sleep task
+                for task in pending:
+                    task.cancel()
+                # potentially raise exceptions
+                self.task_stop_future.result()
+                return
 
     @abstractmethod
     async def update(self) -> None:
         """A user-provided method called at intervals given by :attr:`period`.
 
         Override this method to produce data points at a constant rate.
```

## metricq/logging.py

```diff
@@ -1,12 +1,13 @@
 # FROM https://stackoverflow.com/a/36294984/620382
 import functools
 import logging
 import types
-from typing import Callable, Optional, TypeVar
+from collections.abc import Callable
+from typing import Optional, TypeVar
 
 T = TypeVar("T")
 
 
 def _get_message(record: logging.LogRecord) -> str:
     """Replacement for logging.LogRecord.getMessage
     that uses the new-style string formatting for
@@ -22,22 +23,20 @@
 
 def _handle_wrap(fcn: Callable[..., T]) -> Callable[..., T]:
     """Wrap the handle function to replace the passed in
     record's getMessage function before calling handle"""
 
     @functools.wraps(fcn)
     def handle(record: logging.LogRecord) -> T:
-        # Mypy don't like method assign: https://github.com/python/mypy/issues/2427
-        record.getMessage = types.MethodType(_get_message, record)  # type: ignore
+        record.getMessage = types.MethodType(_get_message, record)  # type: ignore[method-assign]
         return fcn(record)
 
     return handle
 
 
 def get_logger(name: Optional[str] = None) -> logging.Logger:
     """Get a logger instance that uses new-style string formatting"""
     log = logging.getLogger(name)
     if not hasattr(log, "_newstyle"):
-        # Mypy don't like method assign: https://github.com/python/mypy/issues/2427
-        log.handle = _handle_wrap(log.handle)  # type: ignore
+        log.handle = _handle_wrap(log.handle)  # type: ignore[method-assign]
     setattr(log, "_newstyle", True)
     return log
```

## metricq/rpc.py

```diff
@@ -26,35 +26,35 @@
 # PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 # LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 # NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 from abc import ABCMeta
 from collections import defaultdict
-from collections.abc import Awaitable
-from typing import Any, Callable, DefaultDict, Dict, List, Optional, Tuple
+from collections.abc import Awaitable, Callable
+from typing import Any, Optional
 
 RPCHandlerType = Callable[..., Optional[Any]]
 
 
 class RPCMeta(ABCMeta):
     """
     The created classes will have an _rpc_handlers attribute which contains
     lists of handlers for each rpc tag.
     In each list, the base-class rpc handlers will be before the child class ones
     """
 
     def __new__(
-        mcs: type,
+        mcs: type["RPCMeta"],
         name: str,
-        bases: Tuple[type, ...],
-        attrs: Dict[Any, Any],
+        bases: tuple[type, ...],
+        attrs: dict[Any, Any],
         **kwargs: Any,
     ) -> "RPCMeta":
-        rpc_handlers: DefaultDict[str, List[RPCHandlerType]] = defaultdict(list)
+        rpc_handlers: defaultdict[str, list[RPCHandlerType]] = defaultdict(list)
         for base in bases:
             try:
                 for function_tag, handlers in base._rpc_handlers.items():  # type: ignore
                     rpc_handlers[function_tag] += handlers
             except AttributeError:
                 pass
 
@@ -64,21 +64,19 @@
                 for function_tag in function_tags:
                     rpc_handlers[function_tag].append(handler)
             except AttributeError:
                 # oops, not an rpc handler
                 pass
 
         attrs["_rpc_handlers"] = rpc_handlers
-        # Mypy complains about arguments of super(). Seems like this issue:
-        # https://github.com/python/mypy/issues/9282
-        return super().__new__(mcs, name, bases, attrs)  # type: ignore
+        return super().__new__(mcs, name, bases, attrs)
 
 
 class RPCDispatcher(metaclass=RPCMeta):
-    _rpc_handlers: DefaultDict[str, List[RPCHandlerType]] = defaultdict(list)
+    _rpc_handlers: defaultdict[str, list[RPCHandlerType]] = defaultdict(list)
 
     async def rpc_dispatch(self, function: str, **kwargs: Any) -> Any:
         """Dispatch an incoming (or fake) RPC to all handlers, beginning with the base class handlers
 
         :meta private:
 
         Return values are only allowed for unique RPC handlers.
@@ -89,15 +87,15 @@
 
         Warning:
             Do not rename the :literal:`function` argument.
             It must be called :literal:`function` because it is called directly with the json dict.
 
         Raises:
             KeyError: if no corresponding handler is defined.
-            TypeError: if a handler is not an class:`Awaitable`.
+            TypeError: if a handler is not an :class:`Awaitable`.
             TypeError: if multiple handlers are defined that return a :literal:`not None` value.
         """
         if function not in self._rpc_handlers:
             raise KeyError("Missing rpc handler for {}".format(function))
 
         for handler in self._rpc_handlers[function]:
             task = handler(self, **kwargs)
```

## metricq/sink.py

```diff
@@ -24,23 +24,23 @@
 # PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 # LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 # NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 from abc import abstractmethod
 from asyncio import CancelledError, Task
-from typing import Any, Dict, List, Optional, Set, Union
+from collections.abc import Iterable
+from typing import Any, Optional
 
 import aio_pika
-from aio_pika.queue import Queue
 
 from .data_client import DataClient
 from .datachunk_pb2 import DataChunk
 from .logging import get_logger
-from .types import JsonDict, Metric, Timestamp
+from .timeseries import JsonDict, Metric, Timestamp
 
 logger = get_logger(__name__)
 
 
 class Sink(DataClient):
     """A base class intended to be subclassed to create user-defined :term:`Sinks<Sink>`.
 
@@ -51,18 +51,18 @@
             whether to append a randomly generated UUID to this Sink's :term:`Token`.
             This is useful to distinguish different instances of the same Sink.
     """
 
     def __init__(self, *args: Any, add_uuid: bool = True, **kwargs: Any):
         super().__init__(*args, add_uuid=add_uuid, **kwargs)
 
-        self._data_queue: Optional[Queue] = None
+        self._data_queue: Optional[aio_pika.abc.AbstractQueue] = None
         self._data_consumer_tag: Optional[str] = None
-        self._subscribed_metrics: Set[str] = set()
-        self._subscribe_args: Dict[str, Any] = dict()
+        self._subscribed_metrics: set[str] = set()
+        self._subscribe_args: dict[str, Any] = dict()
         self._resubscribe_task: Optional[Task[None]] = None
 
     async def _declare_data_queue(self, name: str) -> None:
         assert self.data_channel is not None
         self._data_queue = await self.data_channel.declare_queue(
             name=name, robust=False, passive=True
         )
@@ -73,27 +73,25 @@
 
         assert self._data_queue is not None
 
         logger.info("starting sink consume")
         self._data_consumer_tag = await self._data_queue.consume(self._on_data_message)
 
     def _on_data_connection_reconnect(
-        self, sender: Any, connection: aio_pika.Connection
+        self, sender: aio_pika.abc.AbstractConnection
     ) -> None:
-        logger.info("Sink data connection ({}) reestablished!", connection)
+        logger.info("Sink data connection ({}) reestablished!", sender)
 
         if self._resubscribe_task is not None and not self._resubscribe_task.done():
             logger.warning(
                 "Sink data connection was reestablished, but another resubscribe task is still running!"
             )
             self._resubscribe_task.cancel()
 
-        self._resubscribe_task = self._event_loop.create_task(
-            self._resubscribe(connection)
-        )
+        self._resubscribe_task = self._event_loop.create_task(self._resubscribe(sender))
 
         def resubscribe_done(task: Task[None]) -> None:
             try:
                 exception = task.exception()
                 if exception is None:
                     self._data_connection_watchdog.set_established()
                 else:
@@ -102,15 +100,15 @@
                     )
                     raise exception
             except CancelledError:
                 logger.warning("Resubscribe task was cancelled!")
 
         self._resubscribe_task.add_done_callback(resubscribe_done)
 
-    async def _resubscribe(self, connection: aio_pika.Connection) -> None:
+    async def _resubscribe(self, connection: aio_pika.abc.AbstractConnection) -> None:
         assert self._data_queue is not None
         # Reuse manager-assigned data queue name for resubscription.
         self._subscribe_args.update(dataQueue=self._data_queue.name)
 
         metrics = tuple(self._subscribed_metrics)
         logger.info(
             "Resubscribing to {} metric(s) with RPC parameters {}...",
@@ -126,68 +124,77 @@
         logger.debug("Restarting consume...")
         await self._data_queue.consume(
             self._on_data_message, consumer_tag=self._data_consumer_tag
         )
 
     async def subscribe(
         self,
-        metrics: List[Metric],
-        expires: Union[None, int, float] = None,
+        metrics: Iterable[Metric],
+        expires: int | float | None = None,
         metadata: Optional[bool] = None,
         **kwargs: Any,
     ) -> JsonDict:
         """Subscribe to a list of metrics.
 
         Args:
             metrics: names of the metrics to subscribe to
             expires: queue expiration time in seconds
-            metadata: whether to return metric metadata in the response, defaults to ``True``
+            metadata: whether to return metric metadata in the response, defaults to ``True`` as per the RPC spec
 
         Returns:
             rpc response
         """
+        metrics_list = list(metrics)
 
         if self._data_queue is not None:
             kwargs.update(dataQueue=self._data_queue.name)
 
         if expires is not None:
             kwargs.update(expires=expires)
 
         if metadata is not None:
             kwargs.update(metadata=metadata)
 
-        response = await self.rpc("sink.subscribe", metrics=metrics, **kwargs)
+        response = await self.rpc("sink.subscribe", metrics=metrics_list, **kwargs)
         assert response is not None
 
-        self._subscribed_metrics.update(metrics)
+        self._subscribed_metrics.update(metrics_list)
         # Save the subscription RPC args in case we need to resubscribe (after a reconnect).
         self._subscribe_args = kwargs
 
         if self._data_queue is None:
             await self.sink_config(**response)
         return response
 
-    async def unsubscribe(self, metrics: List[Metric]) -> None:
+    async def unsubscribe(self, metrics: Iterable[Metric]) -> None:
         assert self._data_queue
+        metrics_list = list(metrics)
         await self.rpc(
-            "sink.unsubscribe", dataQueue=self._data_queue.name, metrics=metrics
+            "sink.unsubscribe", dataQueue=self._data_queue.name, metrics=metrics_list
         )
 
-        self._subscribed_metrics.difference_update(metrics)
+        self._subscribed_metrics.difference_update(metrics_list)
 
         # If we just unsubscribed from all metrics, reset the subscription args
         # to their defaults.
         if not self._subscribed_metrics:
             self._subscribe_args = dict()
 
-    async def _on_data_message(self, message: aio_pika.IncomingMessage) -> None:
+    async def _on_data_message(
+        self, message: aio_pika.abc.AbstractIncomingMessage
+    ) -> None:
         async with message.process(requeue=True):
             body = message.body
             from_token = message.app_id
             metric = message.routing_key
+            if metric is None:
+                logger.warning(
+                    "received data message without routing key from {}", from_token
+                )
+                return
 
             logger.debug("received message from {}", from_token)
             data_response = DataChunk()
             data_response.ParseFromString(body)
 
             await self._on_data_chunk(metric, data_response)
 
@@ -209,14 +216,27 @@
             metric: name of the metric for which a new data point arrived
             timestamp: timepoint at which this metric was measured
             value: value of the metric at time of measurement
         """
 
 
 class DurableSink(Sink):
+    """
+    A base class for user-defined :term:`Sinks<Sink>` that uses a configuration.
+    General :class:`Sink` implementations are transient and therefore do not register as
+    unique agents with a configuration. This implementation does call the
+    `sink.register` RPC and receives a configuration in response that is passed
+    to the `config` rpc handler.
+
+    See :class:`Sink` for the general API.
+
+    Constructor arguments are passed to :class:`Sink`.
+    However, ``add_uuid`` is not supported and always ``False``.
+    """
+
     def __init__(self, *args: Any, **kwargs: Any):
         super().__init__(*args, add_uuid=False, **kwargs)
 
     async def connect(self) -> None:
         await super().connect()
 
         response = await self.rpc("sink.register")
```

## metricq/source.py

```diff
@@ -26,31 +26,29 @@
 # PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 # LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 # NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 import asyncio
 from abc import abstractmethod
-from typing import Any, Dict, Optional, cast
+from collections.abc import Mapping
+from typing import Any, Optional, cast
 
 import aio_pika
-from aiormq import ChannelInvalidStateError
 
 from .data_client import DataClient
 from .datachunk_pb2 import DataChunk
 from .exceptions import PublishFailed
 from .logging import get_logger
 from .rpc import rpc_handler
 from .source_metric import ChunkSize, SourceMetric
-from .types import Metric, Timestamp
+from .timeseries import MetadataDict, Metric, Timestamp
 
 logger = get_logger(__name__)
 
-MetadataDict = Dict[str, Any]
-
 
 class Source(DataClient):
     """A MetricQ :term:`Source`.
 
     See :ref:`source-how-to` on how to implement a new Source.
 
     Example:
@@ -102,16 +100,18 @@
     Raises:
         TypeError: if value set is neither :literal:`None` nor an integer
         ValueError: if value set not a positive, non-zero integer
     """
 
     def __init__(self, *args: Any, **kwargs: Any):
         super().__init__(*args, **kwargs)
-        self.metrics: Dict[str, SourceMetric] = dict()
+        self.metrics: dict[str, SourceMetric] = dict()
         self.chunk_size = 1
+        self._task: Optional[asyncio.Task[None]] = None
+        self.task_stop_future: Optional[asyncio.Future[None]] = None
 
     async def connect(self) -> None:
         await super().connect()
         response = await self.rpc("source.register")
         assert response is not None
         logger.info("register response: {}", response)
         await self.data_config(**response)
@@ -121,46 +121,64 @@
         self.data_exchange = await self.data_channel.declare_exchange(
             name=response["dataExchange"], passive=True
         )
 
         if "config" in response:
             await self.rpc_dispatch("config", **response["config"])
 
-        self._event_loop.create_task(self.task())
+        self.task_stop_future = asyncio.Future()
+        self._task = self._event_loop.create_task(self.task())
 
     @abstractmethod
     async def task(self) -> None:
         """Override this with your main task for generating data points.
 
         The task is started after the source has connected and received its initial configuration.
 
         Note:
             This task is not restarted if it fails.
-            You are responsible for handling all relevant exceptions.
+            You are responsible for handling all relevant exceptions and for stopping
+            the task in :meth:`Agent.stop` or :meth:`close`
+        """
+
+    async def teardown(self) -> None:
+        """
+        .. Important::
+            Do not call this function, it is called indirectly by :meth:`Agent.stop`.
+
+        Triggers a stop for the source task and waits for it to complete in addition to
+        :meth:`DataClient.teardown()`.
         """
+        logger.debug("stopping Source.task")
+        assert self._task is not None
+        assert self.task_stop_future is not None
+        self.task_stop_future.set_result(None)
+        # Wait for the task to complete before actually closing the connections etc.
+        await self._task
+        await super().teardown()
 
     def __getitem__(self, id: Metric) -> SourceMetric:
         if id not in self.metrics:
             self.metrics[id] = SourceMetric(id, self, chunk_size=self.chunk_size)
         return self.metrics[id]
 
     def _augment_metadata(
-        self, metrics: Dict[Metric, MetadataDict]
-    ) -> Dict[Metric, MetadataDict]:
+        self, metrics: Mapping[Metric, MetadataDict]
+    ) -> dict[Metric, MetadataDict]:
         # Do not modify the user-supplied metadata.  The user expects this to
         # be a read-only parameter, modifying it without their consent could
         # lead to surprises.
-        augmented = metrics.copy()
+        augmented = dict(metrics)
         for metric, metadata in augmented.items():
-            # If a SourceMetric has a chunk_size of 0, chunking is disable.
+            # If a SourceMetric has a chunk_size of 0, chunking is disabled.
             metadata.setdefault("chunkSize", self[metric].chunk_size)
 
         return augmented
 
-    async def declare_metrics(self, metrics: Dict[str, MetadataDict]) -> None:
+    async def declare_metrics(self, metrics: Mapping[str, MetadataDict]) -> None:
         """Declare a set of :term:`metrics<Metric>` this Source produces values for.
 
         Before producing :term:`data points<Data Point>` for some metric, a Source must have declared that Metric.
 
         Args:
             metrics:
                 A dictionary mapping metrics to their metadata.
@@ -211,17 +229,17 @@
             PublishFailed: if sending a data point failed
 
         Warning:
             In case of failure, unsent data points remain buffered.
             An attempt at sending them is made once :meth:`flush` is triggered,
             either manually or on the next call to :meth:`send`.
 
-            In particular you should not call this method again with the same data point,
+            In particular, you should not call this method again with the same data point,
             even if the first call failed.
-            Otherwise duplicate data points will be sent, which results in an invalid :term:`metric<Metric>`.
+            Otherwise, duplicate data points will be sent, which results in an invalid :term:`metric<Metric>`.
         """
         logger.debug("send({},{},{})", metric, time, value)
         metric_object = self[metric]
         assert metric_object is not None
         await metric_object.send(time, value)
 
     async def flush(self) -> None:
@@ -245,15 +263,15 @@
         msg = aio_pika.Message(data_chunk.SerializeToString())
         assert self.data_exchange is not None
         await self._data_connection_watchdog.established()
         try:
             # TOC/TOU hazard: by the time we publish, the data connection might
             # be gone again, even if we waited for it to be established before.
             await self.data_exchange.publish(msg, routing_key=metric, mandatory=False)
-        except ChannelInvalidStateError as e:
+        except aio_pika.exceptions.ChannelInvalidStateError as e:
             # Trying to publish on a closed channel results in a ChannelInvalidStateError
             # from aiormq.  Let's wrap that in a more descriptive error.
             raise PublishFailed(
                 f"Failed to publish data chunk for metric '{metric!r}' "
                 f"on exchange '{self.data_exchange}' ({self.data_connection})"
             ) from e
```

## metricq/source_metric.py

```diff
@@ -29,15 +29,15 @@
 # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 import math
 from typing import Any, Optional, cast
 
 from . import source
 from .datachunk_pb2 import DataChunk
-from .types import Metric, Timestamp
+from .timeseries import Metric, Timestamp
 
 
 class ChunkSize:
     def __set_name__(self, owner: Any, name: str) -> None:
         self._field_name = f"_{name}"
 
     def __get__(self, instance: "SourceMetric", cls: Optional[type] = None) -> int:
```

## metricq/subscription.py

```diff
@@ -21,45 +21,45 @@
 # CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 # EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 # PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 # PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 # LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 # NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-
+from collections.abc import Iterable
 from types import TracebackType
-from typing import Any, List, Optional, Type, Union
+from typing import Any, Optional
 
 from .client import Client
 from .drain import Drain
 from .logging import get_logger
-from .types import Timedelta
+from .timeseries import Timedelta
 
 logger = get_logger(__name__)
 
 
 class Subscriber(Client):
     def __init__(
         self,
         *args: Any,
-        expires: Union[Timedelta, int, float],
-        metrics: List[str],
+        expires: Timedelta | int | float,
+        metrics: Iterable[str],
         **kwargs: Any,
     ):
         """Subscribes to a list of metrics
 
         Args:
-            metrics (List[str], optional): List of metrics that you want to subscribe to.
-            expires (Union[Timedelta, int, float]): The lifetime of the subscription queue in seconds.
+            metrics: List of metrics that you want to subscribe to.
+            expires: The lifetime of the subscription queue in seconds.
         """
-        if not metrics:
-            raise ValueError("Metrics list must not be empty")
 
         super().__init__(*args, **kwargs)
-        self._metrics = metrics
+        self._metrics = list(metrics)
+        if not self._metrics:
+            raise ValueError("Metrics must not be empty")
 
         if isinstance(expires, Timedelta):
             self.expires = expires
         else:
             self.expires = Timedelta.from_s(expires)
 
         if self.expires < Timedelta(0) or self.expires == Timedelta(0):
@@ -77,15 +77,15 @@
     async def connect(self, **kwargs: Any) -> None:
         """Connects to the MetricQ network, sends the subscribe request, and disconnects again.
 
         After it has successfully finished, the :attr:`.queue` name is set.
 
         .. note::
 
-            This performes the RPC and closes the connection before the return.
+            This performs the RPC and closes the connection before the return.
 
         """
 
         await super().connect()
 
         response = await self.rpc(
             "sink.subscribe", metrics=self._metrics, expires=self.expires, **kwargs
@@ -113,13 +113,13 @@
 
         new_kwargs = self._kwargs
         new_kwargs.update(kwargs)
         return Drain(*self._args, **new_kwargs, queue=self.queue, metrics=self._metrics)
 
     async def __aexit__(
         self,
-        exc_type: Optional[Type[BaseException]],
+        exc_type: Optional[type[BaseException]],
         exc_value: Optional[BaseException],
         exc_traceback: Optional[TracebackType],
     ) -> None:
         # We don't need to `await self.stop()` here, as this is already done in `Subscriber.connect()`
         pass
```

## metricq/synchronous_source.py

```diff
@@ -24,40 +24,41 @@
 # EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 # PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 # PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 # LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 # NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 # SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 import asyncio
+from collections.abc import Mapping
 from threading import Event, Lock, Thread
-from typing import Any, Dict
+from typing import Any
 
 from .logging import get_logger
-from .source import MetadataDict, Source
-from .types import Timestamp
+from .source import Source
+from .timeseries import MetadataDict, Metric, Timestamp
 
 logger = get_logger(__name__)
 
 
 class _SynchronousSource(Source):
     def __init__(self, *args: Any, **kwargs: Any):
         super().__init__(*args, **kwargs)
         # Remember this is a threading.Event, which is threadsafe
-        # not a asyncio.Event which is not threadsafe
+        # not an :class:`asyncio.Event` which is not threadsafe
         # Because we use threads anyway
-        # There is no threading.Future
+        # There is no :class:`threading.Future`
         self.exception = None
         self._ready_event = Event()
 
     async def connect(self) -> None:
         await super().connect()
         self._ready_event.set()
 
     def on_exception(
-        self, loop: asyncio.AbstractEventLoop, context: Dict[str, Any]
+        self, loop: asyncio.AbstractEventLoop, context: Mapping[str, Any]
     ) -> None:
         super().on_exception(loop, context)
 
         if not self._ready_event.is_set():
             self.exception = context["exception"]
             self._ready_event.set()
 
@@ -75,14 +76,22 @@
             raise self.exception
 
     def run(self, *args: Any, **kwargs: Any) -> None:
         super().run(catch_signals=())
 
 
 class SynchronousSource:
+    """
+    This is a :term:`Source` that can be used in a synchronous context.
+    It spawns a new thread and runs an asynchronous :class:`Source` in it.
+    Therefore, this class does not actually derive from :class:`Source`.
+
+    All parameters are passed to `Source.__init__`.
+    """
+
     _lock = Lock()
     _tid = 0
 
     def __init__(self, *args: Any, **kwargs: Any):
         self._source = _SynchronousSource(*args, **kwargs)
         self._thread = Thread(target=self._source.run)
 
@@ -100,43 +109,85 @@
             self.stop()
             raise e
 
         logger.info("[SynchronousSource] ready")
 
     def send(
         self,
-        metric: str,
+        metric: Metric,
         time: Timestamp,
         value: float,
         block: bool = True,
         timeout: float = 60,
     ) -> None:
+        """
+        Send a single metric timestamp / value to MetricQ.
+        Returns immediately unless `block` is `True`.
+        Exceptions other than a timeout are not propagated to the caller, but logged instead.
+
+        Args:
+            metric: name of the metric
+            time: timestamp
+            value: value of the metric
+            block: wait for completion of the asynchronous send operation
+            timeout: in seconds, is only used in blocking mode
+
+        Raises:
+            TimeoutError: in case of timeout
+        """
         f = asyncio.run_coroutine_threadsafe(
             self._source.send(metric, time, value), self._source._event_loop
         )
         if block:
             exception = f.exception(timeout)
             if exception:
                 logger.error("[SynchronousSource] failed to send data {}", exception)
                 # Keep going for reconnect. If you want to panic, do the following instead
                 # self.stop()
                 # raise exception
 
     def declare_metrics(
-        self, metrics: Dict[str, MetadataDict], block: bool = True, timeout: float = 60
+        self,
+        metrics: Mapping[Metric, MetadataDict],
+        block: bool = True,
+        timeout: float = 60,
     ) -> None:
+        """
+        Declare the metrics that are published by this source.
+        Returns immediately unless `block` is `True`.
+        Exceptions other than a timeout are not propagated to the caller, but logged instead.
+
+        Args:
+            metrics: a mapping from metric name to metadata
+            block: wait for completion of the asynchronous send operation
+            timeout: in seconds, is only used in blocking mode
+
+        Raises:
+            TimeoutError: in case of timeout
+        """
         f = asyncio.run_coroutine_threadsafe(
             self._source.declare_metrics(metrics), self._source._event_loop
         )
         if block:
             exception = f.exception(timeout)
             if exception:
                 logger.error("[SynchronousSource] failed to send data {}", exception)
 
     def stop(self, timeout: float = 60) -> None:
+        """
+        Stop the source and wait for the thread to join.
+        Exceptions other than a timeout are not propagated to the caller, but logged instead.
+
+        Args:
+            timeout: timeout in seconds
+
+        Raises:
+            TimeoutError: in case of timeout
+        """
+
         logger.info("[SynchronousSource] stopping")
         f = asyncio.run_coroutine_threadsafe(
             self._source.stop(), self._source._event_loop
         )
         exception = f.exception(timeout=timeout)
         if exception:
             logger.error("[SynchronousSource] stop call failed {}", exception)
```

## Comparing `metricq_proto/LICENSE` & `metricq-5.0.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `metricq-4.2.0.dist-info/METADATA` & `metricq-5.0.0.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,80 +1,78 @@
 Metadata-Version: 2.1
 Name: metricq
-Version: 4.2.0
+Version: 5.0.0
 Summary: A highly-scalable, distributed metric data processing framework based on RabbitMQ
 Home-page: https://github.com/metricq/metricq-python
 Author: TU Dresden
 License: BSD 3-clause "New" or "Revised License"
 Classifier: License :: OSI Approved :: BSD License
 Classifier: Programming Language :: Python :: 3
 Requires-Python: >=3.10
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: aio-pika (>=6.7.1,~=6.7)
-Requires-Dist: protobuf (~=3.20.0)
+Requires-Dist: aio-pika (~=9.0)
+Requires-Dist: protobuf (<3.13,>=3.12)
+Requires-Dist: Deprecated (~=1.2.13)
 Requires-Dist: python-dateutil (>=2.8.1,~=2.8)
 Requires-Dist: yarl
 Requires-Dist: setuptools
 Provides-Extra: dev
 Requires-Dist: pytest ; extra == 'dev'
 Requires-Dist: pytest-asyncio ; extra == 'dev'
 Requires-Dist: pytest-mock ; extra == 'dev'
-Requires-Dist: black ; extra == 'dev'
+Requires-Dist: black (~=23.1.0) ; extra == 'dev'
 Requires-Dist: flake8 ; extra == 'dev'
 Requires-Dist: isort (~=5.0) ; extra == 'dev'
 Requires-Dist: check-manifest ; extra == 'dev'
 Requires-Dist: pre-commit ; extra == 'dev'
-Requires-Dist: aiomonitor ; extra == 'dev'
+Requires-Dist: aiomonitor-ng ; extra == 'dev'
 Requires-Dist: click ; extra == 'dev'
 Requires-Dist: click-log ; extra == 'dev'
 Requires-Dist: click-completion ; extra == 'dev'
-Requires-Dist: mypy (>=0.900) ; extra == 'dev'
+Requires-Dist: mypy (>=1.2.0) ; extra == 'dev'
 Requires-Dist: mypy-protobuf ; extra == 'dev'
+Requires-Dist: types-Deprecated ; extra == 'dev'
 Requires-Dist: types-setuptools ; extra == 'dev'
 Requires-Dist: types-protobuf ; extra == 'dev'
 Requires-Dist: types-python-dateutil ; extra == 'dev'
-Requires-Dist: sphinx (~=4.5.0) ; extra == 'dev'
-Requires-Dist: sphinx-rtd-theme (~=1.0.0) ; extra == 'dev'
-Requires-Dist: sphinx-autodoc-typehints (~=1.18.0) ; extra == 'dev'
-Requires-Dist: sphinxcontrib-trio (~=1.1) ; extra == 'dev'
-Requires-Dist: scanpydoc (~=0.7.7) ; extra == 'dev'
+Requires-Dist: sphinx (~=6.1.3) ; extra == 'dev'
+Requires-Dist: sphinx-rtd-theme (~=1.2.0) ; extra == 'dev'
+Requires-Dist: sphinx-autodoc-typehints (~=1.22.0) ; extra == 'dev'
+Requires-Dist: sphinxcontrib-trio (~=1.1.2) ; extra == 'dev'
+Requires-Dist: scanpydoc (~=0.7.8) ; extra == 'dev'
 Requires-Dist: tox ; extra == 'dev'
 Provides-Extra: docs
-Requires-Dist: sphinx (~=4.5.0) ; extra == 'docs'
-Requires-Dist: sphinx-rtd-theme (~=1.0.0) ; extra == 'docs'
-Requires-Dist: sphinx-autodoc-typehints (~=1.18.0) ; extra == 'docs'
-Requires-Dist: sphinxcontrib-trio (~=1.1) ; extra == 'docs'
-Requires-Dist: scanpydoc (~=0.7.7) ; extra == 'docs'
+Requires-Dist: sphinx (~=6.1.3) ; extra == 'docs'
+Requires-Dist: sphinx-rtd-theme (~=1.2.0) ; extra == 'docs'
+Requires-Dist: sphinx-autodoc-typehints (~=1.22.0) ; extra == 'docs'
+Requires-Dist: sphinxcontrib-trio (~=1.1.2) ; extra == 'docs'
+Requires-Dist: scanpydoc (~=0.7.8) ; extra == 'docs'
 Provides-Extra: examples
-Requires-Dist: aiomonitor ; extra == 'examples'
+Requires-Dist: aiomonitor-ng ; extra == 'examples'
 Requires-Dist: click ; extra == 'examples'
 Requires-Dist: click-log ; extra == 'examples'
 Requires-Dist: click-completion ; extra == 'examples'
 Provides-Extra: lint
-Requires-Dist: black ; extra == 'lint'
+Requires-Dist: black (~=23.1.0) ; extra == 'lint'
 Requires-Dist: flake8 ; extra == 'lint'
 Requires-Dist: isort (~=5.0) ; extra == 'lint'
 Requires-Dist: check-manifest ; extra == 'lint'
 Requires-Dist: pre-commit ; extra == 'lint'
+Provides-Extra: pandas
+Requires-Dist: pandas (~=2.0.1) ; extra == 'pandas'
+Requires-Dist: pandas-stubs (~=2.0.1) ; extra == 'pandas'
 Provides-Extra: test
 Requires-Dist: pytest ; extra == 'test'
 Requires-Dist: pytest-asyncio ; extra == 'test'
 Requires-Dist: pytest-mock ; extra == 'test'
-Provides-Extra: tools
-Requires-Dist: click ; extra == 'tools'
-Requires-Dist: click-log ; extra == 'tools'
-Requires-Dist: click-completion ; extra == 'tools'
-Requires-Dist: humanize (~=2.5) ; extra == 'tools'
-Requires-Dist: python-dateutil (~=2.8) ; extra == 'tools'
-Requires-Dist: numpy ; extra == 'tools'
-Requires-Dist: termplotlib ; extra == 'tools'
 Provides-Extra: typing
-Requires-Dist: mypy (>=0.900) ; extra == 'typing'
+Requires-Dist: mypy (>=1.2.0) ; extra == 'typing'
 Requires-Dist: mypy-protobuf ; extra == 'typing'
+Requires-Dist: types-Deprecated ; extra == 'typing'
 Requires-Dist: types-setuptools ; extra == 'typing'
 Requires-Dist: types-protobuf ; extra == 'typing'
 Requires-Dist: types-python-dateutil ; extra == 'typing'
 
 ![BSD 3-clause](https://img.shields.io/badge/license-BSD%203--clause-blue.svg)
 ![Python package](https://github.com/metricq/metricq-python/workflows/Python%20package/badge.svg)
 ![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)
@@ -110,15 +108,15 @@
 
 A simple Source is implemented in `metricq_source.py`, as is a Sink in `metricq_sink.py`.
 We will use the former to produce data for a metric called `test.py.dummy`, which we
 will then receive and print with the latter.
 
 Assuming a MetricQ instance is reachable at `localhost`, configure a
 client<sup>(consult the documentation of your favourite config provider on how
-to do that)</sup> named `source-py-dummy` to produce values with a frequence of
+to do that)</sup> named `source-py-dummy` to produce values with a frequency of
 0.5Hz (i.e. every 2 seconds) :
 
 ```json
 {
     "rate": 0.5
 }
 ```
@@ -134,15 +132,15 @@
 
 On the other side, run
 
 ```sh
 $ ./examples/metricq_sink.py --server 'amqp://localhost/' --metrics 'test.py.dummy'
 ```
 
-and you should see new values for the metric `test.py.dummy` appear ever 2 seconds.
+and you should see new values for the metric `test.py.dummy` appear every 2 seconds.
 
 ## Tools and utility scripts
 
 The repository [metricq/metricq-tools](https://github.com/metricq/metricq-tools)
 contains a collection of tools and utility scripts to monitor and administrate
 a MetricQ network.
 Install them from [PyPI](https://pypi.org/project/metricq-tools/):
```

## Comparing `metricq-4.2.0.dist-info/RECORD` & `metricq-5.0.0.dist-info/RECORD`

 * *Files 24% similar despite different names*

```diff
@@ -1,33 +1,36 @@
-metricq/__init__.py,sha256=r-SxOtsnhGI_i1udgYVoMR5wF2BNuQt9zveyTWSjm_Q,2499
-metricq/_protobuf_version.py,sha256=ipuJyuOKjp4bZyRJ_LcKWeqEGxtXb-DmDKbALvT-hB4,72
-metricq/agent.py,sha256=aZnG8qwmGCIo1VlnkRdu_rMBStB8fT90kewHIz0c4QU,25488
-metricq/client.py,sha256=LZSjTNGVpQu-bPaQAJR7NFZ4Y6tbV1pDr8rbC97PQKE,9182
-metricq/connection_watchdog.py,sha256=MKYsRG2dno1cUL8bpAhLqFtdp600uWqG_GhfMWIq0Ig,5340
-metricq/data_client.py,sha256=EeuG_2fn5-wThgbIoV0yPFIpbTWrFNCfwwliPOfbiW4,5509
-metricq/datachunk_pb2.py,sha256=-Q7mU58dy6TiY99dzt5Ef4FMsCCAhPlTxCxcdB4N1WI,999
-metricq/datachunk_pb2.pyi,sha256=HAqFeZFWj4dmPayfa6va5f2_7F4o-z0jxNnoQjBxAUk,1082
-metricq/drain.py,sha256=O0oScIneFCHJdSsgW6k5s8PJIeoSlPqXJUZIhGPCpj8,4535
+metricq/__init__.py,sha256=6kgzmmXhfX5Mb2TR77ALUrV3OKDJsp_K48PGPf8wQ20,2680
+metricq/_protobuf_version.py,sha256=jWesju_2oXjeJF2B92qFyG6R17gqL18r-XFOTFoMpQI,124
+metricq/agent.py,sha256=GmkqWk3X_sXp-HtBXLf46QTzM4ub5X-bxpFtz0ioq9I,31105
+metricq/client.py,sha256=4m9_wwy2DQ7Xx6eYMYunl_5u-2Ux_GfxLXcPGao3vQM,9409
+metricq/connection_watchdog.py,sha256=cv5gXbHlzHUlHqQbkeRnJ6yEsalt6Gi1Vb6_Z4rcOnY,4990
+metricq/data_client.py,sha256=OVW-iWzcQA-mww1NTbW5yMNnyli7_drcvFNAp6ck3ts,5945
+metricq/datachunk_pb2.py,sha256=pvFDu94IdU01_5amJf72gy6mK-ekGZmnrNlZ23lTBt8,2394
+metricq/datachunk_pb2.pyi,sha256=LWw976S7SJLKrUD1Doq-aZFxyf9a54cFVHQCo-vBRNc,2776
+metricq/drain.py,sha256=KcBqPwny8ztaMPlPzbDhzEHSNxxl0wL2Wf6gNWFnumg,4600
 metricq/exceptions.py,sha256=ZMVRqEypFQ78L_MzGPBoOrlXJjwNZuS89JUQlDiu7Bg,3716
-metricq/history_client.py,sha256=eis7_sCJpZLnypMdyG0EvGjhgJudlBdl9i-c69wgfvQ,30618
-metricq/history_pb2.py,sha256=-I7QOjgTyUHB35f4v7Cwnijad9ggDz63aXLVQTrfZ78,2685
-metricq/history_pb2.pyi,sha256=2VsMxfw5wwE8i3L0MBA334ULEyBS1CtTXuQwSwBJMA8,6142
-metricq/interval_source.py,sha256=K88isJjTIX69Eroq73SWuZQjQI7qjvVBgf6aXfUKaOk,7446
-metricq/logging.py,sha256=w2q4l2Sis_sKWhyh3MJUaHxvpFtvFFQUHgrfPMptbts,1383
+metricq/history_client.py,sha256=zWCvdH1iDtCtRxX4ZAC7Q8ix94wBfWZginrNmaD_Z9I,31510
+metricq/history_pb2.py,sha256=Zs8opF7Ut7Ufc8dQ_CbEE26yJXPqjORTMigkCYbeVtI,13343
+metricq/history_pb2.pyi,sha256=6gMoBy6OBgmfAbldMoHL0HkE6RW4w7T6KFVPnfrg86E,7949
+metricq/interval_source.py,sha256=5159jdurisOGSn3CjORDFMOSGGCMEGx2MtsGJUoFvUc,7625
+metricq/logging.py,sha256=FD9PMRmkq-DbVTjegKVOfknGwoFnipPlNnnaCxCSbII,1272
 metricq/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-metricq/rpc.py,sha256=sSZhMczBEZ4twFiyRCiBrhnCB7WfBXDIREFr4BJf_vA,5771
-metricq/sink.py,sha256=gSO-rFzxc2h6hMDijMkpwy0eRDQYclv60Or_VyqCuV0,8865
-metricq/source.py,sha256=nA3igmAoioaECnNHP9-79lJvqiBmdh1ga8AjlVsuS0w,10625
-metricq/source_metric.py,sha256=gbxE3aay530k5Q7fZPKIVUCB204FPYjsZMYVWApZvwE,4168
-metricq/subscription.py,sha256=z_ZEUdxoQbcw_e4Ri3yRgW8W1r0KeG_ZJzVZsT9ZUmo,4684
-metricq/synchronous_source.py,sha256=lKnpNP8BY_0smcQHE9hlyVBFu2wXSR9UZRNMlnhaxNI,5519
-metricq/types.py,sha256=JdyPg4yNsStpHrrAfxTHMQoMndVDrrlh0k64srOx4Gk,28067
+metricq/rpc.py,sha256=m_ucVGoahd_75vlWTQtnR0U0EHbr-b_S7CpQsZkkV7I,5606
+metricq/sink.py,sha256=71ApFNv97vLUBASEiXltov7OFF5ADiSYemBJHqVicqM,9718
+metricq/source.py,sha256=unqmDqghqMZleB5acDIPrUs5NmeZQ42VKm1wVEbOdiM,11512
+metricq/source_metric.py,sha256=XLmqts6Kt4F3vdmMzVpQvJ1RxhIeWzUGaxgJ4KJagTU,4173
+metricq/subscription.py,sha256=UZtbxtYllckBzjL8xtVDzjZ_G_KjLxfj8tGdbIX6mJ4,4658
+metricq/synchronous_source.py,sha256=_JAw1B96Fr92wEqdEmQimC-iaBifA4cS74Adb-3RJCA,7261
 metricq/version.py,sha256=W-QgfeHfY0bcGOsGVYiT_FUI9C-cbnmtK9jjMSByqRY,1700
-metricq_proto/LICENSE,sha256=4h6-3O0i-fOl6I5GiEgQREkt1oGF13osLvN045KhVEQ,1559
-metricq_proto/README.md,sha256=QUGDnwYssR1QZ33mCtoFQMOzLD7jBlKQjafZJiYFVNE,417
-metricq_proto/datachunk.proto,sha256=OyEM8PpwEdm9MAw5aJph2BOxCyn2VrgKw7PYv2FF75E,1768
-metricq_proto/history.proto,sha256=VbiNsmq3pqU1ulKDR7HtoBd0MGFj3SnScuHL2Vh6qIk,2858
-metricq-4.2.0.dist-info/LICENSE,sha256=4h6-3O0i-fOl6I5GiEgQREkt1oGF13osLvN045KhVEQ,1559
-metricq-4.2.0.dist-info/METADATA,sha256=NlO6doXqis2Ze8j959U0khclDEzrFSNvervfd_CABzQ,6611
-metricq-4.2.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-metricq-4.2.0.dist-info/top_level.txt,sha256=LqxmIpOTxr-NnJtFzyw2AbyXdCycV--C443A7MjdnpQ,22
-metricq-4.2.0.dist-info/RECORD,,
+metricq/pandas/__init__.py,sha256=t9fIMrdjz0GcsdgRFhHacxO3nor_fLg48FMYOexXP1Y,90
+metricq/pandas/pandas_history_client.py,sha256=YlxNOVL3QKQD8tjSvYpDkAvjapTQ6TLwxsfly9dyCxc,4463
+metricq/timeseries/__init__.py,sha256=toIY9gqliQNcwQLvgt8vuz0m1ng3siivR0P9QuicfNA,330
+metricq/timeseries/extras.py,sha256=6MOrfWEUJCP2DyqK-pUb04WYU8qEoO5p7EQDBhU67bE,253
+metricq/timeseries/time_aggregate.py,sha256=hGiexnzEGhX8IsVbHZdWa_d2u7IWkcOxy62i115_g34,5060
+metricq/timeseries/time_value.py,sha256=bcSZmvU4ANkjW40-Gf2_uLV8tWeOlMY56UlDQpCa4Eo,1103
+metricq/timeseries/timedelta.py,sha256=OosCnluEe0GTI63XzBpar53ytvq0HaICbcrf3Q6Y3ZA,15354
+metricq/timeseries/timestamp.py,sha256=sOzup0Uyyye2YJ0QvtoGSpzU1ozVZ3Uzbi1fpiYh-iI,8467
+metricq-5.0.0.dist-info/LICENSE,sha256=4h6-3O0i-fOl6I5GiEgQREkt1oGF13osLvN045KhVEQ,1559
+metricq-5.0.0.dist-info/METADATA,sha256=tM9I-a15BuIZ7YTj5v7yuSbQ16OEtpV9yvF7O3hLYdM,6559
+metricq-5.0.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+metricq-5.0.0.dist-info/top_level.txt,sha256=NEFXKVj4euK5ANjLFlrVJ1pxZYZYmKfGfFw9S4nPKI0,8
+metricq-5.0.0.dist-info/RECORD,,
```

