# Comparing `tmp/cmem_plugin_kafka-1.2.0rc2.tar.gz` & `tmp/cmem_plugin_kafka-1.2.0rc3.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "cmem_plugin_kafka-1.2.0rc2.tar", max compression
+gzip compressed data, was "cmem_plugin_kafka-1.2.0rc3.tar", max compression
```

## Comparing `cmem_plugin_kafka-1.2.0rc2.tar` & `cmem_plugin_kafka-1.2.0rc3.tar`

### file list

```diff
@@ -1,11 +1,11 @@
--rw-r--r--   0        0        0    11334 2023-05-03 07:18:55.574411 cmem_plugin_kafka-1.2.0rc2/LICENSE
--rw-r--r--   0        0        0     3733 2023-05-03 07:18:55.574411 cmem_plugin_kafka-1.2.0rc2/README-public.md
--rw-r--r--   0        0        0        0 2023-05-03 07:18:55.574411 cmem_plugin_kafka-1.2.0rc2/cmem_plugin_kafka/__init__.py
--rw-r--r--   0        0        0     3485 2023-05-03 07:18:55.574411 cmem_plugin_kafka-1.2.0rc2/cmem_plugin_kafka/constants.py
--rw-r--r--   0        0        0    14685 2023-05-03 07:18:55.578411 cmem_plugin_kafka-1.2.0rc2/cmem_plugin_kafka/kafka_handlers.py
--rw-r--r--   0        0        0    11073 2023-05-03 07:18:55.578411 cmem_plugin_kafka-1.2.0rc2/cmem_plugin_kafka/utils.py
--rw-r--r--   0        0        0        0 2023-05-03 07:18:55.578411 cmem_plugin_kafka-1.2.0rc2/cmem_plugin_kafka/workflow/__init__.py
--rw-r--r--   0        0        0    10682 2023-05-03 07:18:55.578411 cmem_plugin_kafka-1.2.0rc2/cmem_plugin_kafka/workflow/consumer.py
--rw-r--r--   0        0        0     8722 2023-05-03 07:18:55.578411 cmem_plugin_kafka-1.2.0rc2/cmem_plugin_kafka/workflow/producer.py
--rw-r--r--   0        0        0     2206 2023-05-03 07:19:24.338569 cmem_plugin_kafka-1.2.0rc2/pyproject.toml
--rw-r--r--   0        0        0     4792 1970-01-01 00:00:00.000000 cmem_plugin_kafka-1.2.0rc2/PKG-INFO
+-rw-r--r--   0        0        0    11334 2023-05-17 07:44:13.374173 cmem_plugin_kafka-1.2.0rc3/LICENSE
+-rw-r--r--   0        0        0     3733 2023-05-17 07:44:13.374173 cmem_plugin_kafka-1.2.0rc3/README-public.md
+-rw-r--r--   0        0        0        0 2023-05-17 07:44:13.374173 cmem_plugin_kafka-1.2.0rc3/cmem_plugin_kafka/__init__.py
+-rw-r--r--   0        0        0     3836 2023-05-17 07:44:13.374173 cmem_plugin_kafka-1.2.0rc3/cmem_plugin_kafka/constants.py
+-rw-r--r--   0        0        0    14685 2023-05-17 07:44:13.374173 cmem_plugin_kafka-1.2.0rc3/cmem_plugin_kafka/kafka_handlers.py
+-rw-r--r--   0        0        0    13450 2023-05-17 07:44:13.374173 cmem_plugin_kafka-1.2.0rc3/cmem_plugin_kafka/utils.py
+-rw-r--r--   0        0        0        0 2023-05-17 07:44:13.374173 cmem_plugin_kafka-1.2.0rc3/cmem_plugin_kafka/workflow/__init__.py
+-rw-r--r--   0        0        0    10768 2023-05-17 07:44:13.374173 cmem_plugin_kafka-1.2.0rc3/cmem_plugin_kafka/workflow/consumer.py
+-rw-r--r--   0        0        0     9203 2023-05-17 07:44:13.374173 cmem_plugin_kafka-1.2.0rc3/cmem_plugin_kafka/workflow/producer.py
+-rw-r--r--   0        0        0     2268 2023-05-17 07:44:44.446513 cmem_plugin_kafka-1.2.0rc3/pyproject.toml
+-rw-r--r--   0        0        0     4918 1970-01-01 00:00:00.000000 cmem_plugin_kafka-1.2.0rc3/PKG-INFO
```

### Comparing `cmem_plugin_kafka-1.2.0rc2/LICENSE` & `cmem_plugin_kafka-1.2.0rc3/LICENSE`

 * *Files identical despite different names*

### Comparing `cmem_plugin_kafka-1.2.0rc2/README-public.md` & `cmem_plugin_kafka-1.2.0rc3/README-public.md`

 * *Files identical despite different names*

### Comparing `cmem_plugin_kafka-1.2.0rc2/cmem_plugin_kafka/constants.py` & `cmem_plugin_kafka-1.2.0rc3/cmem_plugin_kafka/constants.py`

 * *Files 12% similar despite different names*

```diff
@@ -11,14 +11,25 @@
     }
 )
 # SASL Mechanisms
 SASL_MECHANISMS = collections.OrderedDict(
     {"PLAIN": "Authentication based on username and passwords (PLAIN)"}
 )
 
+# Compression Types
+COMPRESSION_TYPES = collections.OrderedDict(
+    {
+        "none": "None",
+        "gzip": "gzip",
+        "snappy": "Snappy",
+        "lz4": "LZ4",
+        "zstd": "Zstandard"
+    }
+)
+
 # Auto Offset Resets
 AUTO_OFFSET_RESET = collections.OrderedDict(
     {
         "earliest": "automatically reset the offset to the earliest offset (earliest)",
         "latest": "automatically reset the offset to the latest offset (latest)",
         "error": "throw exception to the consumer if no previous offset "
         "is found for the consumer's group (error)",
@@ -62,14 +73,20 @@
 The sole purpose of this is to be able to track the source of requests beyond just
 ip and port by allowing a logical application name to be included in Kafka logs
 and monitoring aggregates.
 
 When the Client Id field is empty, the plugin defaults to DNS:PROJECT ID:TASK ID.
 """
 
+COMPRESSION_TYPE_DESCRIPTION = """
+The compression type for all data generated by the producer.
+
+The default is none (i.e. no compression).
+"""
+
 LOCAL_CONSUMER_QUEUE_MAX_SIZE_DESCRIPTION = """
 Maximum total message size in kilobytes that the consumer can buffer for
 a specific partition. The consumer will stop fetching from the partition
 if it hits this limit. This helps prevent consumers from running out of memory.
 """
 
 XML_SAMPLE = """
```

### Comparing `cmem_plugin_kafka-1.2.0rc2/cmem_plugin_kafka/kafka_handlers.py` & `cmem_plugin_kafka-1.2.0rc3/cmem_plugin_kafka/kafka_handlers.py`

 * *Files identical despite different names*

### Comparing `cmem_plugin_kafka-1.2.0rc2/cmem_plugin_kafka/utils.py` & `cmem_plugin_kafka-1.2.0rc3/cmem_plugin_kafka/utils.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,13 +1,16 @@
 """Kafka utils modules"""
+import gzip
 import json
 import re
 from typing import Dict, Any, Iterator, Optional
 from urllib.parse import urlparse
 
+import lz4.frame as lz4frame
+import zstandard
 from cmem.cmempy.config import get_cmem_base_uri
 from cmem.cmempy.workspace.projects.resources.resource import get_resource_response
 from cmem.cmempy.workspace.search import list_items
 from cmem.cmempy.workspace.tasks import get_task
 from cmem_plugin_base.dataintegration.context import (
     ExecutionContext,
     ExecutionReport,
@@ -19,14 +22,15 @@
 from cmem_plugin_base.dataintegration.utils import (
     setup_cmempy_user_access,
     split_task_id,
 )
 from confluent_kafka import Producer, Consumer, KafkaException, KafkaError
 from confluent_kafka.admin import AdminClient, TopicMetadata, ClusterMetadata
 from defusedxml import ElementTree
+from snappy import snappy
 
 from cmem_plugin_kafka.constants import KAFKA_TIMEOUT
 
 
 # pylint: disable-msg=too-few-public-methods
 class KafkaMessage:
     """
@@ -54,22 +58,58 @@
     """Kafka producer wrapper over confluent producer"""
 
     def __init__(self, config: dict, topic: str):
         """Create Producer instance"""
         self._producer = Producer(config)
         self._topic = topic
         self._no_of_success_messages: int = 0
+        self.compression_type = config.get("compression.type", 'none')
 
     def process(self, message: KafkaMessage):
         """Produce message to topic."""
         self._no_of_success_messages += 1
+        headers = message.headers if message.headers else {}
+        if self.compression_type != 'none' :
+            headers['compression.type'] = self.compression_type
         self._producer.produce(
-            self._topic, value=message.value, key=message.key, headers=message.headers
+            self._topic,
+            value=self.compress(message.value),
+            key=message.key,
+            headers=headers
         )
 
+    def compress(self, value: str):
+        """
+        Compresses the given value based on the configured compression type.
+
+        Args:
+            value (bytes): The value to compress.
+
+        Returns:
+            bytes: The compressed value.
+
+        Raises:
+            ValueError: If an unsupported compression type is provided.
+        """
+        _ = value.encode('utf-8')
+        if self.compression_type == 'none':
+            return _
+        if self.compression_type == 'gzip':
+            return gzip.compress(_)
+        if self.compression_type == 'snappy':
+            return snappy.compress(_)
+        if self.compression_type == 'zstd':
+            compressor = zstandard.ZstdCompressor(level=3)
+            compressed_data = compressor.compress(_)
+            return compressed_data
+        if self.compression_type == 'lz4':
+            compressed_data = lz4frame.compress(_)
+            return compressed_data
+        raise ValueError(f'Unsupported compression type: {self.compression_type}')
+
     def poll(self, timeout):
         """Polls the producer for events and calls the corresponding callbacks"""
         self._producer.poll(timeout)
 
     def flush(self, timeout=KAFKA_TIMEOUT):
         """Wait for all messages in the Producer queue to be delivered."""
         prev = 0
@@ -138,19 +178,39 @@
                 self._log.info("Messages are empty")
                 break
             if msg.error():
                 self._log.error(f"Consumer poll Error:{msg.error()}")
                 raise KafkaException(msg.error())
 
             self._no_of_success_messages += 1
+            headers = dict(msg.headers()) if msg.headers() else {}
+            if headers and 'compression.type' in headers:
+                compression_type = headers['compression.type']
+                if compression_type == b'gzip':
+                    decompressed_message = gzip.decompress(msg.value())
+                elif compression_type == b'snappy':
+                    decompressed_message = snappy.decompress(msg.value())
+                elif compression_type == b'zstd':
+                    decompressor = zstandard.ZstdDecompressor()
+                    decompressed_message = decompressor.decompress(msg.value())
+                elif compression_type == b'lz4':
+                    decompressed_message = lz4frame.decompress(msg.value())
+                else:
+                    raise ValueError(
+                        f'Unsupported compression codec: {compression_type}'
+                    )
+            else:
+                decompressed_message = msg.value()
+
             kafka_message = KafkaMessage(
                 key=msg.key().decode("utf-8") if msg.key() else "",
                 headers=msg.headers(),
-                value=msg.value().decode("utf-8"),
+                value=decompressed_message.decode("utf-8"),
             )
+
             if not self._first_message:
                 self._first_message = kafka_message
             if not self._no_of_success_messages % 10:
                 self._context.report.update(
                     ExecutionReport(
                         entity_count=self._no_of_success_messages,
                         operation="read",
```

### Comparing `cmem_plugin_kafka-1.2.0rc2/cmem_plugin_kafka/workflow/consumer.py` & `cmem_plugin_kafka-1.2.0rc3/cmem_plugin_kafka/workflow/consumer.py`

 * *Files 1% similar despite different names*

```diff
@@ -4,15 +4,16 @@
 from cmem.cmempy.workspace.tasks import get_task
 from cmem_plugin_base.dataintegration.context import ExecutionContext, ExecutionReport
 from cmem_plugin_base.dataintegration.description import PluginParameter, Plugin
 from cmem_plugin_base.dataintegration.entity import Entities
 from cmem_plugin_base.dataintegration.parameter.choice import ChoiceParameterType
 from cmem_plugin_base.dataintegration.plugins import WorkflowPlugin
 from cmem_plugin_base.dataintegration.types import IntParameterType
-from cmem_plugin_base.dataintegration.utils import write_to_dataset
+from cmem_plugin_base.dataintegration.utils import write_to_dataset, \
+    setup_cmempy_user_access
 from confluent_kafka import KafkaError
 
 from cmem_plugin_kafka.constants import (
     SECURITY_PROTOCOLS,
     SASL_MECHANISMS,
     AUTO_OFFSET_RESET,
     BOOTSTRAP_SERVERS_DESCRIPTION,
@@ -256,15 +257,15 @@
             context=context,
         )
         kafka_consumer.subscribe()
         if not self.message_dataset:
             return KafkaEntitiesDataHandler(
                 context=context, plugin_logger=self.log, kafka_consumer=kafka_consumer
             ).consume_messages()
-
+        setup_cmempy_user_access(context=context.user)
         task_meta_data = get_task(
             project=context.task.project_id(), task=self.message_dataset
         )
         if task_meta_data["data"]["type"] == "json":
             handler: KafkaDatasetHandler = KafkaJSONDataHandler(
                 context=context, plugin_logger=self.log, kafka_consumer=kafka_consumer
             )
```

### Comparing `cmem_plugin_kafka-1.2.0rc2/cmem_plugin_kafka/workflow/producer.py` & `cmem_plugin_kafka-1.2.0rc3/cmem_plugin_kafka/workflow/producer.py`

 * *Files 8% similar despite different names*

```diff
@@ -16,15 +16,15 @@
     SASL_MECHANISMS,
     BOOTSTRAP_SERVERS_DESCRIPTION,
     SECURITY_PROTOCOL_DESCRIPTION,
     SASL_ACCOUNT_DESCRIPTION,
     SASL_PASSWORD_DESCRIPTION,
     CLIENT_ID_DESCRIPTION,
     XML_SAMPLE,
-    JSON_SAMPLE,
+    JSON_SAMPLE, COMPRESSION_TYPES, COMPRESSION_TYPE_DESCRIPTION,
 )
 from cmem_plugin_kafka.kafka_handlers import (
     KafkaJSONDataHandler,
     KafkaXMLDataHandler,
     KafkaEntitiesDataHandler,
     KafkaDataHandler,
 )
@@ -127,14 +127,22 @@
         PluginParameter(
             name="client_id",
             label="Client Id",
             advanced=True,
             default_value="",
             description=CLIENT_ID_DESCRIPTION,
         ),
+        PluginParameter(
+            name="compression_type",
+            label="Compression Type",
+            advanced=True,
+            param_type=ChoiceParameterType(COMPRESSION_TYPES),
+            default_value="none",
+            description=COMPRESSION_TYPE_DESCRIPTION,
+        ),
     ],
 )
 # pylint: disable-msg=too-many-instance-attributes
 class KafkaProducerPlugin(WorkflowPlugin):
     """Kafka Producer Plugin"""
 
     def __init__(
@@ -143,25 +151,27 @@
         bootstrap_servers: str,
         security_protocol: str,
         sasl_mechanisms: str,
         sasl_username: str,
         sasl_password: str,
         kafka_topic: str,
         client_id: str = "",
+        compression_type: str = "none"
     ) -> None:
         if not isinstance(bootstrap_servers, str):
             raise ValueError("Specified server id is invalid")
         self.message_dataset = message_dataset
         self.bootstrap_servers = bootstrap_servers
         self.security_protocol = security_protocol
         self.sasl_mechanisms = sasl_mechanisms
         self.sasl_username = sasl_username
         self.sasl_password = sasl_password
         self.kafka_topic = kafka_topic
         self.client_id = client_id
+        self.compression_type = compression_type
         self._kafka_stats: dict = {}
 
     def metrics_callback(self, json: str):
         """sends producer metrics to server"""
         self._kafka_stats = get_kafka_statistics(json_data=json)
         for key, value in self._kafka_stats.items():
             self.log.info(f"kafka-stats: {key:10} - {value:10}")
@@ -177,14 +187,15 @@
         config = {
             "bootstrap.servers": self.bootstrap_servers,
             "security.protocol": self.security_protocol,
             "client.id": self.client_id
             if self.client_id
             else get_default_client_id(project_id=project_id, task_id=task_id),
             "statistics.interval.ms": "1000",
+            "compression.type": self.compression_type,
             "stats_cb": self.metrics_callback,
             "error_cb": self.error_callback,
         }
         if self.security_protocol.startswith("SASL"):
             config.update(
                 {
                     "sasl.mechanisms": self.sasl_mechanisms,
```

### Comparing `cmem_plugin_kafka-1.2.0rc2/pyproject.toml` & `cmem_plugin_kafka-1.2.0rc3/pyproject.toml`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 [tool.poetry]
 name = "cmem-plugin-kafka"
-version = "1.2.0rc2"
+version = "1.2.0rc3"
 license = "Apache-2.0"
 description = "Send and receive messages from Apache Kafka."
 authors = ["eccenca GmbH <cmempy-developer@eccenca.com>"]
 classifiers = [
     "Development Status :: 5 - Production/Stable",
     "Environment :: Plugins",
     "Topic :: Software Development :: Libraries :: Python Modules",
@@ -24,14 +24,17 @@
 confluent-kafka = [
     { version = "1.9.2" },
     # before you publish, comment out this next line
     # { markers = "sys_platform == 'darwin'", url = "https://files.pythonhosted.org/packages/fb/16/d04dded73439266a3dbcd585f1128483dcf509e039bacd93642ac5de97d4/confluent-kafka-1.8.2.tar.gz"}
 ]
 defusedxml = "^0.7.1"
 json-stream = "2.3.0"
+python-snappy = "^0.6.1"
+zstandard = "^0.21.0"
+lz4 = "^4.3.2"
 
 [tool.poetry.group.dev.dependencies]
 bandit = "^1.7.5"
 black = "^23.3.0"
 coverage = "^7.2.3"
 defusedxml = "^0.7.1"
 flake8-formatter-junit-xml = "^0.0.6"
```

### Comparing `cmem_plugin_kafka-1.2.0rc2/PKG-INFO` & `cmem_plugin_kafka-1.2.0rc3/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: cmem-plugin-kafka
-Version: 1.2.0rc2
+Version: 1.2.0rc3
 Summary: Send and receive messages from Apache Kafka.
 Home-page: https://github.com/eccenca/cmem-plugin-kafka
 License: Apache-2.0
 Keywords: eccenca Corporate Memory,plugin,kafka,kafka-producer,kafka-consumer
 Author: eccenca GmbH
 Author-email: cmempy-developer@eccenca.com
 Requires-Python: >=3.9,<4.0
@@ -17,14 +17,17 @@
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Requires-Dist: cmem-plugin-base (>=3.1.0,<4.0.0)
 Requires-Dist: confluent-kafka (==1.9.2)
 Requires-Dist: defusedxml (>=0.7.1,<0.8.0)
 Requires-Dist: json-stream (==2.3.0)
+Requires-Dist: lz4 (>=4.3.2,<5.0.0)
+Requires-Dist: python-snappy (>=0.6.1,<0.7.0)
+Requires-Dist: zstandard (>=0.21.0,<0.22.0)
 Description-Content-Type: text/markdown
 
 # cmem-plugin-kafka
 
 Send and receive messages from Apache Kafka.
 
 This is a plugin for [eccenca](https://eccenca.com) [Corporate Memory](https://documentation.eccenca.com).
```

